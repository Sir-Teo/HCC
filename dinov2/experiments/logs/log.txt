I20241029 13:25:44 3844484 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 13:25:44 3844484 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 13:25:44 3844484 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 13:25:44 3844484 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 13:25:44 3844484 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 13:26:05 3844484 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 13:26:26 3844484 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 13:26:26 3844484 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 13:26:26 3844484 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 13:26:26 3844484 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 13:26:26 3844484 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 13:26:26 3844484 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 13:26:26 3844484 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 13:26:27 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 13:26:28 3844484 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 13:26:28 3844484 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 13:26:28 3844484 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 13:26:28 3844484 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 13:26:28 3844484 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 13:26:28 3844484 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 13:26:29 3844484 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 13:26:29 3844484 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 13:26:29 3844484 dinov2 param_groups.py:54] chunked fsdp
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 13:26:29 3844484 dinov2 param_groups.py:64] else code branch
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:26:29 3844484 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 13:26:29 3844484 dinov2 train.py:102] Schedulers ready.
W20241029 13:26:29 3844484 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 13:26:29 3844484 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 13:26:29 3844484 dinov2 augmentations.py:34] ###################################
I20241029 13:26:29 3844484 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 13:26:29 3844484 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 13:26:29 3844484 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 13:26:29 3844484 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 13:26:29 3844484 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 13:26:29 3844484 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 13:26:29 3844484 dinov2 augmentations.py:41] ###################################
I20241029 13:26:29 3844484 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 13:26:30 3844484 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 13:26:30 3844484 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 13:26:30 3844484 dinov2 loaders.py:208] using PyTorch data loader
I20241029 13:26:30 3844484 dinov2 loaders.py:223] infinite data loader
I20241029 13:26:30 3844484 dinov2 train.py:217] Starting training from iteration 0
E20241029 13:27:08 3844484 submitit submission.py:68] Submitted job triggered an exception
I20241029 13:46:45 3847921 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 13:46:45 3847921 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 13:46:45 3847921 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 13:46:45 3847921 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 13:46:45 3847921 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 13:47:07 3847921 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 13:47:28 3847921 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 13:47:28 3847921 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 13:47:28 3847921 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 13:47:28 3847921 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 13:47:28 3847921 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 13:47:28 3847921 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 13:47:28 3847921 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 13:47:29 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 13:47:30 3847921 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 13:47:30 3847921 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 13:47:30 3847921 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 13:47:30 3847921 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 13:47:30 3847921 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 13:47:30 3847921 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 13:47:31 3847921 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 13:47:31 3847921 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 13:47:31 3847921 dinov2 param_groups.py:54] chunked fsdp
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 13:47:31 3847921 dinov2 param_groups.py:64] else code branch
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:47:31 3847921 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 13:47:31 3847921 dinov2 train.py:102] Schedulers ready.
W20241029 13:47:31 3847921 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 13:47:31 3847921 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 13:47:31 3847921 dinov2 augmentations.py:34] ###################################
I20241029 13:47:31 3847921 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 13:47:31 3847921 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 13:47:31 3847921 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 13:47:31 3847921 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 13:47:31 3847921 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 13:47:31 3847921 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 13:47:31 3847921 dinov2 augmentations.py:41] ###################################
I20241029 13:47:31 3847921 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 13:47:32 3847921 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 13:47:32 3847921 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 13:47:32 3847921 dinov2 loaders.py:208] using PyTorch data loader
I20241029 13:47:32 3847921 dinov2 loaders.py:223] infinite data loader
I20241029 13:47:32 3847921 dinov2 train.py:217] Starting training from iteration 0
E20241029 13:48:10 3847921 submitit submission.py:68] Submitted job triggered an exception
I20241029 13:54:55 3849773 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 13:54:55 3849773 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 13:54:55 3849773 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 13:54:55 3849773 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 13:54:55 3849773 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 13:55:16 3849773 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 13:55:37 3849773 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 13:55:37 3849773 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 13:55:37 3849773 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 13:55:37 3849773 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 13:55:37 3849773 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 13:55:37 3849773 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 13:55:37 3849773 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 13:55:38 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 13:55:39 3849773 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 13:55:39 3849773 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 13:55:39 3849773 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 13:55:39 3849773 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 13:55:39 3849773 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 13:55:39 3849773 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 13:55:40 3849773 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 13:55:40 3849773 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 13:55:40 3849773 dinov2 param_groups.py:54] chunked fsdp
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 13:55:40 3849773 dinov2 param_groups.py:64] else code branch
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 13:55:40 3849773 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 13:55:40 3849773 dinov2 train.py:102] Schedulers ready.
W20241029 13:55:40 3849773 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 13:55:40 3849773 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 13:55:40 3849773 dinov2 augmentations.py:34] ###################################
I20241029 13:55:40 3849773 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 13:55:40 3849773 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 13:55:40 3849773 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 13:55:40 3849773 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 13:55:40 3849773 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 13:55:40 3849773 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 13:55:40 3849773 dinov2 augmentations.py:41] ###################################
I20241029 13:55:40 3849773 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 13:55:41 3849773 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 13:55:41 3849773 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 13:55:41 3849773 dinov2 loaders.py:208] using PyTorch data loader
I20241029 13:55:41 3849773 dinov2 loaders.py:223] infinite data loader
I20241029 13:55:41 3849773 dinov2 train.py:217] Starting training from iteration 0
E20241029 13:56:17 3849773 submitit submission.py:68] Submitted job triggered an exception
I20241029 14:07:41 3852204 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 14:07:41 3852204 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 14:07:41 3852204 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 14:07:41 3852204 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 14:07:41 3852204 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 14:08:03 3852204 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 14:08:24 3852204 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 14:08:24 3852204 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 14:08:24 3852204 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 14:08:24 3852204 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 14:08:24 3852204 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 14:08:24 3852204 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 14:08:24 3852204 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 14:08:25 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 14:08:26 3852204 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 14:08:26 3852204 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 14:08:26 3852204 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 14:08:26 3852204 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 14:08:26 3852204 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 14:08:26 3852204 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 14:08:27 3852204 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 14:08:27 3852204 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 14:08:27 3852204 dinov2 param_groups.py:54] chunked fsdp
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 14:08:27 3852204 dinov2 param_groups.py:64] else code branch
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 14:08:27 3852204 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 14:08:27 3852204 dinov2 train.py:102] Schedulers ready.
W20241029 14:08:27 3852204 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 14:08:27 3852204 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 14:08:27 3852204 dinov2 augmentations.py:34] ###################################
I20241029 14:08:27 3852204 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 14:08:27 3852204 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 14:08:27 3852204 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 14:08:27 3852204 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 14:08:27 3852204 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 14:08:27 3852204 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 14:08:27 3852204 dinov2 augmentations.py:41] ###################################
I20241029 14:08:27 3852204 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 14:08:28 3852204 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 14:08:28 3852204 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 14:08:28 3852204 dinov2 loaders.py:208] using PyTorch data loader
I20241029 14:08:28 3852204 dinov2 loaders.py:223] infinite data loader
I20241029 14:08:28 3852204 dinov2 train.py:217] Starting training from iteration 0
E20241029 14:09:07 3852204 submitit submission.py:68] Submitted job triggered an exception
I20241029 19:53:35 3906953 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 19:53:35 3906953 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 19:53:35 3906953 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 19:53:35 3906953 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 19:53:36 3906953 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 19:53:50 3906953 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 19:54:04 3906953 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 19:54:04 3906953 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 19:54:04 3906953 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 19:54:04 3906953 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 19:54:04 3906953 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 19:54:04 3906953 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 19:54:04 3906953 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 19:54:05 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 19:54:05 3906953 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 19:54:05 3906953 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 19:54:05 3906953 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 19:54:05 3906953 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 19:54:05 3906953 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 19:54:05 3906953 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 19:54:06 3906953 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 19:54:06 3906953 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 19:54:06 3906953 dinov2 param_groups.py:54] chunked fsdp
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 19:54:06 3906953 dinov2 param_groups.py:64] else code branch
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 19:54:06 3906953 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 19:54:06 3906953 dinov2 train.py:102] Schedulers ready.
W20241029 19:54:06 3906953 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 19:54:06 3906953 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 19:54:06 3906953 dinov2 augmentations.py:34] ###################################
I20241029 19:54:06 3906953 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 19:54:06 3906953 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 19:54:06 3906953 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 19:54:06 3906953 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 19:54:06 3906953 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 19:54:06 3906953 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 19:54:06 3906953 dinov2 augmentations.py:41] ###################################
I20241029 19:54:06 3906953 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 19:54:07 3906953 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 19:54:07 3906953 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 19:54:07 3906953 dinov2 loaders.py:208] using PyTorch data loader
I20241029 19:54:07 3906953 dinov2 loaders.py:223] infinite data loader
I20241029 19:54:07 3906953 dinov2 train.py:217] Starting training from iteration 0
E20241029 19:54:08 3906953 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:03:41 2116075 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:03:41 2116075 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:03:41 2116075 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:03:42 2116075 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:03:42 2116075 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:03:46 2116075 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 20:03:51 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:03:51 2116075 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:03:52 2116075 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 20:03:52 2116075 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:03:52 2116075 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:03:52 2116075 dinov2 param_groups.py:64] else code branch
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:03:52 2116075 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:03:52 2116075 dinov2 train.py:102] Schedulers ready.
W20241029 20:03:52 2116075 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 20:03:52 2116075 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:03:52 2116075 dinov2 augmentations.py:34] ###################################
I20241029 20:03:52 2116075 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:03:52 2116075 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:03:52 2116075 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:03:52 2116075 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:03:52 2116075 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:03:52 2116075 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:03:52 2116075 dinov2 augmentations.py:41] ###################################
I20241029 20:03:52 2116075 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:03:53 2116075 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:03:53 2116075 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:03:53 2116075 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:03:53 2116075 dinov2 loaders.py:223] infinite data loader
I20241029 20:03:53 2116075 dinov2 train.py:217] Starting training from iteration 0
E20241029 20:03:55 2116075 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:10:48 3909756 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:10:48 3909756 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:10:48 3909756 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:10:48 3909756 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:10:48 3909756 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:11:02 3909756 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:11:16 3909756 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:11:16 3909756 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:11:16 3909756 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:11:16 3909756 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:11:16 3909756 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:11:16 3909756 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:11:16 3909756 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 20:11:17 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 20:11:18 3909756 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:11:18 3909756 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:11:18 3909756 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:11:18 3909756 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:11:18 3909756 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:11:18 3909756 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:11:18 3909756 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 20:11:18 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 20:11:19 3909756 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:11:19 3909756 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:11:19 3909756 dinov2 param_groups.py:64] else code branch
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:11:19 3909756 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:11:19 3909756 dinov2 train.py:102] Schedulers ready.
W20241029 20:11:19 3909756 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 20:11:19 3909756 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:11:19 3909756 dinov2 augmentations.py:34] ###################################
I20241029 20:11:19 3909756 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:11:19 3909756 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:11:19 3909756 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:11:19 3909756 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:11:19 3909756 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:11:19 3909756 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:11:19 3909756 dinov2 augmentations.py:41] ###################################
I20241029 20:11:19 3909756 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:11:20 3909756 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:11:20 3909756 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:11:20 3909756 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:11:20 3909756 dinov2 loaders.py:223] infinite data loader
I20241029 20:11:20 3909756 dinov2 train.py:217] Starting training from iteration 0
E20241029 20:11:50 3909756 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:14:43 2123287 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:14:43 2123287 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:14:43 2123287 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:14:43 2123287 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:14:43 2123287 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:14:48 2123287 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:14:52 2123287 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:14:52 2123287 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:14:52 2123287 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:14:52 2123287 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:14:52 2123287 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:14:52 2123287 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:14:52 2123287 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 20:14:53 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 20:14:53 2123287 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:14:53 2123287 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:14:53 2123287 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:14:53 2123287 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:14:53 2123287 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:14:53 2123287 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:14:53 2123287 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 20:14:53 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 20:14:54 2123287 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:14:54 2123287 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:14:54 2123287 dinov2 param_groups.py:64] else code branch
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:14:54 2123287 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:14:54 2123287 dinov2 train.py:102] Schedulers ready.
W20241029 20:14:54 2123287 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 20:14:54 2123287 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:14:54 2123287 dinov2 augmentations.py:34] ###################################
I20241029 20:14:54 2123287 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:14:54 2123287 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:14:54 2123287 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:14:54 2123287 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:14:54 2123287 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:14:54 2123287 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:14:54 2123287 dinov2 augmentations.py:41] ###################################
I20241029 20:14:54 2123287 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
E20241029 20:14:54 2123287 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:19:24 2126383 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:19:24 2126383 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:19:24 2126383 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:19:24 2126383 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:19:24 2126383 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:19:29 2126383 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:19:33 2126383 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:19:33 2126383 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:19:33 2126383 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:19:33 2126383 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:19:33 2126383 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:19:33 2126383 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:19:33 2126383 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 20:19:34 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 20:19:34 2126383 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:19:34 2126383 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:19:34 2126383 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:19:34 2126383 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:19:34 2126383 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:19:34 2126383 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:19:35 2126383 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 20:19:35 2126383 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:19:35 2126383 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:19:35 2126383 dinov2 param_groups.py:64] else code branch
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:19:35 2126383 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:19:35 2126383 dinov2 train.py:102] Schedulers ready.
W20241029 20:19:35 2126383 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 20:19:35 2126383 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:19:35 2126383 dinov2 augmentations.py:34] ###################################
I20241029 20:19:35 2126383 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:19:35 2126383 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:19:35 2126383 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:19:35 2126383 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:19:35 2126383 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:19:35 2126383 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:19:35 2126383 dinov2 augmentations.py:41] ###################################
I20241029 20:19:35 2126383 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:19:36 2126383 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:19:36 2126383 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:19:36 2126383 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:19:36 2126383 dinov2 loaders.py:223] infinite data loader
I20241029 20:19:36 2126383 dinov2 train.py:217] Starting training from iteration 0
E20241029 20:20:06 2126383 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:26:10 2756882 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:26:10 2756882 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:26:10 2756882 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:26:10 2756882 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:26:10 2756882 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:26:13 2756882 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 20:26:16 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:26:16 2756882 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:26:17 2756882 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 20:26:17 2756882 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:26:17 2756882 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:26:17 2756882 dinov2 param_groups.py:64] else code branch
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:26:17 2756882 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:26:17 2756882 dinov2 train.py:102] Schedulers ready.
W20241029 20:26:17 2756882 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 20:26:17 2756882 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:26:17 2756882 dinov2 augmentations.py:34] ###################################
I20241029 20:26:17 2756882 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:26:17 2756882 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:26:17 2756882 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:26:17 2756882 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:26:17 2756882 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:26:17 2756882 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:26:17 2756882 dinov2 augmentations.py:41] ###################################
I20241029 20:26:17 2756882 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:26:17 2756882 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:26:17 2756882 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:26:17 2756882 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:26:17 2756882 dinov2 loaders.py:223] infinite data loader
I20241029 20:26:17 2756882 dinov2 train.py:217] Starting training from iteration 0
E20241029 20:27:03 2756882 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:43:05 2759695 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:43:05 2759695 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:43:05 2759695 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:43:05 2759695 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:43:05 2759695 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:43:08 2759695 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:43:10 2759695 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:43:10 2759695 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:43:10 2759695 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:43:10 2759695 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:43:10 2759695 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:43:10 2759695 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:43:10 2759695 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 20:43:11 2759695 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:43:11 2759695 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:43:11 2759695 dinov2 param_groups.py:64] else code branch
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:43:11 2759695 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:43:11 2759695 dinov2 train.py:102] Schedulers ready.
W20241029 20:43:11 2759695 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 20:43:11 2759695 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:43:11 2759695 dinov2 augmentations.py:34] ###################################
I20241029 20:43:11 2759695 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:43:11 2759695 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:43:11 2759695 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:43:11 2759695 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:43:11 2759695 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:43:11 2759695 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:43:11 2759695 dinov2 augmentations.py:41] ###################################
I20241029 20:43:11 2759695 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:43:12 2759695 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:43:12 2759695 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:43:12 2759695 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:43:12 2759695 dinov2 loaders.py:223] infinite data loader
I20241029 20:43:12 2759695 dinov2 train.py:217] Starting training from iteration 0
E20241029 20:43:58 2759695 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:45:45 2761208 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:45:45 2761208 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:45:45 2761208 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:45:45 2761208 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:45:45 2761208 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:45:48 2761208 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): DropPath()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): DropPath()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-5): 6 x Identity()
  (6-11): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-11): 12 x Identity()
  (12-17): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

W20241029 20:45:51 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py:100: UserWarning: FSDP will not all-gather parameters for containers that do not implement forward: BlockChunk(
  (0-17): 18 x Identity()
  (18-23): 6 x NestedTensorBlock(
    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (attn): MemEffAttention(
      (qkv): Linear(in_features=1024, out_features=3072, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=1024, out_features=1024, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (ls1): LayerScale()
    (drop_path1): Identity()
    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (ls2): LayerScale()
    (drop_path2): Identity()
  )
)
  return fsdp_fn(module, **kwargs)

I20241029 20:45:51 2761208 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:45:51 2761208 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:45:51 2761208 dinov2 param_groups.py:64] else code branch
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:45:51 2761208 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:45:52 2761208 dinov2 train.py:102] Schedulers ready.
W20241029 20:45:52 2761208 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20241029 20:45:52 2761208 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:45:52 2761208 dinov2 augmentations.py:34] ###################################
I20241029 20:45:52 2761208 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:45:52 2761208 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:45:52 2761208 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:45:52 2761208 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:45:52 2761208 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:45:52 2761208 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:45:52 2761208 dinov2 augmentations.py:41] ###################################
I20241029 20:45:52 2761208 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:45:52 2761208 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:45:52 2761208 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:45:52 2761208 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:45:52 2761208 dinov2 loaders.py:223] infinite data loader
I20241029 20:45:52 2761208 dinov2 train.py:217] Starting training from iteration 0
E20241029 20:46:36 2761208 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:51:49 2763002 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:51:49 2763002 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:51:49 2763002 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:51:49 2763002 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:51:49 2763002 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:51:52 2763002 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:51:55 2763002 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:51:56 2763002 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
I20241029 20:51:56 2763002 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:51:56 2763002 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:51:56 2763002 dinov2 param_groups.py:64] else code branch
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:51:56 2763002 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:51:56 2763002 dinov2 train.py:102] Schedulers ready.
I20241029 20:51:56 2763002 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:51:56 2763002 dinov2 augmentations.py:34] ###################################
I20241029 20:51:56 2763002 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:51:56 2763002 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:51:56 2763002 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:51:56 2763002 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:51:56 2763002 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:51:56 2763002 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:51:56 2763002 dinov2 augmentations.py:41] ###################################
I20241029 20:51:56 2763002 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:51:56 2763002 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:51:56 2763002 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:51:56 2763002 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:51:56 2763002 dinov2 loaders.py:223] infinite data loader
I20241029 20:51:56 2763002 dinov2 train.py:217] Starting training from iteration 0
E20241029 20:52:43 2763002 submitit submission.py:68] Submitted job triggered an exception
I20241029 20:55:32 2764610 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 20:55:32 2764610 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short,gpu4_short,gpu4_medium,a100_dev,a100_long,gpu8_short,gpu8_medium
timeout: 280
I20241029 20:55:32 2764610 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 20:55:32 2764610 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 20:55:32 2764610 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:55:35 2764610 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 20:55:38 2764610 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 20:55:39 2764610 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
I20241029 20:55:39 2764610 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 20:55:39 2764610 dinov2 param_groups.py:54] chunked fsdp
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:55:39 2764610 dinov2 param_groups.py:64] else code branch
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 20:55:39 2764610 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 20:55:39 2764610 dinov2 train.py:102] Schedulers ready.
I20241029 20:55:39 2764610 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 20:55:39 2764610 dinov2 augmentations.py:34] ###################################
I20241029 20:55:39 2764610 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 20:55:39 2764610 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 20:55:39 2764610 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 20:55:39 2764610 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 20:55:39 2764610 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 20:55:39 2764610 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 20:55:39 2764610 dinov2 augmentations.py:41] ###################################
I20241029 20:55:39 2764610 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 20:55:39 2764610 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 20:55:39 2764610 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 20:55:39 2764610 dinov2 loaders.py:208] using PyTorch data loader
I20241029 20:55:39 2764610 dinov2 loaders.py:223] infinite data loader
I20241029 20:55:39 2764610 dinov2 train.py:217] Starting training from iteration 0
W20241029 20:56:06 2764610 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()

I20241029 20:56:06 2764610 dinov2 helpers.py:102] Training  [     0/125000]  eta: 38 days, 23:26:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7430 (13.7430)  dino_local_crops_loss: 9.1752 (9.1752)  dino_global_crops_loss: 1.1469 (1.1469)  koleo_loss: 0.6465 (0.6465)  ibot_loss: 2.7744 (2.7744)  time: 26.940701  data: 23.797283  max mem: 31863
I20241029 20:56:25 2764610 dinov2 helpers.py:102] Training  [    10/125000]  eta: 6 days, 1:33:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.1477 (14.1134)  dino_local_crops_loss: 9.5212 (9.4937)  dino_global_crops_loss: 1.1901 (1.1867)  koleo_loss: 0.6426 (0.6430)  ibot_loss: 2.7918 (2.7900)  time: 4.192608  data: 3.359959  max mem: 33117
I20241029 20:56:47 2764610 dinov2 helpers.py:102] Training  [    20/125000]  eta: 4 days, 15:12:56  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3853 (14.3075)  dino_local_crops_loss: 9.7294 (9.6653)  dino_global_crops_loss: 1.2162 (1.2082)  koleo_loss: 0.6377 (0.6359)  ibot_loss: 2.8010 (2.7981)  time: 2.016664  data: 1.413213  max mem: 33122
I20241029 20:57:10 2764610 dinov2 helpers.py:102] Training  [    30/125000]  eta: 4 days, 5:04:58  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5932 (14.4083)  dino_local_crops_loss: 9.9256 (9.7630)  dino_global_crops_loss: 1.2407 (1.2204)  koleo_loss: 0.6162 (0.6221)  ibot_loss: 2.8107 (2.8028)  time: 2.207488  data: 1.474407  max mem: 33122
I20241029 20:57:29 2764610 dinov2 helpers.py:102] Training  [    40/125000]  eta: 3 days, 20:59:45  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.6156 (14.4578)  dino_local_crops_loss: 9.9915 (9.8215)  dino_global_crops_loss: 1.2489 (1.2277)  koleo_loss: 0.5654 (0.6028)  ibot_loss: 2.8139 (2.8058)  time: 2.128539  data: 1.314613  max mem: 33122
I20241029 20:57:48 2764610 dinov2 helpers.py:102] Training  [    50/125000]  eta: 3 days, 15:23:28  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5958 (14.4792)  dino_local_crops_loss: 10.0035 (9.8571)  dino_global_crops_loss: 1.2504 (1.2321)  koleo_loss: 0.5225 (0.5822)  ibot_loss: 2.8155 (2.8078)  time: 1.907163  data: 1.021869  max mem: 33122
I20241029 20:58:08 2764610 dinov2 helpers.py:102] Training  [    60/125000]  eta: 3 days, 12:24:57  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5230 (14.4814)  dino_local_crops_loss: 9.9954 (9.8782)  dino_global_crops_loss: 1.2494 (1.2348)  koleo_loss: 0.4609 (0.5593)  ibot_loss: 2.8160 (2.8091)  time: 1.926433  data: 0.704947  max mem: 33122
I20241029 20:58:28 2764610 dinov2 helpers.py:102] Training  [    70/125000]  eta: 3 days, 10:40:43  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.4465 (14.4722)  dino_local_crops_loss: 9.9723 (9.8900)  dino_global_crops_loss: 1.2465 (1.2362)  koleo_loss: 0.4116 (0.5360)  ibot_loss: 2.8157 (2.8101)  time: 2.037241  data: 0.585836  max mem: 33122
I20241029 20:58:48 2764610 dinov2 helpers.py:102] Training  [    80/125000]  eta: 3 days, 8:58:05  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3921 (14.4618)  dino_local_crops_loss: 9.9501 (9.8963)  dino_global_crops_loss: 1.2438 (1.2370)  koleo_loss: 0.3921 (0.5177)  ibot_loss: 2.8154 (2.8107)  time: 2.031503  data: 0.551876  max mem: 33122
I20241029 20:59:10 2764610 dinov2 helpers.py:102] Training  [    90/125000]  eta: 3 days, 8:09:21  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3761 (14.4498)  dino_local_crops_loss: 9.9323 (9.8996)  dino_global_crops_loss: 1.2415 (1.2374)  koleo_loss: 0.3821 (0.5016)  ibot_loss: 2.8151 (2.8112)  time: 2.053391  data: 0.540931  max mem: 33122
I20241029 20:59:28 2764610 dinov2 helpers.py:102] Training  [   100/125000]  eta: 3 days, 6:36:14  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3239 (14.4353)  dino_local_crops_loss: 9.9192 (9.9011)  dino_global_crops_loss: 1.2399 (1.2376)  koleo_loss: 0.3506 (0.4852)  ibot_loss: 2.8142 (2.8115)  time: 1.991141  data: 0.512333  max mem: 33122
W20241029 20:59:41 2764610 submitit job_environment.py:196] Bypassing signal SIGTERM
W20241029 20:59:41 2764610 submitit job_environment.py:196] Bypassing signal SIGCONT
I20241029 20:59:48 2764610 dinov2 helpers.py:102] Training  [   110/125000]  eta: 3 days, 5:43:28  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.2793 (14.4191)  dino_local_crops_loss: 9.9094 (9.9015)  dino_global_crops_loss: 1.2387 (1.2377)  koleo_loss: 0.3167 (0.4683)  ibot_loss: 2.8136 (2.8116)  time: 1.923207  data: 0.457061  max mem: 33122
I20241029 21:01:32 2766422 dinov2 config.py:59] git:
  sha: 9427795ef9b4cdfa1ad7fb419d457b21c80f416c, status: has uncommitted changes, branch: main

I20241029 21:01:32 2766422 dinov2 config.py:60] config_file: dinov2/configs/train/vitl16_short.yaml
cpus_per_task: 16
eval: 
eval_only: False
mem: 100
ngpus: 2
no_resume: False
nodes: 1
opts: ['train.dataset_path=UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images', 'output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments', 'train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/experiments']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
partition: a100_short
timeout: 4319
I20241029 21:01:32 2766422 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241029 21:01:32 2766422 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments

I20241029 21:01:32 2766422 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 21:01:35 2766422 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241029 21:01:38 2766422 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20241029 21:01:38 2766422 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241029 21:01:38 2766422 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241029 21:01:38 2766422 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241029 21:01:38 2766422 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241029 21:01:38 2766422 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241029 21:01:38 2766422 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
I20241029 21:01:39 2766422 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241029 21:01:39 2766422 dinov2 param_groups.py:54] chunked fsdp
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 21:01:39 2766422 dinov2 param_groups.py:64] else code branch
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241029 21:01:39 2766422 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241029 21:01:39 2766422 dinov2 train.py:102] Schedulers ready.
I20241029 21:01:39 2766422 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241029 21:01:39 2766422 dinov2 augmentations.py:34] ###################################
I20241029 21:01:39 2766422 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241029 21:01:39 2766422 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241029 21:01:39 2766422 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241029 21:01:39 2766422 dinov2 augmentations.py:38] local_crops_number: 8
I20241029 21:01:39 2766422 dinov2 augmentations.py:39] global_crops_size: 224
I20241029 21:01:39 2766422 dinov2 augmentations.py:40] local_crops_size: 96
I20241029 21:01:39 2766422 dinov2 augmentations.py:41] ###################################
I20241029 21:01:39 2766422 dinov2 loaders.py:86] using dataset: "UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images"
I20241029 21:01:39 2766422 dinov2 loaders.py:91] # of dataset samples: 3,984
I20241029 21:01:39 2766422 dinov2 loaders.py:124] sampler: sharded infinite
I20241029 21:01:39 2766422 dinov2 loaders.py:208] using PyTorch data loader
I20241029 21:01:39 2766422 dinov2 loaders.py:223] infinite data loader
I20241029 21:01:39 2766422 dinov2 train.py:217] Starting training from iteration 0
W20241029 21:02:06 2766422 py.warnings warnings.py:110] /gpfs/scratch/wz1492/miniconda3/envs/dinov2/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()

I20241029 21:02:06 2766422 dinov2 helpers.py:102] Training  [     0/125000]  eta: 39 days, 1:50:28  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7430 (13.7430)  dino_local_crops_loss: 9.1752 (9.1752)  dino_global_crops_loss: 1.1469 (1.1469)  koleo_loss: 0.6465 (0.6465)  ibot_loss: 2.7744 (2.7744)  time: 27.009825  data: 23.962084  max mem: 31863
I20241029 21:02:25 2766422 dinov2 helpers.py:102] Training  [    10/125000]  eta: 6 days, 1:44:57  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.1477 (14.1134)  dino_local_crops_loss: 9.5212 (9.4937)  dino_global_crops_loss: 1.1901 (1.1867)  koleo_loss: 0.6426 (0.6430)  ibot_loss: 2.7918 (2.7900)  time: 4.197913  data: 3.369013  max mem: 33118
I20241029 21:02:46 2766422 dinov2 helpers.py:102] Training  [    20/125000]  eta: 4 days, 14:51:18  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3853 (14.3075)  dino_local_crops_loss: 9.7294 (9.6653)  dino_global_crops_loss: 1.2162 (1.2082)  koleo_loss: 0.6377 (0.6359)  ibot_loss: 2.8010 (2.7981)  time: 2.002306  data: 1.392655  max mem: 33122
I20241029 21:03:09 2766422 dinov2 helpers.py:102] Training  [    30/125000]  eta: 4 days, 4:27:43  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5932 (14.4083)  dino_local_crops_loss: 9.9256 (9.7630)  dino_global_crops_loss: 1.2407 (1.2204)  koleo_loss: 0.6162 (0.6221)  ibot_loss: 2.8107 (2.8028)  time: 2.176851  data: 1.553867  max mem: 33122
I20241029 21:03:29 2766422 dinov2 helpers.py:102] Training  [    40/125000]  eta: 3 days, 20:58:22  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.6156 (14.4578)  dino_local_crops_loss: 9.9915 (9.8215)  dino_global_crops_loss: 1.2489 (1.2277)  koleo_loss: 0.5654 (0.6028)  ibot_loss: 2.8139 (2.8058)  time: 2.138087  data: 1.514276  max mem: 33122
I20241029 21:03:48 2766422 dinov2 helpers.py:102] Training  [    50/125000]  eta: 3 days, 15:34:52  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5950 (14.4790)  dino_local_crops_loss: 10.0035 (9.8571)  dino_global_crops_loss: 1.2504 (1.2321)  koleo_loss: 0.5215 (0.5819)  ibot_loss: 2.8155 (2.8078)  time: 1.948844  data: 1.333717  max mem: 33122
I20241029 21:04:08 2766422 dinov2 helpers.py:102] Training  [    60/125000]  eta: 3 days, 12:44:19  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5230 (14.4812)  dino_local_crops_loss: 9.9954 (9.8782)  dino_global_crops_loss: 1.2494 (1.2348)  koleo_loss: 0.4624 (0.5592)  ibot_loss: 2.8160 (2.8091)  time: 1.956131  data: 1.235699  max mem: 33122
I20241029 21:04:29 2766422 dinov2 helpers.py:102] Training  [    70/125000]  eta: 3 days, 11:07:07  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.4400 (14.4720)  dino_local_crops_loss: 9.9723 (9.8900)  dino_global_crops_loss: 1.2465 (1.2362)  koleo_loss: 0.4082 (0.5357)  ibot_loss: 2.8157 (2.8101)  time: 2.068265  data: 1.324093  max mem: 33122
I20241029 21:04:50 2766422 dinov2 helpers.py:102] Training  [    80/125000]  eta: 3 days, 9:41:06  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3924 (14.4617)  dino_local_crops_loss: 9.9501 (9.8963)  dino_global_crops_loss: 1.2438 (1.2370)  koleo_loss: 0.3921 (0.5176)  ibot_loss: 2.8154 (2.8107)  time: 2.086852  data: 1.445641  max mem: 33122
I20241029 21:05:12 2766422 dinov2 helpers.py:102] Training  [    90/125000]  eta: 3 days, 9:07:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3761 (14.4496)  dino_local_crops_loss: 9.9323 (9.8996)  dino_global_crops_loss: 1.2415 (1.2374)  koleo_loss: 0.3823 (0.5014)  ibot_loss: 2.8151 (2.8112)  time: 2.135375  data: 1.516768  max mem: 33122
I20241029 21:05:31 2766422 dinov2 helpers.py:102] Training  [   100/125000]  eta: 3 days, 7:37:25  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3246 (14.4352)  dino_local_crops_loss: 9.9192 (9.9011)  dino_global_crops_loss: 1.2399 (1.2376)  koleo_loss: 0.3513 (0.4850)  ibot_loss: 2.8142 (2.8115)  time: 2.055897  data: 1.436914  max mem: 33122
I20241029 21:05:51 2766422 dinov2 helpers.py:102] Training  [   110/125000]  eta: 3 days, 6:45:58  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.2795 (14.4189)  dino_local_crops_loss: 9.9094 (9.9015)  dino_global_crops_loss: 1.2387 (1.2377)  koleo_loss: 0.3169 (0.4682)  ibot_loss: 2.8136 (2.8116)  time: 1.962920  data: 1.342758  max mem: 33122
I20241029 21:06:15 2766422 dinov2 helpers.py:102] Training  [   120/125000]  eta: 3 days, 7:01:51  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.2282 (14.4009)  dino_local_crops_loss: 9.9007 (9.9011)  dino_global_crops_loss: 1.2376 (1.2376)  koleo_loss: 0.2783 (0.4506)  ibot_loss: 2.8125 (2.8116)  time: 2.193832  data: 1.573288  max mem: 33122
I20241029 21:06:34 2766422 dinov2 helpers.py:102] Training  [   130/125000]  eta: 3 days, 6:02:49  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.1677 (14.3811)  dino_local_crops_loss: 9.8923 (9.9001)  dino_global_crops_loss: 1.2365 (1.2375)  koleo_loss: 0.2278 (0.4319)  ibot_loss: 2.8109 (2.8115)  time: 2.137030  data: 1.517593  max mem: 33122
I20241029 21:06:53 2766422 dinov2 helpers.py:102] Training  [   140/125000]  eta: 3 days, 5:13:23  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.1015 (14.3589)  dino_local_crops_loss: 9.8828 (9.8986)  dino_global_crops_loss: 1.2354 (1.2373)  koleo_loss: 0.1750 (0.4117)  ibot_loss: 2.8086 (2.8113)  time: 1.913396  data: 1.293312  max mem: 33122
I20241029 21:07:15 2766422 dinov2 helpers.py:102] Training  [   150/125000]  eta: 3 days, 5:12:16  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.0248 (14.3343)  dino_local_crops_loss: 9.8723 (9.8965)  dino_global_crops_loss: 1.2340 (1.2371)  koleo_loss: 0.1122 (0.3899)  ibot_loss: 2.8069 (2.8109)  time: 2.069374  data: 1.447852  max mem: 33122
I20241029 21:07:37 2766422 dinov2 helpers.py:102] Training  [   160/125000]  eta: 3 days, 5:06:02  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9438 (14.3085)  dino_local_crops_loss: 9.8575 (9.8933)  dino_global_crops_loss: 1.2324 (1.2367)  koleo_loss: 0.0493 (0.3680)  ibot_loss: 2.8047 (2.8105)  time: 2.200917  data: 1.579868  max mem: 33122
I20241029 21:07:56 2766422 dinov2 helpers.py:102] Training  [   170/125000]  eta: 3 days, 4:22:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8961 (14.2837)  dino_local_crops_loss: 9.8334 (9.8894)  dino_global_crops_loss: 1.2294 (1.2362)  koleo_loss: 0.0301 (0.3480)  ibot_loss: 2.8032 (2.8100)  time: 2.024885  data: 1.404561  max mem: 33122
I20241029 21:08:18 2766422 dinov2 helpers.py:102] Training  [   180/125000]  eta: 3 days, 4:19:09  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8754 (14.2611)  dino_local_crops_loss: 9.8260 (9.8859)  dino_global_crops_loss: 1.2284 (1.2358)  koleo_loss: 0.0207 (0.3298)  ibot_loss: 2.8018 (2.8095)  time: 2.022638  data: 1.402217  max mem: 33122
I20241029 21:08:39 2766422 dinov2 helpers.py:102] Training  [   190/125000]  eta: 3 days, 4:07:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8755 (14.2412)  dino_local_crops_loss: 9.8305 (9.8835)  dino_global_crops_loss: 1.2290 (1.2355)  koleo_loss: 0.0146 (0.3133)  ibot_loss: 2.8002 (2.8090)  time: 2.137113  data: 1.516280  max mem: 33122
I20241029 21:09:02 2766422 dinov2 helpers.py:102] Training  [   200/125000]  eta: 3 days, 4:19:52  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8862 (14.2241)  dino_local_crops_loss: 9.8471 (9.8822)  dino_global_crops_loss: 1.2311 (1.2353)  koleo_loss: 0.0119 (0.2982)  ibot_loss: 2.7977 (2.8084)  time: 2.208214  data: 1.587109  max mem: 33122
I20241029 21:09:22 2766422 dinov2 helpers.py:102] Training  [   210/125000]  eta: 3 days, 4:04:23  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9036 (14.2093)  dino_local_crops_loss: 9.8667 (9.8818)  dino_global_crops_loss: 1.2335 (1.2353)  koleo_loss: 0.0092 (0.2845)  ibot_loss: 2.7955 (2.8077)  time: 2.183454  data: 1.563475  max mem: 33122
I20241029 21:09:42 2766422 dinov2 helpers.py:102] Training  [   220/125000]  eta: 3 days, 3:43:23  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9196 (14.1965)  dino_local_crops_loss: 9.8828 (9.8822)  dino_global_crops_loss: 1.2356 (1.2353)  koleo_loss: 0.0066 (0.2718)  ibot_loss: 2.7941 (2.8071)  time: 2.011913  data: 1.392266  max mem: 33122
I20241029 21:10:03 2766422 dinov2 helpers.py:102] Training  [   230/125000]  eta: 3 days, 3:35:03  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9328 (14.1854)  dino_local_crops_loss: 9.8967 (9.8831)  dino_global_crops_loss: 1.2374 (1.2355)  koleo_loss: 0.0052 (0.2603)  ibot_loss: 2.7936 (2.8065)  time: 2.035740  data: 1.415181  max mem: 33122
I20241029 21:10:24 2766422 dinov2 helpers.py:102] Training  [   240/125000]  eta: 3 days, 3:28:47  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9421 (14.1754)  dino_local_crops_loss: 9.9086 (9.8844)  dino_global_crops_loss: 1.2390 (1.2356)  koleo_loss: 0.0041 (0.2496)  ibot_loss: 2.7907 (2.8057)  time: 2.104258  data: 1.483134  max mem: 33122
I20241029 21:10:45 2766422 dinov2 helpers.py:102] Training  [   250/125000]  eta: 3 days, 3:24:02  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9497 (14.1665)  dino_local_crops_loss: 9.9178 (9.8859)  dino_global_crops_loss: 1.2402 (1.2359)  koleo_loss: 0.0021 (0.2397)  ibot_loss: 2.7888 (2.8050)  time: 2.118787  data: 1.498415  max mem: 33123
I20241029 21:11:04 2766422 dinov2 helpers.py:102] Training  [   260/125000]  eta: 3 days, 2:57:29  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9532 (14.1584)  dino_local_crops_loss: 9.9257 (9.8876)  dino_global_crops_loss: 1.2412 (1.2361)  koleo_loss: 0.0008 (0.2306)  ibot_loss: 2.7863 (2.8042)  time: 1.986045  data: 1.367418  max mem: 33123
I20241029 21:11:23 2766422 dinov2 helpers.py:102] Training  [   270/125000]  eta: 3 days, 2:39:12  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9553 (14.1510)  dino_local_crops_loss: 9.9311 (9.8892)  dino_global_crops_loss: 1.2420 (1.2363)  koleo_loss: -0.0003 (0.2220)  ibot_loss: 2.7835 (2.8034)  time: 1.888317  data: 1.269900  max mem: 33123
I20241029 21:11:44 2766422 dinov2 helpers.py:102] Training  [   280/125000]  eta: 3 days, 2:31:01  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9564 (14.1440)  dino_local_crops_loss: 9.9343 (9.8909)  dino_global_crops_loss: 1.2424 (1.2365)  koleo_loss: -0.0013 (0.2141)  ibot_loss: 2.7804 (2.8026)  time: 1.989331  data: 1.370655  max mem: 33123
I20241029 21:12:06 2766422 dinov2 helpers.py:102] Training  [   290/125000]  eta: 3 days, 2:34:26  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9534 (14.1373)  dino_local_crops_loss: 9.9344 (9.8924)  dino_global_crops_loss: 1.2425 (1.2367)  koleo_loss: -0.0021 (0.2066)  ibot_loss: 2.7779 (2.8016)  time: 2.126356  data: 1.507390  max mem: 33123
I20241029 21:12:28 2766422 dinov2 helpers.py:102] Training  [   300/125000]  eta: 3 days, 2:39:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9474 (14.1309)  dino_local_crops_loss: 9.9327 (9.8936)  dino_global_crops_loss: 1.2421 (1.2369)  koleo_loss: -0.0031 (0.1996)  ibot_loss: 2.7749 (2.8008)  time: 2.219719  data: 1.600260  max mem: 33123
I20241029 21:12:47 2766422 dinov2 helpers.py:102] Training  [   310/125000]  eta: 3 days, 2:24:04  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9375 (14.1246)  dino_local_crops_loss: 9.9288 (9.8947)  dino_global_crops_loss: 1.2417 (1.2370)  koleo_loss: -0.0042 (0.1931)  ibot_loss: 2.7735 (2.7998)  time: 2.080544  data: 1.460707  max mem: 33123
I20241029 21:13:06 2766422 dinov2 helpers.py:102] Training  [   320/125000]  eta: 3 days, 2:07:09  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9204 (14.1180)  dino_local_crops_loss: 9.9201 (9.8953)  dino_global_crops_loss: 1.2407 (1.2371)  koleo_loss: -0.0048 (0.1869)  ibot_loss: 2.7669 (2.7987)  time: 1.908794  data: 1.286730  max mem: 33123
I20241029 21:13:26 2766422 dinov2 helpers.py:102] Training  [   330/125000]  eta: 3 days, 1:57:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8962 (14.1108)  dino_local_crops_loss: 9.9063 (9.8954)  dino_global_crops_loss: 1.2389 (1.2371)  koleo_loss: -0.0053 (0.1811)  ibot_loss: 2.7575 (2.7972)  time: 1.941792  data: 1.317296  max mem: 33123
I20241029 21:13:49 2766422 dinov2 helpers.py:102] Training  [   340/125000]  eta: 3 days, 2:04:56  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8626 (14.1030)  dino_local_crops_loss: 9.8881 (9.8948)  dino_global_crops_loss: 1.2365 (1.2371)  koleo_loss: -0.0060 (0.1756)  ibot_loss: 2.7485 (2.7955)  time: 2.127790  data: 1.502798  max mem: 33123
I20241029 21:14:09 2766422 dinov2 helpers.py:102] Training  [   350/125000]  eta: 3 days, 1:56:38  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8113 (14.0941)  dino_local_crops_loss: 9.8629 (9.8935)  dino_global_crops_loss: 1.2335 (1.2369)  koleo_loss: -0.0072 (0.1704)  ibot_loss: 2.7234 (2.7932)  time: 2.134768  data: 1.510256  max mem: 33123
I20241029 21:14:33 2766422 dinov2 helpers.py:102] Training  [   360/125000]  eta: 3 days, 2:10:19  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7716 (14.0846)  dino_local_crops_loss: 9.8352 (9.8917)  dino_global_crops_loss: 1.2298 (1.2367)  koleo_loss: -0.0076 (0.1654)  ibot_loss: 2.7149 (2.7908)  time: 2.192423  data: 1.567750  max mem: 33123
I20241029 21:14:53 2766422 dinov2 helpers.py:102] Training  [   370/125000]  eta: 3 days, 2:04:20  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7245 (14.0743)  dino_local_crops_loss: 9.8137 (9.8894)  dino_global_crops_loss: 1.2275 (1.2364)  koleo_loss: -0.0077 (0.1608)  ibot_loss: 2.6933 (2.7877)  time: 2.210598  data: 1.586855  max mem: 33123
I20241029 21:15:13 2766422 dinov2 helpers.py:102] Training  [   380/125000]  eta: 3 days, 1:55:58  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6998 (14.0642)  dino_local_crops_loss: 9.8009 (9.8870)  dino_global_crops_loss: 1.2254 (1.2361)  koleo_loss: -0.0079 (0.1563)  ibot_loss: 2.6761 (2.7848)  time: 2.017265  data: 1.394188  max mem: 33123
I20241029 21:15:35 2766422 dinov2 helpers.py:102] Training  [   390/125000]  eta: 3 days, 1:57:35  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7009 (14.0549)  dino_local_crops_loss: 9.7935 (9.8845)  dino_global_crops_loss: 1.2246 (1.2358)  koleo_loss: -0.0079 (0.1521)  ibot_loss: 2.6916 (2.7824)  time: 2.082752  data: 1.458409  max mem: 33123
I20241029 21:15:56 2766422 dinov2 helpers.py:102] Training  [   400/125000]  eta: 3 days, 1:56:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6835 (14.0452)  dino_local_crops_loss: 9.7895 (9.8822)  dino_global_crops_loss: 1.2238 (1.2355)  koleo_loss: -0.0076 (0.1482)  ibot_loss: 2.6766 (2.7793)  time: 2.150566  data: 1.526796  max mem: 33123
I20241029 21:16:16 2766422 dinov2 helpers.py:102] Training  [   410/125000]  eta: 3 days, 1:49:19  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6686 (14.0362)  dino_local_crops_loss: 9.7895 (9.8798)  dino_global_crops_loss: 1.2220 (1.2352)  koleo_loss: -0.0080 (0.1443)  ibot_loss: 2.6649 (2.7768)  time: 2.062094  data: 1.440181  max mem: 33123
I20241029 21:16:38 2766422 dinov2 helpers.py:102] Training  [   420/125000]  eta: 3 days, 1:50:56  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6625 (14.0271)  dino_local_crops_loss: 9.7790 (9.8773)  dino_global_crops_loss: 1.2217 (1.2349)  koleo_loss: -0.0080 (0.1407)  ibot_loss: 2.6741 (2.7741)  time: 2.084244  data: 1.462863  max mem: 33123
I20241029 21:16:59 2766422 dinov2 helpers.py:102] Training  [   430/125000]  eta: 3 days, 1:49:15  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6425 (14.0183)  dino_local_crops_loss: 9.7686 (9.8747)  dino_global_crops_loss: 1.2209 (1.2345)  koleo_loss: -0.0078 (0.1373)  ibot_loss: 2.6692 (2.7718)  time: 2.139681  data: 1.518046  max mem: 33123
I20241029 21:17:16 2766422 dinov2 helpers.py:102] Training  [   440/125000]  eta: 3 days, 1:29:34  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6313 (14.0096)  dino_local_crops_loss: 9.7537 (9.8720)  dino_global_crops_loss: 1.2184 (1.2342)  koleo_loss: -0.0082 (0.1340)  ibot_loss: 2.6722 (2.7694)  time: 1.914655  data: 1.292811  max mem: 33123
I20241029 21:17:37 2766422 dinov2 helpers.py:102] Training  [   450/125000]  eta: 3 days, 1:28:08  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6265 (14.0010)  dino_local_crops_loss: 9.7499 (9.8692)  dino_global_crops_loss: 1.2173 (1.2338)  koleo_loss: -0.0083 (0.1308)  ibot_loss: 2.6725 (2.7671)  time: 1.911741  data: 1.288459  max mem: 33123
I20241029 21:17:57 2766422 dinov2 helpers.py:102] Training  [   460/125000]  eta: 3 days, 1:21:35  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6265 (13.9925)  dino_local_crops_loss: 9.7477 (9.8665)  dino_global_crops_loss: 1.2169 (1.2334)  koleo_loss: -0.0081 (0.1278)  ibot_loss: 2.6638 (2.7647)  time: 2.043260  data: 1.419401  max mem: 33123
I20241029 21:18:20 2766422 dinov2 helpers.py:102] Training  [   470/125000]  eta: 3 days, 1:31:00  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6161 (13.9840)  dino_local_crops_loss: 9.7338 (9.8635)  dino_global_crops_loss: 1.2138 (1.2330)  koleo_loss: -0.0082 (0.1250)  ibot_loss: 2.6667 (2.7625)  time: 2.164015  data: 1.540515  max mem: 33123
I20241029 21:18:41 2766422 dinov2 helpers.py:102] Training  [   480/125000]  eta: 3 days, 1:25:52  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6161 (13.9770)  dino_local_crops_loss: 9.7448 (9.8616)  dino_global_crops_loss: 1.2130 (1.2327)  koleo_loss: -0.0077 (0.1222)  ibot_loss: 2.6709 (2.7605)  time: 2.178501  data: 1.555601  max mem: 33123
I20241029 21:19:01 2766422 dinov2 helpers.py:102] Training  [   490/125000]  eta: 3 days, 1:21:10  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6128 (13.9691)  dino_local_crops_loss: 9.7408 (9.8588)  dino_global_crops_loss: 1.2129 (1.2323)  koleo_loss: -0.0076 (0.1196)  ibot_loss: 2.6741 (2.7585)  time: 2.017481  data: 1.394526  max mem: 33123
I20241029 21:19:24 2766422 dinov2 helpers.py:102] Training  [   500/125000]  eta: 3 days, 1:27:12  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5884 (13.9613)  dino_local_crops_loss: 9.7142 (9.8560)  dino_global_crops_loss: 1.2123 (1.2319)  koleo_loss: -0.0085 (0.1170)  ibot_loss: 2.6576 (2.7564)  time: 2.147579  data: 1.522611  max mem: 33123
I20241029 21:19:46 2766422 dinov2 helpers.py:102] Training  [   510/125000]  eta: 3 days, 1:31:02  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5791 (13.9534)  dino_local_crops_loss: 9.6990 (9.8531)  dino_global_crops_loss: 1.2128 (1.2315)  koleo_loss: -0.0079 (0.1146)  ibot_loss: 2.6447 (2.7542)  time: 2.251028  data: 1.626656  max mem: 33123
I20241029 21:20:07 2766422 dinov2 helpers.py:102] Training  [   520/125000]  eta: 3 days, 1:28:54  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5601 (13.9458)  dino_local_crops_loss: 9.6977 (9.8502)  dino_global_crops_loss: 1.2075 (1.2311)  koleo_loss: -0.0078 (0.1122)  ibot_loss: 2.6542 (2.7524)  time: 2.154156  data: 1.532064  max mem: 33123
I20241029 21:20:27 2766422 dinov2 helpers.py:102] Training  [   530/125000]  eta: 3 days, 1:24:00  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5784 (13.9392)  dino_local_crops_loss: 9.7063 (9.8478)  dino_global_crops_loss: 1.2081 (1.2307)  koleo_loss: -0.0084 (0.1100)  ibot_loss: 2.6629 (2.7507)  time: 2.044947  data: 1.422439  max mem: 33123
I20241029 21:20:47 2766422 dinov2 helpers.py:102] Training  [   540/125000]  eta: 3 days, 1:19:09  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5875 (13.9326)  dino_local_crops_loss: 9.7089 (9.8452)  dino_global_crops_loss: 1.2130 (1.2304)  koleo_loss: -0.0083 (0.1078)  ibot_loss: 2.6657 (2.7492)  time: 2.007181  data: 1.382844  max mem: 33124
I20241029 21:21:08 2766422 dinov2 helpers.py:102] Training  [   550/125000]  eta: 3 days, 1:19:54  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5817 (13.9260)  dino_local_crops_loss: 9.7053 (9.8428)  dino_global_crops_loss: 1.2100 (1.2300)  koleo_loss: -0.0088 (0.1057)  ibot_loss: 2.6657 (2.7476)  time: 2.077873  data: 1.452814  max mem: 33124
I20241029 21:21:31 2766422 dinov2 helpers.py:102] Training  [   560/125000]  eta: 3 days, 1:24:35  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5521 (13.9194)  dino_local_crops_loss: 9.6830 (9.8398)  dino_global_crops_loss: 1.2063 (1.2296)  koleo_loss: -0.0088 (0.1036)  ibot_loss: 2.6806 (2.7465)  time: 2.203942  data: 1.580102  max mem: 33124
I20241029 21:21:52 2766422 dinov2 helpers.py:102] Training  [   570/125000]  eta: 3 days, 1:24:07  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5596 (13.9135)  dino_local_crops_loss: 9.6793 (9.8373)  dino_global_crops_loss: 1.2063 (1.2292)  koleo_loss: -0.0088 (0.1016)  ibot_loss: 2.6847 (2.7454)  time: 2.189078  data: 1.565576  max mem: 33124
I20241029 21:22:11 2766422 dinov2 helpers.py:102] Training  [   580/125000]  eta: 3 days, 1:14:17  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5589 (13.9072)  dino_local_crops_loss: 9.6791 (9.8344)  dino_global_crops_loss: 1.2063 (1.2288)  koleo_loss: -0.0090 (0.0997)  ibot_loss: 2.6828 (2.7443)  time: 1.989296  data: 1.365215  max mem: 33124
I20241029 21:22:31 2766422 dinov2 helpers.py:102] Training  [   590/125000]  eta: 3 days, 1:11:21  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5550 (13.9013)  dino_local_crops_loss: 9.6766 (9.8317)  dino_global_crops_loss: 1.2058 (1.2284)  koleo_loss: -0.0091 (0.0979)  ibot_loss: 2.6845 (2.7433)  time: 1.951774  data: 1.326566  max mem: 33124
I20241029 21:22:51 2766422 dinov2 helpers.py:102] Training  [   600/125000]  eta: 3 days, 1:06:55  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5523 (13.8955)  dino_local_crops_loss: 9.6728 (9.8290)  dino_global_crops_loss: 1.2068 (1.2281)  koleo_loss: -0.0098 (0.0961)  ibot_loss: 2.6875 (2.7424)  time: 2.022593  data: 1.396480  max mem: 33124
I20241029 21:23:11 2766422 dinov2 helpers.py:102] Training  [   610/125000]  eta: 3 days, 1:03:44  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5528 (13.8901)  dino_local_crops_loss: 9.6683 (9.8265)  dino_global_crops_loss: 1.2072 (1.2277)  koleo_loss: -0.0098 (0.0944)  ibot_loss: 2.6898 (2.7415)  time: 2.015989  data: 1.390362  max mem: 33124
I20241029 21:23:34 2766422 dinov2 helpers.py:102] Training  [   620/125000]  eta: 3 days, 1:09:18  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5541 (13.8843)  dino_local_crops_loss: 9.6688 (9.8239)  dino_global_crops_loss: 1.2066 (1.2274)  koleo_loss: -0.0100 (0.0927)  ibot_loss: 2.6846 (2.7404)  time: 2.162179  data: 1.537123  max mem: 33124
I20241029 21:23:58 2766422 dinov2 helpers.py:102] Training  [   630/125000]  eta: 3 days, 1:16:42  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5400 (13.8790)  dino_local_crops_loss: 9.6708 (9.8216)  dino_global_crops_loss: 1.2030 (1.2270)  koleo_loss: -0.0106 (0.0911)  ibot_loss: 2.6756 (2.7393)  time: 2.322649  data: 1.698775  max mem: 33124
I20241029 21:24:24 2766422 dinov2 helpers.py:102] Training  [   640/125000]  eta: 3 days, 1:31:53  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5353 (13.8739)  dino_local_crops_loss: 9.6687 (9.8190)  dino_global_crops_loss: 1.2052 (1.2267)  koleo_loss: -0.0100 (0.0895)  ibot_loss: 2.6846 (2.7387)  time: 2.477447  data: 1.853964  max mem: 33124
I20241029 21:24:46 2766422 dinov2 helpers.py:102] Training  [   650/125000]  eta: 3 days, 1:33:07  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5316 (13.8685)  dino_local_crops_loss: 9.6598 (9.8166)  dino_global_crops_loss: 1.2026 (1.2263)  koleo_loss: -0.0096 (0.0880)  ibot_loss: 2.6846 (2.7377)  time: 2.390240  data: 1.767760  max mem: 33124
I20241029 21:25:10 2766422 dinov2 helpers.py:102] Training  [   660/125000]  eta: 3 days, 1:40:45  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5222 (13.8636)  dino_local_crops_loss: 9.6547 (9.8143)  dino_global_crops_loss: 1.2038 (1.2261)  koleo_loss: -0.0102 (0.0865)  ibot_loss: 2.6777 (2.7368)  time: 2.281278  data: 1.659583  max mem: 33124
I20241029 21:25:29 2766422 dinov2 helpers.py:102] Training  [   670/125000]  eta: 3 days, 1:33:23  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5273 (13.8585)  dino_local_crops_loss: 9.6525 (9.8117)  dino_global_crops_loss: 1.2041 (1.2257)  koleo_loss: -0.0108 (0.0850)  ibot_loss: 2.6794 (2.7360)  time: 2.144968  data: 1.523068  max mem: 33124
I20241029 21:25:50 2766422 dinov2 helpers.py:102] Training  [   680/125000]  eta: 3 days, 1:32:42  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5273 (13.8534)  dino_local_crops_loss: 9.6529 (9.8094)  dino_global_crops_loss: 1.2049 (1.2254)  koleo_loss: -0.0113 (0.0836)  ibot_loss: 2.6758 (2.7350)  time: 2.012578  data: 1.388981  max mem: 33124
I20241029 21:26:11 2766422 dinov2 helpers.py:102] Training  [   690/125000]  eta: 3 days, 1:33:19  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5163 (13.8486)  dino_local_crops_loss: 9.6537 (9.8071)  dino_global_crops_loss: 1.2061 (1.2251)  koleo_loss: -0.0112 (0.0822)  ibot_loss: 2.6726 (2.7341)  time: 2.140795  data: 1.515579  max mem: 33124
I20241029 21:26:32 2766422 dinov2 helpers.py:102] Training  [   700/125000]  eta: 3 days, 1:30:01  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5216 (13.8438)  dino_local_crops_loss: 9.6488 (9.8047)  dino_global_crops_loss: 1.2038 (1.2248)  koleo_loss: -0.0109 (0.0809)  ibot_loss: 2.6763 (2.7334)  time: 2.096196  data: 1.472199  max mem: 33124
I20241029 21:26:52 2766422 dinov2 helpers.py:102] Training  [   710/125000]  eta: 3 days, 1:26:25  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5053 (13.8388)  dino_local_crops_loss: 9.6385 (9.8023)  dino_global_crops_loss: 1.2022 (1.2245)  koleo_loss: -0.0116 (0.0796)  ibot_loss: 2.6736 (2.7325)  time: 2.023728  data: 1.399462  max mem: 33124
I20241029 21:27:14 2766422 dinov2 helpers.py:102] Training  [   720/125000]  eta: 3 days, 1:28:37  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4815 (13.8340)  dino_local_crops_loss: 9.6265 (9.7997)  dino_global_crops_loss: 1.1978 (1.2241)  koleo_loss: -0.0117 (0.0783)  ibot_loss: 2.6736 (2.7319)  time: 2.116719  data: 1.490897  max mem: 33124
I20241029 21:27:35 2766422 dinov2 helpers.py:102] Training  [   730/125000]  eta: 3 days, 1:28:05  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4689 (13.8291)  dino_local_crops_loss: 9.5941 (9.7973)  dino_global_crops_loss: 1.1976 (1.2237)  koleo_loss: -0.0117 (0.0771)  ibot_loss: 2.6741 (2.7310)  time: 2.169226  data: 1.544434  max mem: 33124
I20241029 21:27:53 2766422 dinov2 helpers.py:102] Training  [   740/125000]  eta: 3 days, 1:17:25  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4799 (13.8246)  dino_local_crops_loss: 9.6290 (9.7950)  dino_global_crops_loss: 1.2006 (1.2234)  koleo_loss: -0.0117 (0.0759)  ibot_loss: 2.6699 (2.7303)  time: 1.940904  data: 1.317093  max mem: 33124
I20241029 21:28:16 2766422 dinov2 helpers.py:102] Training  [   750/125000]  eta: 3 days, 1:21:17  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4740 (13.8198)  dino_local_crops_loss: 9.6093 (9.7924)  dino_global_crops_loss: 1.1982 (1.2231)  koleo_loss: -0.0123 (0.0747)  ibot_loss: 2.6814 (2.7296)  time: 2.017667  data: 1.392326  max mem: 33124
I20241029 21:28:35 2766422 dinov2 helpers.py:102] Training  [   760/125000]  eta: 3 days, 1:16:48  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4679 (13.8153)  dino_local_crops_loss: 9.6056 (9.7901)  dino_global_crops_loss: 1.1957 (1.2227)  koleo_loss: -0.0119 (0.0736)  ibot_loss: 2.6799 (2.7290)  time: 2.125000  data: 1.498997  max mem: 33124
I20241029 21:28:55 2766422 dinov2 helpers.py:102] Training  [   770/125000]  eta: 3 days, 1:12:23  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4528 (13.8106)  dino_local_crops_loss: 9.5993 (9.7875)  dino_global_crops_loss: 1.1964 (1.2224)  koleo_loss: -0.0116 (0.0725)  ibot_loss: 2.6788 (2.7283)  time: 1.973036  data: 1.347679  max mem: 33124
I20241029 21:29:15 2766422 dinov2 helpers.py:102] Training  [   780/125000]  eta: 3 days, 1:07:05  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4509 (13.8063)  dino_local_crops_loss: 9.5993 (9.7851)  dino_global_crops_loss: 1.1966 (1.2220)  koleo_loss: -0.0123 (0.0714)  ibot_loss: 2.6870 (2.7278)  time: 1.953264  data: 1.327971  max mem: 33124
I20241029 21:29:35 2766422 dinov2 helpers.py:102] Training  [   790/125000]  eta: 3 days, 1:04:54  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4595 (13.8016)  dino_local_crops_loss: 9.5760 (9.7823)  dino_global_crops_loss: 1.1923 (1.2216)  koleo_loss: -0.0120 (0.0704)  ibot_loss: 2.6870 (2.7273)  time: 1.991797  data: 1.366106  max mem: 33124
I20241029 21:29:55 2766422 dinov2 helpers.py:102] Training  [   800/125000]  eta: 3 days, 1:02:34  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4300 (13.7972)  dino_local_crops_loss: 9.5722 (9.7797)  dino_global_crops_loss: 1.1904 (1.2213)  koleo_loss: -0.0121 (0.0693)  ibot_loss: 2.6927 (2.7269)  time: 2.045507  data: 1.419897  max mem: 33124
I20241029 21:30:16 2766422 dinov2 helpers.py:102] Training  [   810/125000]  eta: 3 days, 1:00:28  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4129 (13.7924)  dino_local_crops_loss: 9.5415 (9.7767)  dino_global_crops_loss: 1.1865 (1.2209)  koleo_loss: -0.0127 (0.0683)  ibot_loss: 2.6959 (2.7265)  time: 2.045354  data: 1.419615  max mem: 33124
I20241029 21:30:39 2766422 dinov2 helpers.py:102] Training  [   820/125000]  eta: 3 days, 1:04:21  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4262 (13.7882)  dino_local_crops_loss: 9.5422 (9.7741)  dino_global_crops_loss: 1.1904 (1.2205)  koleo_loss: -0.0123 (0.0673)  ibot_loss: 2.6959 (2.7262)  time: 2.166446  data: 1.540546  max mem: 33124
I20241029 21:31:00 2766422 dinov2 helpers.py:102] Training  [   830/125000]  eta: 3 days, 1:03:03  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4461 (13.7838)  dino_local_crops_loss: 9.5564 (9.7713)  dino_global_crops_loss: 1.1929 (1.2202)  koleo_loss: -0.0122 (0.0664)  ibot_loss: 2.6989 (2.7259)  time: 2.182198  data: 1.556600  max mem: 33124
I20241029 21:31:21 2766422 dinov2 helpers.py:102] Training  [   840/125000]  eta: 3 days, 1:02:12  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4374 (13.7798)  dino_local_crops_loss: 9.5358 (9.7687)  dino_global_crops_loss: 1.1921 (1.2198)  koleo_loss: -0.0125 (0.0654)  ibot_loss: 2.7066 (2.7258)  time: 2.089178  data: 1.463298  max mem: 33124
I20241029 21:31:40 2766422 dinov2 helpers.py:102] Training  [   850/125000]  eta: 3 days, 0:58:31  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4571 (13.7760)  dino_local_crops_loss: 9.5513 (9.7663)  dino_global_crops_loss: 1.1897 (1.2195)  koleo_loss: -0.0122 (0.0645)  ibot_loss: 2.7066 (2.7257)  time: 2.039389  data: 1.413266  max mem: 33124
I20241029 21:32:00 2766422 dinov2 helpers.py:102] Training  [   860/125000]  eta: 3 days, 0:55:43  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4574 (13.7724)  dino_local_crops_loss: 9.5565 (9.7640)  dino_global_crops_loss: 1.1904 (1.2192)  koleo_loss: -0.0124 (0.0636)  ibot_loss: 2.7218 (2.7256)  time: 1.997286  data: 1.370719  max mem: 33124
I20241029 21:32:21 2766422 dinov2 helpers.py:102] Training  [   870/125000]  eta: 3 days, 0:54:44  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4514 (13.7685)  dino_local_crops_loss: 9.5539 (9.7613)  dino_global_crops_loss: 1.1894 (1.2188)  koleo_loss: -0.0128 (0.0627)  ibot_loss: 2.7218 (2.7256)  time: 2.051112  data: 1.424645  max mem: 33124
I20241029 21:32:41 2766422 dinov2 helpers.py:102] Training  [   880/125000]  eta: 3 days, 0:51:48  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4442 (13.7649)  dino_local_crops_loss: 9.5450 (9.7588)  dino_global_crops_loss: 1.1887 (1.2185)  koleo_loss: -0.0129 (0.0619)  ibot_loss: 2.7273 (2.7257)  time: 2.046558  data: 1.422004  max mem: 33124
I20241029 21:33:03 2766422 dinov2 helpers.py:102] Training  [   890/125000]  eta: 3 days, 0:52:56  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4623 (13.7616)  dino_local_crops_loss: 9.5671 (9.7567)  dino_global_crops_loss: 1.1913 (1.2182)  koleo_loss: -0.0127 (0.0611)  ibot_loss: 2.7198 (2.7256)  time: 2.091122  data: 1.467630  max mem: 33124
I20241029 21:33:26 2766422 dinov2 helpers.py:102] Training  [   900/125000]  eta: 3 days, 0:55:18  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4472 (13.7577)  dino_local_crops_loss: 9.5567 (9.7543)  dino_global_crops_loss: 1.1889 (1.2179)  koleo_loss: -0.0130 (0.0602)  ibot_loss: 2.7149 (2.7254)  time: 2.204873  data: 1.582193  max mem: 33124
I20241029 21:33:47 2766422 dinov2 helpers.py:102] Training  [   910/125000]  eta: 3 days, 0:54:36  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4275 (13.7542)  dino_local_crops_loss: 9.5436 (9.7521)  dino_global_crops_loss: 1.1881 (1.2176)  koleo_loss: -0.0128 (0.0594)  ibot_loss: 2.7077 (2.7251)  time: 2.166346  data: 1.545249  max mem: 33124
I20241029 21:34:08 2766422 dinov2 helpers.py:102] Training  [   920/125000]  eta: 3 days, 0:55:43  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4221 (13.7506)  dino_local_crops_loss: 9.5368 (9.7497)  dino_global_crops_loss: 1.1884 (1.2172)  koleo_loss: -0.0128 (0.0586)  ibot_loss: 2.7042 (2.7250)  time: 2.140610  data: 1.519366  max mem: 33124
I20241029 21:34:27 2766422 dinov2 helpers.py:102] Training  [   930/125000]  eta: 3 days, 0:50:52  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4123 (13.7469)  dino_local_crops_loss: 9.5317 (9.7474)  dino_global_crops_loss: 1.1910 (1.2170)  koleo_loss: -0.0129 (0.0579)  ibot_loss: 2.7031 (2.7246)  time: 2.046852  data: 1.424018  max mem: 33124
I20241029 21:34:48 2766422 dinov2 helpers.py:102] Training  [   940/125000]  eta: 3 days, 0:48:39  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4001 (13.7432)  dino_local_crops_loss: 9.5453 (9.7454)  dino_global_crops_loss: 1.1913 (1.2167)  koleo_loss: -0.0125 (0.0571)  ibot_loss: 2.6766 (2.7240)  time: 1.971124  data: 1.346018  max mem: 33124
I20241029 21:35:07 2766422 dinov2 helpers.py:102] Training  [   950/125000]  eta: 3 days, 0:43:18  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3898 (13.7395)  dino_local_crops_loss: 9.5465 (9.7433)  dino_global_crops_loss: 1.1888 (1.2164)  koleo_loss: -0.0122 (0.0564)  ibot_loss: 2.6692 (2.7234)  time: 1.956038  data: 1.329976  max mem: 33124
I20241029 21:35:30 2766422 dinov2 helpers.py:102] Training  [   960/125000]  eta: 3 days, 0:48:21  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3504 (13.7354)  dino_local_crops_loss: 9.5170 (9.7408)  dino_global_crops_loss: 1.1855 (1.2161)  koleo_loss: -0.0126 (0.0557)  ibot_loss: 2.6710 (2.7228)  time: 2.122015  data: 1.496412  max mem: 33124
I20241029 21:35:51 2766422 dinov2 helpers.py:102] Training  [   970/125000]  eta: 3 days, 0:47:25  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3362 (13.7315)  dino_local_crops_loss: 9.5149 (9.7386)  dino_global_crops_loss: 1.1842 (1.2158)  koleo_loss: -0.0128 (0.0550)  ibot_loss: 2.6669 (2.7222)  time: 2.223566  data: 1.598167  max mem: 33124
I20241029 21:36:10 2766422 dinov2 helpers.py:102] Training  [   980/125000]  eta: 3 days, 0:42:44  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3322 (13.7274)  dino_local_crops_loss: 9.5144 (9.7362)  dino_global_crops_loss: 1.1851 (1.2155)  koleo_loss: -0.0128 (0.0543)  ibot_loss: 2.6492 (2.7214)  time: 1.996442  data: 1.371234  max mem: 33124
I20241029 21:36:30 2766422 dinov2 helpers.py:102] Training  [   990/125000]  eta: 3 days, 0:39:19  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3387 (13.7236)  dino_local_crops_loss: 9.5081 (9.7341)  dino_global_crops_loss: 1.1864 (1.2152)  koleo_loss: -0.0132 (0.0536)  ibot_loss: 2.6492 (2.7208)  time: 1.935532  data: 1.309528  max mem: 33124
I20241029 21:36:50 2766422 dinov2 helpers.py:102] Training  [  1000/125000]  eta: 3 days, 0:36:22  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3463 (13.7198)  dino_local_crops_loss: 9.5081 (9.7318)  dino_global_crops_loss: 1.1856 (1.2149)  koleo_loss: -0.0132 (0.0529)  ibot_loss: 2.6633 (2.7202)  time: 1.973651  data: 1.346660  max mem: 33124
I20241029 21:37:09 2766422 dinov2 helpers.py:102] Training  [  1010/125000]  eta: 3 days, 0:33:12  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3200 (13.7157)  dino_local_crops_loss: 9.4972 (9.7296)  dino_global_crops_loss: 1.1855 (1.2146)  koleo_loss: -0.0139 (0.0523)  ibot_loss: 2.6416 (2.7193)  time: 1.976885  data: 1.349103  max mem: 33124
I20241029 21:37:29 2766422 dinov2 helpers.py:102] Training  [  1020/125000]  eta: 3 days, 0:28:56  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3120 (13.7117)  dino_local_crops_loss: 9.5064 (9.7272)  dino_global_crops_loss: 1.1833 (1.2142)  koleo_loss: -0.0140 (0.0516)  ibot_loss: 2.6439 (2.7186)  time: 1.941672  data: 1.314298  max mem: 33124
I20241029 21:37:49 2766422 dinov2 helpers.py:102] Training  [  1030/125000]  eta: 3 days, 0:26:34  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2937 (13.7075)  dino_local_crops_loss: 9.5009 (9.7249)  dino_global_crops_loss: 1.1795 (1.2139)  koleo_loss: -0.0139 (0.0510)  ibot_loss: 2.6439 (2.7177)  time: 1.958438  data: 1.331606  max mem: 33124
I20241029 21:38:12 2766422 dinov2 helpers.py:102] Training  [  1040/125000]  eta: 3 days, 0:30:37  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2670 (13.7032)  dino_local_crops_loss: 9.4792 (9.7224)  dino_global_crops_loss: 1.1772 (1.2136)  koleo_loss: -0.0141 (0.0504)  ibot_loss: 2.6252 (2.7169)  time: 2.164782  data: 1.538078  max mem: 33124
I20241029 21:38:31 2766422 dinov2 helpers.py:102] Training  [  1050/125000]  eta: 3 days, 0:26:09  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2628 (13.6992)  dino_local_crops_loss: 9.4641 (9.7201)  dino_global_crops_loss: 1.1772 (1.2133)  koleo_loss: -0.0143 (0.0498)  ibot_loss: 2.6297 (2.7161)  time: 2.111022  data: 1.485064  max mem: 33124
I20241029 21:38:52 2766422 dinov2 helpers.py:102] Training  [  1060/125000]  eta: 3 days, 0:26:46  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2681 (13.6950)  dino_local_crops_loss: 9.4659 (9.7177)  dino_global_crops_loss: 1.1785 (1.2129)  koleo_loss: -0.0141 (0.0492)  ibot_loss: 2.6156 (2.7152)  time: 2.025043  data: 1.398624  max mem: 33124
I20241029 21:39:12 2766422 dinov2 helpers.py:102] Training  [  1070/125000]  eta: 3 days, 0:24:40  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2430 (13.6908)  dino_local_crops_loss: 9.4659 (9.7153)  dino_global_crops_loss: 1.1769 (1.2126)  koleo_loss: -0.0143 (0.0486)  ibot_loss: 2.6153 (2.7143)  time: 2.083617  data: 1.456338  max mem: 33124
I20241029 21:39:34 2766422 dinov2 helpers.py:102] Training  [  1080/125000]  eta: 3 days, 0:25:29  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2165 (13.6862)  dino_local_crops_loss: 9.4449 (9.7126)  dino_global_crops_loss: 1.1735 (1.2122)  koleo_loss: -0.0148 (0.0480)  ibot_loss: 2.6232 (2.7134)  time: 2.089160  data: 1.460283  max mem: 33124
I20241029 21:39:54 2766422 dinov2 helpers.py:102] Training  [  1090/125000]  eta: 3 days, 0:23:18  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2014 (13.6817)  dino_local_crops_loss: 9.4159 (9.7099)  dino_global_crops_loss: 1.1702 (1.2118)  koleo_loss: -0.0151 (0.0474)  ibot_loss: 2.6141 (2.7126)  time: 2.085585  data: 1.455600  max mem: 33124
I20241029 21:40:16 2766422 dinov2 helpers.py:102] Training  [  1100/125000]  eta: 3 days, 0:24:58  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1973 (13.6773)  dino_local_crops_loss: 9.4301 (9.7073)  dino_global_crops_loss: 1.1699 (1.2114)  koleo_loss: -0.0147 (0.0468)  ibot_loss: 2.6142 (2.7117)  time: 2.108765  data: 1.479584  max mem: 33124
I20241029 21:40:36 2766422 dinov2 helpers.py:102] Training  [  1110/125000]  eta: 3 days, 0:22:43  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2130 (13.6731)  dino_local_crops_loss: 9.4235 (9.7047)  dino_global_crops_loss: 1.1717 (1.2111)  koleo_loss: -0.0147 (0.0463)  ibot_loss: 2.6196 (2.7110)  time: 2.106496  data: 1.476211  max mem: 33124
I20241029 21:41:01 2766422 dinov2 helpers.py:102] Training  [  1120/125000]  eta: 3 days, 0:28:14  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1728 (13.6684)  dino_local_crops_loss: 9.4001 (9.7018)  dino_global_crops_loss: 1.1694 (1.2107)  koleo_loss: -0.0149 (0.0457)  ibot_loss: 2.6281 (2.7102)  time: 2.211677  data: 1.580314  max mem: 33124
I20241029 21:41:21 2766422 dinov2 helpers.py:102] Training  [  1130/125000]  eta: 3 days, 0:26:19  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1400 (13.6636)  dino_local_crops_loss: 9.3733 (9.6988)  dino_global_crops_loss: 1.1649 (1.2103)  koleo_loss: -0.0148 (0.0452)  ibot_loss: 2.6174 (2.7093)  time: 2.220888  data: 1.591005  max mem: 33124
I20241029 21:41:39 2766422 dinov2 helpers.py:102] Training  [  1140/125000]  eta: 3 days, 0:21:31  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0880 (13.6583)  dino_local_crops_loss: 9.3190 (9.6952)  dino_global_crops_loss: 1.1585 (1.2098)  koleo_loss: -0.0149 (0.0447)  ibot_loss: 2.6174 (2.7086)  time: 1.940013  data: 1.311735  max mem: 33124
I20241029 21:41:58 2766422 dinov2 helpers.py:102] Training  [  1150/125000]  eta: 3 days, 0:16:25  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0026 (13.6520)  dino_local_crops_loss: 9.2374 (9.6909)  dino_global_crops_loss: 1.1500 (1.2092)  koleo_loss: -0.0152 (0.0442)  ibot_loss: 2.6113 (2.7078)  time: 1.848921  data: 1.219712  max mem: 33124
I20241029 21:42:19 2766422 dinov2 helpers.py:102] Training  [  1160/125000]  eta: 3 days, 0:17:11  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8224 (13.6438)  dino_local_crops_loss: 9.0881 (9.6848)  dino_global_crops_loss: 1.1321 (1.2084)  koleo_loss: -0.0153 (0.0436)  ibot_loss: 2.6057 (2.7069)  time: 2.000735  data: 1.370530  max mem: 33124
I20241029 21:42:41 2766422 dinov2 helpers.py:102] Training  [  1170/125000]  eta: 3 days, 0:17:55  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5718 (13.6332)  dino_local_crops_loss: 8.8964 (9.6768)  dino_global_crops_loss: 1.1074 (1.2074)  koleo_loss: -0.0153 (0.0432)  ibot_loss: 2.5985 (2.7059)  time: 2.163366  data: 1.532450  max mem: 33124
I20241029 21:43:04 2766422 dinov2 helpers.py:102] Training  [  1180/125000]  eta: 3 days, 0:21:02  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1250 (13.6185)  dino_local_crops_loss: 8.4922 (9.6649)  dino_global_crops_loss: 1.0601 (1.2059)  koleo_loss: -0.0148 (0.0427)  ibot_loss: 2.6002 (2.7050)  time: 2.231482  data: 1.601201  max mem: 33124
I20241029 21:43:25 2766422 dinov2 helpers.py:102] Training  [  1190/125000]  eta: 3 days, 0:20:18  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.7460 (13.6021)  dino_local_crops_loss: 8.1490 (9.6518)  dino_global_crops_loss: 1.0152 (1.2042)  koleo_loss: -0.0150 (0.0422)  ibot_loss: 2.6006 (2.7040)  time: 2.190685  data: 1.562856  max mem: 33124
I20241029 21:43:43 2766422 dinov2 helpers.py:102] Training  [  1200/125000]  eta: 3 days, 0:15:48  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4838 (13.5831)  dino_local_crops_loss: 7.9645 (9.6361)  dino_global_crops_loss: 0.9919 (1.2022)  koleo_loss: -0.0154 (0.0417)  ibot_loss: 2.5978 (2.7031)  time: 1.971958  data: 1.343747  max mem: 33124
I20241029 21:44:05 2766422 dinov2 helpers.py:102] Training  [  1210/125000]  eta: 3 days, 0:15:56  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.1166 (13.5629)  dino_local_crops_loss: 7.6024 (9.6196)  dino_global_crops_loss: 0.9511 (1.2001)  koleo_loss: -0.0155 (0.0412)  ibot_loss: 2.5783 (2.7020)  time: 1.995904  data: 1.366193  max mem: 33124
I20241029 21:44:26 2766422 dinov2 helpers.py:102] Training  [  1220/125000]  eta: 3 days, 0:15:57  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.0196 (13.5408)  dino_local_crops_loss: 7.5141 (9.6012)  dino_global_crops_loss: 0.9372 (1.1978)  koleo_loss: -0.0157 (0.0408)  ibot_loss: 2.5768 (2.7011)  time: 2.126475  data: 1.497257  max mem: 33124
I20241029 21:44:47 2766422 dinov2 helpers.py:102] Training  [  1230/125000]  eta: 3 days, 0:15:44  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 10.8138 (13.5187)  dino_local_crops_loss: 7.3336 (9.5829)  dino_global_crops_loss: 0.9106 (1.1955)  koleo_loss: -0.0153 (0.0403)  ibot_loss: 2.5836 (2.7000)  time: 2.116393  data: 1.487471  max mem: 33124
I20241029 21:45:07 2766422 dinov2 helpers.py:102] Training  [  1240/125000]  eta: 3 days, 0:14:10  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 10.7774 (13.4966)  dino_local_crops_loss: 7.2967 (9.5645)  dino_global_crops_loss: 0.9023 (1.1931)  koleo_loss: -0.0154 (0.0399)  ibot_loss: 2.5836 (2.6992)  time: 2.069384  data: 1.440030  max mem: 33124
I20241029 21:45:30 2766422 dinov2 helpers.py:102] Training  [  1250/125000]  eta: 3 days, 0:15:49  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 10.8961 (13.4764)  dino_local_crops_loss: 7.3681 (9.5475)  dino_global_crops_loss: 0.9150 (1.1910)  koleo_loss: -0.0154 (0.0394)  ibot_loss: 2.6077 (2.6986)  time: 2.125840  data: 1.496734  max mem: 33124
I20241029 21:45:49 2766422 dinov2 helpers.py:102] Training  [  1260/125000]  eta: 3 days, 0:13:10  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 10.9324 (13.4565)  dino_local_crops_loss: 7.3956 (9.5307)  dino_global_crops_loss: 0.9196 (1.1889)  koleo_loss: -0.0149 (0.0390)  ibot_loss: 2.6196 (2.6980)  time: 2.092037  data: 1.463095  max mem: 33124
I20241029 21:46:12 2766422 dinov2 helpers.py:102] Training  [  1270/125000]  eta: 3 days, 0:15:53  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.0265 (13.4374)  dino_local_crops_loss: 7.4607 (9.5145)  dino_global_crops_loss: 0.9272 (1.1868)  koleo_loss: -0.0155 (0.0386)  ibot_loss: 2.6350 (2.6976)  time: 2.125428  data: 1.496409  max mem: 33124
I20241029 21:46:34 2766422 dinov2 helpers.py:102] Training  [  1280/125000]  eta: 3 days, 0:16:08  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2173 (13.4216)  dino_local_crops_loss: 7.6220 (9.5011)  dino_global_crops_loss: 0.9496 (1.1852)  koleo_loss: -0.0165 (0.0381)  ibot_loss: 2.6363 (2.6972)  time: 2.214627  data: 1.586046  max mem: 33124
I20241029 21:46:53 2766422 dinov2 helpers.py:102] Training  [  1290/125000]  eta: 3 days, 0:13:33  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8413 (13.4116)  dino_local_crops_loss: 8.2202 (9.4930)  dino_global_crops_loss: 1.0253 (1.1841)  koleo_loss: -0.0169 (0.0377)  ibot_loss: 2.6469 (2.6968)  time: 2.051471  data: 1.423774  max mem: 33124
I20241029 21:47:15 2766422 dinov2 helpers.py:102] Training  [  1300/125000]  eta: 3 days, 0:14:04  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3410 (13.4048)  dino_local_crops_loss: 8.6182 (9.4874)  dino_global_crops_loss: 1.0757 (1.1834)  koleo_loss: -0.0169 (0.0373)  ibot_loss: 2.6620 (2.6966)  time: 2.059883  data: 1.431038  max mem: 33124
I20241029 21:47:36 2766422 dinov2 helpers.py:102] Training  [  1310/125000]  eta: 3 days, 0:14:45  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7834 (13.4009)  dino_local_crops_loss: 8.9883 (9.4845)  dino_global_crops_loss: 1.1197 (1.1831)  koleo_loss: -0.0170 (0.0369)  ibot_loss: 2.6705 (2.6965)  time: 2.162243  data: 1.532476  max mem: 33124
I20241029 21:47:58 2766422 dinov2 helpers.py:102] Training  [  1320/125000]  eta: 3 days, 0:15:28  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9832 (13.3984)  dino_local_crops_loss: 9.1831 (9.4827)  dino_global_crops_loss: 1.1444 (1.1828)  koleo_loss: -0.0174 (0.0365)  ibot_loss: 2.6808 (2.6964)  time: 2.169768  data: 1.540850  max mem: 33124
I20241029 21:48:19 2766422 dinov2 helpers.py:102] Training  [  1330/125000]  eta: 3 days, 0:14:38  lr: 0.0001 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0001 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1255 (13.3966)  dino_local_crops_loss: 9.2965 (9.4815)  dino_global_crops_loss: 1.1562 (1.1827)  koleo_loss: -0.0179 (0.0360)  ibot_loss: 2.6897 (2.6964)  time: 2.121902  data: 1.493626  max mem: 33124
I20241029 21:48:38 2766422 dinov2 helpers.py:102] Training  [  1340/125000]  eta: 3 days, 0:10:52  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1981 (13.3953)  dino_local_crops_loss: 9.3376 (9.4805)  dino_global_crops_loss: 1.1623 (1.1825)  koleo_loss: -0.0181 (0.0356)  ibot_loss: 2.7062 (2.6965)  time: 1.976535  data: 1.348495  max mem: 33124
I20241029 21:48:58 2766422 dinov2 helpers.py:102] Training  [  1350/125000]  eta: 3 days, 0:10:04  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2268 (13.3940)  dino_local_crops_loss: 9.3693 (9.4797)  dino_global_crops_loss: 1.1668 (1.1824)  koleo_loss: -0.0186 (0.0352)  ibot_loss: 2.7157 (2.6967)  time: 1.976121  data: 1.346988  max mem: 33124
I20241029 21:49:21 2766422 dinov2 helpers.py:102] Training  [  1360/125000]  eta: 3 days, 0:11:55  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2406 (13.3931)  dino_local_crops_loss: 9.3805 (9.4790)  dino_global_crops_loss: 1.1677 (1.1823)  koleo_loss: -0.0188 (0.0348)  ibot_loss: 2.7226 (2.6969)  time: 2.158984  data: 1.529751  max mem: 33124
I20241029 21:49:43 2766422 dinov2 helpers.py:102] Training  [  1370/125000]  eta: 3 days, 0:13:11  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2629 (13.3921)  dino_local_crops_loss: 9.3774 (9.4783)  dino_global_crops_loss: 1.1686 (1.1822)  koleo_loss: -0.0188 (0.0344)  ibot_loss: 2.7299 (2.6972)  time: 2.228418  data: 1.601739  max mem: 33124
I20241029 21:50:04 2766422 dinov2 helpers.py:102] Training  [  1380/125000]  eta: 3 days, 0:12:37  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2629 (13.3913)  dino_local_crops_loss: 9.3774 (9.4777)  dino_global_crops_loss: 1.1685 (1.1821)  koleo_loss: -0.0188 (0.0341)  ibot_loss: 2.7331 (2.6974)  time: 2.149246  data: 1.524558  max mem: 33124
I20241029 21:50:22 2766422 dinov2 helpers.py:102] Training  [  1390/125000]  eta: 3 days, 0:08:17  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2519 (13.3901)  dino_local_crops_loss: 9.3677 (9.4767)  dino_global_crops_loss: 1.1658 (1.1820)  koleo_loss: -0.0188 (0.0337)  ibot_loss: 2.7333 (2.6977)  time: 1.961272  data: 1.337071  max mem: 33124
I20241029 21:50:44 2766422 dinov2 helpers.py:102] Training  [  1400/125000]  eta: 3 days, 0:08:32  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2304 (13.3889)  dino_local_crops_loss: 9.3421 (9.4757)  dino_global_crops_loss: 1.1624 (1.1818)  koleo_loss: -0.0187 (0.0333)  ibot_loss: 2.7387 (2.6980)  time: 1.987948  data: 1.362797  max mem: 33124
I20241029 21:51:06 2766422 dinov2 helpers.py:102] Training  [  1410/125000]  eta: 3 days, 0:10:39  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2304 (13.3878)  dino_local_crops_loss: 9.3448 (9.4748)  dino_global_crops_loss: 1.1625 (1.1817)  koleo_loss: -0.0190 (0.0329)  ibot_loss: 2.7397 (2.6983)  time: 2.205470  data: 1.579460  max mem: 33124
I20241029 21:51:29 2766422 dinov2 helpers.py:102] Training  [  1420/125000]  eta: 3 days, 0:13:14  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2236 (13.3866)  dino_local_crops_loss: 9.3382 (9.4738)  dino_global_crops_loss: 1.1609 (1.1815)  koleo_loss: -0.0193 (0.0326)  ibot_loss: 2.7415 (2.6986)  time: 2.287430  data: 1.662645  max mem: 33124
I20241029 21:51:50 2766422 dinov2 helpers.py:102] Training  [  1430/125000]  eta: 3 days, 0:12:39  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1924 (13.3851)  dino_local_crops_loss: 9.3106 (9.4726)  dino_global_crops_loss: 1.1571 (1.1814)  koleo_loss: -0.0194 (0.0322)  ibot_loss: 2.7415 (2.6989)  time: 2.196441  data: 1.572678  max mem: 33124
I20241029 21:52:10 2766422 dinov2 helpers.py:102] Training  [  1440/125000]  eta: 3 days, 0:10:52  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1655 (13.3837)  dino_local_crops_loss: 9.2885 (9.4714)  dino_global_crops_loss: 1.1562 (1.1812)  koleo_loss: -0.0191 (0.0319)  ibot_loss: 2.7435 (2.6992)  time: 2.045437  data: 1.421303  max mem: 33124
I20241029 21:52:34 2766422 dinov2 helpers.py:102] Training  [  1450/125000]  eta: 3 days, 0:13:51  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1655 (13.3823)  dino_local_crops_loss: 9.2878 (9.4702)  dino_global_crops_loss: 1.1575 (1.1810)  koleo_loss: -0.0188 (0.0315)  ibot_loss: 2.7438 (2.6996)  time: 2.170737  data: 1.545420  max mem: 33124
I20241029 21:52:56 2766422 dinov2 helpers.py:102] Training  [  1460/125000]  eta: 3 days, 0:15:06  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1555 (13.3808)  dino_local_crops_loss: 9.2750 (9.4689)  dino_global_crops_loss: 1.1560 (1.1808)  koleo_loss: -0.0188 (0.0312)  ibot_loss: 2.7440 (2.6999)  time: 2.277809  data: 1.652948  max mem: 33124
I20241029 21:53:16 2766422 dinov2 helpers.py:102] Training  [  1470/125000]  eta: 3 days, 0:12:55  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1663 (13.3794)  dino_local_crops_loss: 9.2864 (9.4678)  dino_global_crops_loss: 1.1557 (1.1807)  koleo_loss: -0.0190 (0.0308)  ibot_loss: 2.7426 (2.7002)  time: 2.096301  data: 1.472405  max mem: 33124
I20241029 21:53:35 2766422 dinov2 helpers.py:102] Training  [  1480/125000]  eta: 3 days, 0:10:56  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1907 (13.3781)  dino_local_crops_loss: 9.3030 (9.4667)  dino_global_crops_loss: 1.1560 (1.1805)  koleo_loss: -0.0183 (0.0305)  ibot_loss: 2.7449 (2.7005)  time: 1.981049  data: 1.355989  max mem: 33124
I20241029 21:53:55 2766422 dinov2 helpers.py:102] Training  [  1490/125000]  eta: 3 days, 0:09:04  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1811 (13.3766)  dino_local_crops_loss: 9.2951 (9.4653)  dino_global_crops_loss: 1.1543 (1.1803)  koleo_loss: -0.0183 (0.0302)  ibot_loss: 2.7458 (2.7008)  time: 1.990772  data: 1.364299  max mem: 33124
I20241029 21:54:16 2766422 dinov2 helpers.py:102] Training  [  1500/125000]  eta: 3 days, 0:08:26  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1811 (13.3754)  dino_local_crops_loss: 9.2941 (9.4643)  dino_global_crops_loss: 1.1551 (1.1802)  koleo_loss: -0.0184 (0.0298)  ibot_loss: 2.7494 (2.7011)  time: 2.038481  data: 1.411950  max mem: 33124
I20241029 21:54:37 2766422 dinov2 helpers.py:102] Training  [  1510/125000]  eta: 3 days, 0:07:00  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1833 (13.3740)  dino_local_crops_loss: 9.2941 (9.4631)  dino_global_crops_loss: 1.1561 (1.1800)  koleo_loss: -0.0186 (0.0295)  ibot_loss: 2.7496 (2.7014)  time: 2.052916  data: 1.426683  max mem: 33124
I20241029 21:54:56 2766422 dinov2 helpers.py:102] Training  [  1520/125000]  eta: 3 days, 0:04:23  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1790 (13.3728)  dino_local_crops_loss: 9.2825 (9.4619)  dino_global_crops_loss: 1.1547 (1.1798)  koleo_loss: -0.0186 (0.0292)  ibot_loss: 2.7508 (2.7018)  time: 1.978743  data: 1.351616  max mem: 33124
I20241029 21:55:16 2766422 dinov2 helpers.py:102] Training  [  1530/125000]  eta: 3 days, 0:02:15  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1825 (13.3716)  dino_local_crops_loss: 9.2896 (9.4609)  dino_global_crops_loss: 1.1559 (1.1797)  koleo_loss: -0.0191 (0.0289)  ibot_loss: 2.7515 (2.7021)  time: 1.951789  data: 1.323157  max mem: 33124
I20241029 21:55:37 2766422 dinov2 helpers.py:102] Training  [  1540/125000]  eta: 3 days, 0:03:01  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1740 (13.3704)  dino_local_crops_loss: 9.2845 (9.4599)  dino_global_crops_loss: 1.1551 (1.1795)  koleo_loss: -0.0193 (0.0286)  ibot_loss: 2.7515 (2.7024)  time: 2.076325  data: 1.446936  max mem: 33124
I20241029 21:55:58 2766422 dinov2 helpers.py:102] Training  [  1550/125000]  eta: 3 days, 0:02:16  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1574 (13.3690)  dino_local_crops_loss: 9.2735 (9.4586)  dino_global_crops_loss: 1.1510 (1.1793)  koleo_loss: -0.0194 (0.0283)  ibot_loss: 2.7528 (2.7027)  time: 2.127453  data: 1.499004  max mem: 33124
I20241029 21:56:16 2766422 dinov2 helpers.py:102] Training  [  1560/125000]  eta: 2 days, 23:58:17  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1586 (13.3677)  dino_local_crops_loss: 9.2745 (9.4575)  dino_global_crops_loss: 1.1531 (1.1792)  koleo_loss: -0.0192 (0.0280)  ibot_loss: 2.7540 (2.7031)  time: 1.947842  data: 1.320017  max mem: 33124
I20241029 21:56:36 2766422 dinov2 helpers.py:102] Training  [  1570/125000]  eta: 2 days, 23:55:48  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1982 (13.3667)  dino_local_crops_loss: 9.3075 (9.4566)  dino_global_crops_loss: 1.1566 (1.1790)  koleo_loss: -0.0187 (0.0277)  ibot_loss: 2.7531 (2.7034)  time: 1.880257  data: 1.251411  max mem: 33124
I20241029 21:56:59 2766422 dinov2 helpers.py:102] Training  [  1580/125000]  eta: 2 days, 23:58:29  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2150 (13.3658)  dino_local_crops_loss: 9.3188 (9.4558)  dino_global_crops_loss: 1.1608 (1.1789)  koleo_loss: -0.0187 (0.0274)  ibot_loss: 2.7544 (2.7037)  time: 2.133439  data: 1.503428  max mem: 33124
I20241029 21:57:17 2766422 dinov2 helpers.py:102] Training  [  1590/125000]  eta: 2 days, 23:54:22  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2163 (13.3647)  dino_local_crops_loss: 9.3206 (9.4548)  dino_global_crops_loss: 1.1562 (1.1788)  koleo_loss: -0.0187 (0.0271)  ibot_loss: 2.7566 (2.7041)  time: 2.069836  data: 1.440960  max mem: 33124
I20241029 21:57:39 2766422 dinov2 helpers.py:102] Training  [  1600/125000]  eta: 2 days, 23:55:06  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2026 (13.3637)  dino_local_crops_loss: 9.3147 (9.4539)  dino_global_crops_loss: 1.1562 (1.1786)  koleo_loss: -0.0189 (0.0268)  ibot_loss: 2.7566 (2.7044)  time: 1.994968  data: 1.367069  max mem: 33124
I20241029 21:57:58 2766422 dinov2 helpers.py:102] Training  [  1610/125000]  eta: 2 days, 23:52:43  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1952 (13.3626)  dino_local_crops_loss: 9.3010 (9.4529)  dino_global_crops_loss: 1.1548 (1.1785)  koleo_loss: -0.0194 (0.0265)  ibot_loss: 2.7564 (2.7047)  time: 2.060317  data: 1.432781  max mem: 33124
I20241029 21:58:20 2766422 dinov2 helpers.py:102] Training  [  1620/125000]  eta: 2 days, 23:52:54  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1970 (13.3618)  dino_local_crops_loss: 9.3032 (9.4521)  dino_global_crops_loss: 1.1556 (1.1784)  koleo_loss: -0.0195 (0.0262)  ibot_loss: 2.7571 (2.7051)  time: 2.039073  data: 1.412528  max mem: 33124
I20241029 21:58:42 2766422 dinov2 helpers.py:102] Training  [  1630/125000]  eta: 2 days, 23:54:01  lr: 0.0002 (0.0001)  wd: 0.0401 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2198 (13.3609)  dino_local_crops_loss: 9.3251 (9.4513)  dino_global_crops_loss: 1.1592 (1.1783)  koleo_loss: -0.0191 (0.0259)  ibot_loss: 2.7578 (2.7054)  time: 2.176147  data: 1.549401  max mem: 33124
I20241029 21:59:02 2766422 dinov2 helpers.py:102] Training  [  1640/125000]  eta: 2 days, 23:52:53  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2031 (13.3600)  dino_local_crops_loss: 9.3095 (9.4505)  dino_global_crops_loss: 1.1571 (1.1781)  koleo_loss: -0.0188 (0.0257)  ibot_loss: 2.7582 (2.7057)  time: 2.124539  data: 1.498229  max mem: 33124
I20241029 21:59:23 2766422 dinov2 helpers.py:102] Training  [  1650/125000]  eta: 2 days, 23:52:33  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1813 (13.3590)  dino_local_crops_loss: 9.2881 (9.4496)  dino_global_crops_loss: 1.1530 (1.1780)  koleo_loss: -0.0190 (0.0254)  ibot_loss: 2.7597 (2.7060)  time: 2.067183  data: 1.440926  max mem: 33124
I20241029 21:59:45 2766422 dinov2 helpers.py:102] Training  [  1660/125000]  eta: 2 days, 23:53:08  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1917 (13.3581)  dino_local_crops_loss: 9.2921 (9.4488)  dino_global_crops_loss: 1.1575 (1.1779)  koleo_loss: -0.0191 (0.0251)  ibot_loss: 2.7594 (2.7064)  time: 2.136070  data: 1.509041  max mem: 33124
I20241029 22:00:07 2766422 dinov2 helpers.py:102] Training  [  1670/125000]  eta: 2 days, 23:53:55  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1838 (13.3571)  dino_local_crops_loss: 9.2843 (9.4478)  dino_global_crops_loss: 1.1534 (1.1777)  koleo_loss: -0.0195 (0.0249)  ibot_loss: 2.7591 (2.7067)  time: 2.181855  data: 1.554329  max mem: 33124
I20241029 22:00:29 2766422 dinov2 helpers.py:102] Training  [  1680/125000]  eta: 2 days, 23:54:52  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1838 (13.3563)  dino_local_crops_loss: 9.2882 (9.4470)  dino_global_crops_loss: 1.1551 (1.1776)  koleo_loss: -0.0196 (0.0246)  ibot_loss: 2.7608 (2.7070)  time: 2.197814  data: 1.571213  max mem: 33124
I20241029 22:00:49 2766422 dinov2 helpers.py:102] Training  [  1690/125000]  eta: 2 days, 23:53:04  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2008 (13.3553)  dino_local_crops_loss: 9.3072 (9.4462)  dino_global_crops_loss: 1.1556 (1.1775)  koleo_loss: -0.0196 (0.0243)  ibot_loss: 2.7608 (2.7073)  time: 2.092497  data: 1.466959  max mem: 33124
I20241029 22:01:11 2766422 dinov2 helpers.py:102] Training  [  1700/125000]  eta: 2 days, 23:53:50  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2008 (13.3545)  dino_local_crops_loss: 9.3072 (9.4454)  dino_global_crops_loss: 1.1596 (1.1774)  koleo_loss: -0.0199 (0.0241)  ibot_loss: 2.7602 (2.7076)  time: 2.085651  data: 1.458705  max mem: 33124
I20241029 22:01:31 2766422 dinov2 helpers.py:102] Training  [  1710/125000]  eta: 2 days, 23:52:40  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1996 (13.3535)  dino_local_crops_loss: 9.3016 (9.4445)  dino_global_crops_loss: 1.1573 (1.1772)  koleo_loss: -0.0201 (0.0238)  ibot_loss: 2.7603 (2.7079)  time: 2.111383  data: 1.484146  max mem: 33124
I20241029 22:01:52 2766422 dinov2 helpers.py:102] Training  [  1720/125000]  eta: 2 days, 23:52:30  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1996 (13.3526)  dino_local_crops_loss: 9.3016 (9.4436)  dino_global_crops_loss: 1.1558 (1.1771)  koleo_loss: -0.0197 (0.0236)  ibot_loss: 2.7602 (2.7082)  time: 2.072357  data: 1.446032  max mem: 33124
I20241029 22:02:12 2766422 dinov2 helpers.py:102] Training  [  1730/125000]  eta: 2 days, 23:51:23  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2007 (13.3515)  dino_local_crops_loss: 9.3023 (9.4427)  dino_global_crops_loss: 1.1565 (1.1770)  koleo_loss: -0.0195 (0.0233)  ibot_loss: 2.7602 (2.7085)  time: 2.074124  data: 1.448098  max mem: 33124
I20241029 22:02:34 2766422 dinov2 helpers.py:102] Training  [  1740/125000]  eta: 2 days, 23:51:11  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.2062 (13.3506)  dino_local_crops_loss: 9.3117 (9.4418)  dino_global_crops_loss: 1.1550 (1.1768)  koleo_loss: -0.0199 (0.0231)  ibot_loss: 2.7612 (2.7088)  time: 2.073313  data: 1.446824  max mem: 33124
I20241029 22:02:54 2766422 dinov2 helpers.py:102] Training  [  1750/125000]  eta: 2 days, 23:50:12  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1704 (13.3496)  dino_local_crops_loss: 9.2770 (9.4409)  dino_global_crops_loss: 1.1508 (1.1767)  koleo_loss: -0.0200 (0.0228)  ibot_loss: 2.7610 (2.7091)  time: 2.077706  data: 1.450725  max mem: 33124
I20241029 22:03:15 2766422 dinov2 helpers.py:102] Training  [  1760/125000]  eta: 2 days, 23:49:57  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1556 (13.3484)  dino_local_crops_loss: 9.2686 (9.4398)  dino_global_crops_loss: 1.1498 (1.1765)  koleo_loss: -0.0205 (0.0226)  ibot_loss: 2.7602 (2.7094)  time: 2.075472  data: 1.447752  max mem: 33124
I20241029 22:03:35 2766422 dinov2 helpers.py:102] Training  [  1770/125000]  eta: 2 days, 23:47:59  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1497 (13.3473)  dino_local_crops_loss: 9.2658 (9.4389)  dino_global_crops_loss: 1.1482 (1.1764)  koleo_loss: -0.0206 (0.0223)  ibot_loss: 2.7595 (2.7097)  time: 2.032898  data: 1.404646  max mem: 33124
I20241029 22:03:54 2766422 dinov2 helpers.py:102] Training  [  1780/125000]  eta: 2 days, 23:45:53  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1484 (13.3460)  dino_local_crops_loss: 9.2598 (9.4377)  dino_global_crops_loss: 1.1467 (1.1762)  koleo_loss: -0.0204 (0.0221)  ibot_loss: 2.7608 (2.7100)  time: 1.952249  data: 1.323818  max mem: 33124
I20241029 22:04:15 2766422 dinov2 helpers.py:102] Training  [  1790/125000]  eta: 2 days, 23:45:18  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1211 (13.3449)  dino_local_crops_loss: 9.2318 (9.4367)  dino_global_crops_loss: 1.1477 (1.1761)  koleo_loss: -0.0199 (0.0219)  ibot_loss: 2.7605 (2.7103)  time: 2.010755  data: 1.381289  max mem: 33124
I20241029 22:04:37 2766422 dinov2 helpers.py:102] Training  [  1800/125000]  eta: 2 days, 23:46:05  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1377 (13.3437)  dino_local_crops_loss: 9.2498 (9.4356)  dino_global_crops_loss: 1.1464 (1.1759)  koleo_loss: -0.0206 (0.0216)  ibot_loss: 2.7597 (2.7105)  time: 2.136165  data: 1.507041  max mem: 33124
I20241029 22:04:57 2766422 dinov2 helpers.py:102] Training  [  1810/125000]  eta: 2 days, 23:45:01  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.1129 (13.3424)  dino_local_crops_loss: 9.2307 (9.4344)  dino_global_crops_loss: 1.1451 (1.1757)  koleo_loss: -0.0206 (0.0214)  ibot_loss: 2.7597 (2.7108)  time: 2.114811  data: 1.487122  max mem: 33124
I20241029 22:05:17 2766422 dinov2 helpers.py:102] Training  [  1820/125000]  eta: 2 days, 23:43:26  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0989 (13.3409)  dino_local_crops_loss: 9.2174 (9.4331)  dino_global_crops_loss: 1.1422 (1.1755)  koleo_loss: -0.0206 (0.0212)  ibot_loss: 2.7592 (2.7111)  time: 2.010689  data: 1.382178  max mem: 33124
I20241029 22:05:37 2766422 dinov2 helpers.py:102] Training  [  1830/125000]  eta: 2 days, 23:42:21  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0878 (13.3395)  dino_local_crops_loss: 9.2070 (9.4319)  dino_global_crops_loss: 1.1433 (1.1754)  koleo_loss: -0.0205 (0.0209)  ibot_loss: 2.7590 (2.7113)  time: 2.009124  data: 1.379869  max mem: 33124
I20241029 22:05:58 2766422 dinov2 helpers.py:102] Training  [  1840/125000]  eta: 2 days, 23:41:06  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0856 (13.3381)  dino_local_crops_loss: 9.2036 (9.4307)  dino_global_crops_loss: 1.1433 (1.1752)  koleo_loss: -0.0203 (0.0207)  ibot_loss: 2.7582 (2.7116)  time: 2.022591  data: 1.393339  max mem: 33124
I20241029 22:06:16 2766422 dinov2 helpers.py:102] Training  [  1850/125000]  eta: 2 days, 23:37:59  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0497 (13.3366)  dino_local_crops_loss: 9.1709 (9.4293)  dino_global_crops_loss: 1.1399 (1.1750)  koleo_loss: -0.0204 (0.0205)  ibot_loss: 2.7581 (2.7118)  time: 1.930534  data: 1.301532  max mem: 33124
I20241029 22:06:38 2766422 dinov2 helpers.py:102] Training  [  1860/125000]  eta: 2 days, 23:38:55  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0497 (13.3352)  dino_local_crops_loss: 9.1762 (9.4280)  dino_global_crops_loss: 1.1399 (1.1748)  koleo_loss: -0.0206 (0.0203)  ibot_loss: 2.7577 (2.7121)  time: 2.028524  data: 1.400170  max mem: 33124
I20241029 22:07:02 2766422 dinov2 helpers.py:102] Training  [  1870/125000]  eta: 2 days, 23:41:25  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0474 (13.3335)  dino_local_crops_loss: 9.1736 (9.4265)  dino_global_crops_loss: 1.1361 (1.1746)  koleo_loss: -0.0207 (0.0201)  ibot_loss: 2.7563 (2.7123)  time: 2.282521  data: 1.654699  max mem: 33124
I20241029 22:07:24 2766422 dinov2 helpers.py:102] Training  [  1880/125000]  eta: 2 days, 23:42:33  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0337 (13.3320)  dino_local_crops_loss: 9.1602 (9.4252)  dino_global_crops_loss: 1.1354 (1.1744)  koleo_loss: -0.0208 (0.0198)  ibot_loss: 2.7563 (2.7125)  time: 2.292877  data: 1.665799  max mem: 33124
I20241029 22:07:47 2766422 dinov2 helpers.py:102] Training  [  1890/125000]  eta: 2 days, 23:44:01  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.0337 (13.3303)  dino_local_crops_loss: 9.1602 (9.4237)  dino_global_crops_loss: 1.1354 (1.1742)  koleo_loss: -0.0210 (0.0196)  ibot_loss: 2.7548 (2.7128)  time: 2.247821  data: 1.620691  max mem: 33124
I20241029 22:08:09 2766422 dinov2 helpers.py:102] Training  [  1900/125000]  eta: 2 days, 23:45:14  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9747 (13.3284)  dino_local_crops_loss: 9.1124 (9.4221)  dino_global_crops_loss: 1.1300 (1.1739)  koleo_loss: -0.0206 (0.0194)  ibot_loss: 2.7548 (2.7130)  time: 2.253347  data: 1.626482  max mem: 33124
I20241029 22:08:30 2766422 dinov2 helpers.py:102] Training  [  1910/125000]  eta: 2 days, 23:44:18  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9681 (13.3267)  dino_local_crops_loss: 9.1008 (9.4205)  dino_global_crops_loss: 1.1291 (1.1737)  koleo_loss: -0.0204 (0.0192)  ibot_loss: 2.7551 (2.7132)  time: 2.143706  data: 1.517002  max mem: 33124
I20241029 22:08:50 2766422 dinov2 helpers.py:102] Training  [  1920/125000]  eta: 2 days, 23:43:46  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9596 (13.3247)  dino_local_crops_loss: 9.1000 (9.4188)  dino_global_crops_loss: 1.1282 (1.1735)  koleo_loss: -0.0209 (0.0190)  ibot_loss: 2.7527 (2.7134)  time: 2.062441  data: 1.435167  max mem: 33124
I20241029 22:09:13 2766422 dinov2 helpers.py:102] Training  [  1930/125000]  eta: 2 days, 23:45:37  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9453 (13.3228)  dino_local_crops_loss: 9.0883 (9.4172)  dino_global_crops_loss: 1.1254 (1.1732)  koleo_loss: -0.0208 (0.0188)  ibot_loss: 2.7528 (2.7136)  time: 2.192718  data: 1.564375  max mem: 33124
I20241029 22:09:32 2766422 dinov2 helpers.py:102] Training  [  1940/125000]  eta: 2 days, 23:42:30  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9582 (13.3209)  dino_local_crops_loss: 9.0998 (9.4156)  dino_global_crops_loss: 1.1268 (1.1730)  koleo_loss: -0.0203 (0.0186)  ibot_loss: 2.7532 (2.7138)  time: 2.071094  data: 1.442530  max mem: 33124
I20241029 22:09:55 2766422 dinov2 helpers.py:102] Training  [  1950/125000]  eta: 2 days, 23:44:24  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9414 (13.3189)  dino_local_crops_loss: 9.0858 (9.4138)  dino_global_crops_loss: 1.1246 (1.1727)  koleo_loss: -0.0204 (0.0184)  ibot_loss: 2.7524 (2.7140)  time: 2.074335  data: 1.444991  max mem: 33124
I20241029 22:10:16 2766422 dinov2 helpers.py:102] Training  [  1960/125000]  eta: 2 days, 23:43:52  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9141 (13.3167)  dino_local_crops_loss: 9.0574 (9.4119)  dino_global_crops_loss: 1.1180 (1.1724)  koleo_loss: -0.0206 (0.0182)  ibot_loss: 2.7512 (2.7142)  time: 2.196584  data: 1.567065  max mem: 33124
I20241029 22:10:36 2766422 dinov2 helpers.py:102] Training  [  1970/125000]  eta: 2 days, 23:43:16  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8901 (13.3146)  dino_local_crops_loss: 9.0494 (9.4100)  dino_global_crops_loss: 1.1189 (1.1722)  koleo_loss: -0.0205 (0.0180)  ibot_loss: 2.7514 (2.7144)  time: 2.078088  data: 1.448905  max mem: 33124
I20241029 22:10:58 2766422 dinov2 helpers.py:102] Training  [  1980/125000]  eta: 2 days, 23:43:51  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9109 (13.3128)  dino_local_crops_loss: 9.0616 (9.4085)  dino_global_crops_loss: 1.1213 (1.1719)  koleo_loss: -0.0206 (0.0178)  ibot_loss: 2.7514 (2.7146)  time: 2.131733  data: 1.501614  max mem: 33124
I20241029 22:11:18 2766422 dinov2 helpers.py:102] Training  [  1990/125000]  eta: 2 days, 23:42:28  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9510 (13.3109)  dino_local_crops_loss: 9.0940 (9.4068)  dino_global_crops_loss: 1.1244 (1.1717)  koleo_loss: -0.0205 (0.0176)  ibot_loss: 2.7511 (2.7148)  time: 2.094465  data: 1.464340  max mem: 33124
I20241029 22:11:40 2766422 dinov2 helpers.py:102] Training  [  2000/125000]  eta: 2 days, 23:42:28  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.9084 (13.3087)  dino_local_crops_loss: 9.0590 (9.4049)  dino_global_crops_loss: 1.1180 (1.1714)  koleo_loss: -0.0205 (0.0174)  ibot_loss: 2.7514 (2.7150)  time: 2.066102  data: 1.435310  max mem: 33124
I20241029 22:12:00 2766422 dinov2 helpers.py:102] Training  [  2010/125000]  eta: 2 days, 23:41:17  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8889 (13.3067)  dino_local_crops_loss: 9.0436 (9.4031)  dino_global_crops_loss: 1.1179 (1.1712)  koleo_loss: -0.0210 (0.0172)  ibot_loss: 2.7514 (2.7151)  time: 2.074121  data: 1.442452  max mem: 33124
I20241029 22:12:19 2766422 dinov2 helpers.py:102] Training  [  2020/125000]  eta: 2 days, 23:39:03  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8941 (13.3045)  dino_local_crops_loss: 9.0436 (9.4013)  dino_global_crops_loss: 1.1182 (1.1709)  koleo_loss: -0.0211 (0.0170)  ibot_loss: 2.7492 (2.7153)  time: 1.964457  data: 1.331326  max mem: 33124
I20241029 22:12:42 2766422 dinov2 helpers.py:102] Training  [  2030/125000]  eta: 2 days, 23:40:25  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8941 (13.3025)  dino_local_crops_loss: 9.0462 (9.3996)  dino_global_crops_loss: 1.1182 (1.1706)  koleo_loss: -0.0209 (0.0168)  ibot_loss: 2.7481 (2.7155)  time: 2.090186  data: 1.455610  max mem: 33124
I20241029 22:13:03 2766422 dinov2 helpers.py:102] Training  [  2040/125000]  eta: 2 days, 23:40:24  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8751 (13.3004)  dino_local_crops_loss: 9.0319 (9.3977)  dino_global_crops_loss: 1.1172 (1.1704)  koleo_loss: -0.0208 (0.0167)  ibot_loss: 2.7478 (2.7156)  time: 2.199583  data: 1.565747  max mem: 33124
I20241029 22:13:21 2766422 dinov2 helpers.py:102] Training  [  2050/125000]  eta: 2 days, 23:36:58  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8521 (13.2983)  dino_local_crops_loss: 9.0072 (9.3959)  dino_global_crops_loss: 1.1165 (1.1701)  koleo_loss: -0.0210 (0.0165)  ibot_loss: 2.7486 (2.7158)  time: 1.960544  data: 1.326772  max mem: 33124
I20241029 22:13:41 2766422 dinov2 helpers.py:102] Training  [  2060/125000]  eta: 2 days, 23:35:28  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8727 (13.2964)  dino_local_crops_loss: 9.0305 (9.3942)  dino_global_crops_loss: 1.1174 (1.1699)  koleo_loss: -0.0209 (0.0163)  ibot_loss: 2.7501 (2.7160)  time: 1.885466  data: 1.250669  max mem: 33124
I20241029 22:13:59 2766422 dinov2 helpers.py:102] Training  [  2070/125000]  eta: 2 days, 23:32:04  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8704 (13.2942)  dino_local_crops_loss: 9.0273 (9.3923)  dino_global_crops_loss: 1.1162 (1.1696)  koleo_loss: -0.0207 (0.0161)  ibot_loss: 2.7493 (2.7161)  time: 1.885143  data: 1.249856  max mem: 33124
I20241029 22:14:18 2766422 dinov2 helpers.py:102] Training  [  2080/125000]  eta: 2 days, 23:29:50  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8806 (13.2924)  dino_local_crops_loss: 9.0350 (9.3908)  dino_global_crops_loss: 1.1175 (1.1694)  koleo_loss: -0.0206 (0.0159)  ibot_loss: 2.7477 (2.7163)  time: 1.846310  data: 1.211089  max mem: 33124
I20241029 22:14:36 2766422 dinov2 helpers.py:102] Training  [  2090/125000]  eta: 2 days, 23:27:27  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8772 (13.2903)  dino_local_crops_loss: 9.0350 (9.3890)  dino_global_crops_loss: 1.1175 (1.1691)  koleo_loss: -0.0206 (0.0158)  ibot_loss: 2.7477 (2.7164)  time: 1.894533  data: 1.259658  max mem: 33124
I20241029 22:14:55 2766422 dinov2 helpers.py:102] Training  [  2100/125000]  eta: 2 days, 23:24:38  lr: 0.0002 (0.0001)  wd: 0.0402 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8656 (13.2883)  dino_local_crops_loss: 9.0174 (9.3873)  dino_global_crops_loss: 1.1150 (1.1688)  koleo_loss: -0.0210 (0.0156)  ibot_loss: 2.7503 (2.7166)  time: 1.862752  data: 1.227478  max mem: 33124
I20241029 22:15:13 2766422 dinov2 helpers.py:102] Training  [  2110/125000]  eta: 2 days, 23:21:39  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8671 (13.2863)  dino_local_crops_loss: 9.0252 (9.3855)  dino_global_crops_loss: 1.1143 (1.1686)  koleo_loss: -0.0211 (0.0154)  ibot_loss: 2.7502 (2.7167)  time: 1.830131  data: 1.194148  max mem: 33124
I20241029 22:15:34 2766422 dinov2 helpers.py:102] Training  [  2120/125000]  eta: 2 days, 23:21:29  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8598 (13.2843)  dino_local_crops_loss: 9.0204 (9.3838)  dino_global_crops_loss: 1.1150 (1.1683)  koleo_loss: -0.0213 (0.0152)  ibot_loss: 2.7488 (2.7169)  time: 1.965024  data: 1.329596  max mem: 33124
I20241029 22:15:55 2766422 dinov2 helpers.py:102] Training  [  2130/125000]  eta: 2 days, 23:20:57  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8664 (13.2823)  dino_local_crops_loss: 9.0190 (9.3821)  dino_global_crops_loss: 1.1156 (1.1681)  koleo_loss: -0.0215 (0.0151)  ibot_loss: 2.7488 (2.7170)  time: 2.090270  data: 1.455195  max mem: 33124
I20241029 22:16:17 2766422 dinov2 helpers.py:102] Training  [  2140/125000]  eta: 2 days, 23:21:54  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8664 (13.2804)  dino_local_crops_loss: 9.0260 (9.3804)  dino_global_crops_loss: 1.1169 (1.1679)  koleo_loss: -0.0214 (0.0149)  ibot_loss: 2.7492 (2.7172)  time: 2.148538  data: 1.513130  max mem: 33124
I20241029 22:16:38 2766422 dinov2 helpers.py:102] Training  [  2150/125000]  eta: 2 days, 23:21:41  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8805 (13.2784)  dino_local_crops_loss: 9.0382 (9.3787)  dino_global_crops_loss: 1.1178 (1.1676)  koleo_loss: -0.0214 (0.0147)  ibot_loss: 2.7483 (2.7173)  time: 2.165541  data: 1.530290  max mem: 33127
I20241029 22:16:57 2766422 dinov2 helpers.py:102] Training  [  2160/125000]  eta: 2 days, 23:19:14  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8807 (13.2765)  dino_local_crops_loss: 9.0382 (9.3771)  dino_global_crops_loss: 1.1157 (1.1674)  koleo_loss: -0.0217 (0.0146)  ibot_loss: 2.7476 (2.7175)  time: 1.986911  data: 1.352595  max mem: 33127
I20241029 22:17:20 2766422 dinov2 helpers.py:102] Training  [  2170/125000]  eta: 2 days, 23:21:09  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8837 (13.2746)  dino_local_crops_loss: 9.0369 (9.3754)  dino_global_crops_loss: 1.1174 (1.1671)  koleo_loss: -0.0217 (0.0144)  ibot_loss: 2.7489 (2.7176)  time: 2.100117  data: 1.466170  max mem: 33127
I20241029 22:17:43 2766422 dinov2 helpers.py:102] Training  [  2180/125000]  eta: 2 days, 23:22:19  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8678 (13.2727)  dino_local_crops_loss: 9.0240 (9.3738)  dino_global_crops_loss: 1.1154 (1.1669)  koleo_loss: -0.0217 (0.0142)  ibot_loss: 2.7511 (2.7178)  time: 2.291605  data: 1.657584  max mem: 33127
I20241029 22:18:01 2766422 dinov2 helpers.py:102] Training  [  2190/125000]  eta: 2 days, 23:19:52  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8512 (13.2707)  dino_local_crops_loss: 9.0099 (9.3721)  dino_global_crops_loss: 1.1137 (1.1666)  koleo_loss: -0.0217 (0.0141)  ibot_loss: 2.7520 (2.7179)  time: 2.059966  data: 1.425989  max mem: 33127
I20241029 22:18:22 2766422 dinov2 helpers.py:102] Training  [  2200/125000]  eta: 2 days, 23:18:58  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8594 (13.2689)  dino_local_crops_loss: 9.0199 (9.3705)  dino_global_crops_loss: 1.1129 (1.1664)  koleo_loss: -0.0218 (0.0139)  ibot_loss: 2.7479 (2.7181)  time: 1.949592  data: 1.316434  max mem: 33127
I20241029 22:18:43 2766422 dinov2 helpers.py:102] Training  [  2210/125000]  eta: 2 days, 23:19:15  lr: 0.0002 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0002 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8594 (13.2670)  dino_local_crops_loss: 9.0199 (9.3689)  dino_global_crops_loss: 1.1123 (1.1662)  koleo_loss: -0.0218 (0.0137)  ibot_loss: 2.7481 (2.7182)  time: 2.095603  data: 1.462664  max mem: 33127
I20241029 22:19:03 2766422 dinov2 helpers.py:102] Training  [  2220/125000]  eta: 2 days, 23:17:37  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8504 (13.2652)  dino_local_crops_loss: 9.0067 (9.3673)  dino_global_crops_loss: 1.1133 (1.1659)  koleo_loss: -0.0217 (0.0136)  ibot_loss: 2.7491 (2.7183)  time: 2.055203  data: 1.421274  max mem: 33127
I20241029 22:19:24 2766422 dinov2 helpers.py:102] Training  [  2230/125000]  eta: 2 days, 23:17:44  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8716 (13.2636)  dino_local_crops_loss: 9.0289 (9.3659)  dino_global_crops_loss: 1.1180 (1.1657)  koleo_loss: -0.0216 (0.0134)  ibot_loss: 2.7491 (2.7185)  time: 2.045454  data: 1.411094  max mem: 33127
I20241029 22:19:43 2766422 dinov2 helpers.py:102] Training  [  2240/125000]  eta: 2 days, 23:15:07  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8697 (13.2617)  dino_local_crops_loss: 9.0270 (9.3643)  dino_global_crops_loss: 1.1155 (1.1655)  koleo_loss: -0.0218 (0.0133)  ibot_loss: 2.7475 (2.7186)  time: 1.991258  data: 1.356575  max mem: 33127
I20241029 22:20:04 2766422 dinov2 helpers.py:102] Training  [  2250/125000]  eta: 2 days, 23:14:50  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8642 (13.2599)  dino_local_crops_loss: 9.0249 (9.3628)  dino_global_crops_loss: 1.1149 (1.1653)  koleo_loss: -0.0222 (0.0131)  ibot_loss: 2.7461 (2.7187)  time: 1.970012  data: 1.335642  max mem: 33127
I20241029 22:20:24 2766422 dinov2 helpers.py:102] Training  [  2260/125000]  eta: 2 days, 23:14:04  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8435 (13.2581)  dino_local_crops_loss: 9.0095 (9.3612)  dino_global_crops_loss: 1.1162 (1.1651)  koleo_loss: -0.0222 (0.0130)  ibot_loss: 2.7457 (2.7188)  time: 2.069953  data: 1.435764  max mem: 33127
I20241029 22:20:46 2766422 dinov2 helpers.py:102] Training  [  2270/125000]  eta: 2 days, 23:14:26  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8372 (13.2563)  dino_local_crops_loss: 8.9980 (9.3597)  dino_global_crops_loss: 1.1142 (1.1648)  koleo_loss: -0.0219 (0.0128)  ibot_loss: 2.7444 (2.7190)  time: 2.105970  data: 1.471688  max mem: 33127
I20241029 22:21:05 2766422 dinov2 helpers.py:102] Training  [  2280/125000]  eta: 2 days, 23:12:28  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8446 (13.2544)  dino_local_crops_loss: 9.0102 (9.3581)  dino_global_crops_loss: 1.1125 (1.1646)  koleo_loss: -0.0221 (0.0126)  ibot_loss: 2.7451 (2.7191)  time: 2.039526  data: 1.405353  max mem: 33127
I20241029 22:21:23 2766422 dinov2 helpers.py:102] Training  [  2290/125000]  eta: 2 days, 23:09:41  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8281 (13.2526)  dino_local_crops_loss: 8.9925 (9.3565)  dino_global_crops_loss: 1.1113 (1.1644)  koleo_loss: -0.0222 (0.0125)  ibot_loss: 2.7473 (2.7192)  time: 1.862799  data: 1.228377  max mem: 33127
I20241029 22:21:44 2766422 dinov2 helpers.py:102] Training  [  2300/125000]  eta: 2 days, 23:09:04  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8174 (13.2508)  dino_local_crops_loss: 8.9833 (9.3550)  dino_global_crops_loss: 1.1101 (1.1641)  koleo_loss: -0.0219 (0.0123)  ibot_loss: 2.7473 (2.7193)  time: 1.936877  data: 1.302879  max mem: 33127
I20241029 22:22:02 2766422 dinov2 helpers.py:102] Training  [  2310/125000]  eta: 2 days, 23:06:31  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8079 (13.2490)  dino_local_crops_loss: 8.9770 (9.3535)  dino_global_crops_loss: 1.1127 (1.1639)  koleo_loss: -0.0219 (0.0122)  ibot_loss: 2.7462 (2.7194)  time: 1.947327  data: 1.313759  max mem: 33127
I20241029 22:22:22 2766422 dinov2 helpers.py:102] Training  [  2320/125000]  eta: 2 days, 23:05:40  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8372 (13.2472)  dino_local_crops_loss: 8.9969 (9.3519)  dino_global_crops_loss: 1.1124 (1.1637)  koleo_loss: -0.0217 (0.0121)  ibot_loss: 2.7480 (2.7195)  time: 1.933536  data: 1.300016  max mem: 33127
I20241029 22:22:44 2766422 dinov2 helpers.py:102] Training  [  2330/125000]  eta: 2 days, 23:06:06  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8081 (13.2452)  dino_local_crops_loss: 8.9752 (9.3502)  dino_global_crops_loss: 1.1069 (1.1634)  koleo_loss: -0.0217 (0.0119)  ibot_loss: 2.7468 (2.7197)  time: 2.102821  data: 1.469961  max mem: 33127
I20241029 22:23:03 2766422 dinov2 helpers.py:102] Training  [  2340/125000]  eta: 2 days, 23:03:43  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7780 (13.2433)  dino_local_crops_loss: 8.9482 (9.3485)  dino_global_crops_loss: 1.1053 (1.1632)  koleo_loss: -0.0217 (0.0118)  ibot_loss: 2.7467 (2.7198)  time: 2.014205  data: 1.381570  max mem: 33127
I20241029 22:23:25 2766422 dinov2 helpers.py:102] Training  [  2350/125000]  eta: 2 days, 23:04:53  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7635 (13.2411)  dino_local_crops_loss: 8.9319 (9.3467)  dino_global_crops_loss: 1.1032 (1.1629)  koleo_loss: -0.0222 (0.0116)  ibot_loss: 2.7447 (2.7199)  time: 2.056672  data: 1.423115  max mem: 33127
I20241029 22:23:44 2766422 dinov2 helpers.py:102] Training  [  2360/125000]  eta: 2 days, 23:02:54  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7182 (13.2388)  dino_local_crops_loss: 8.8980 (9.3447)  dino_global_crops_loss: 1.0979 (1.1626)  koleo_loss: -0.0223 (0.0115)  ibot_loss: 2.7449 (2.7200)  time: 2.079338  data: 1.445920  max mem: 33127
I20241029 22:24:05 2766422 dinov2 helpers.py:102] Training  [  2370/125000]  eta: 2 days, 23:02:45  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6949 (13.2366)  dino_local_crops_loss: 8.8761 (9.3427)  dino_global_crops_loss: 1.0975 (1.1624)  koleo_loss: -0.0221 (0.0113)  ibot_loss: 2.7465 (2.7201)  time: 2.003074  data: 1.369218  max mem: 33127
I20241029 22:24:24 2766422 dinov2 helpers.py:102] Training  [  2380/125000]  eta: 2 days, 23:00:11  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6795 (13.2340)  dino_local_crops_loss: 8.8564 (9.3405)  dino_global_crops_loss: 1.0929 (1.1621)  koleo_loss: -0.0223 (0.0112)  ibot_loss: 2.7464 (2.7202)  time: 1.967798  data: 1.333912  max mem: 33127
I20241029 22:24:45 2766422 dinov2 helpers.py:102] Training  [  2390/125000]  eta: 2 days, 23:00:10  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5687 (13.2313)  dino_local_crops_loss: 8.7653 (9.3381)  dino_global_crops_loss: 1.0816 (1.1617)  koleo_loss: -0.0226 (0.0111)  ibot_loss: 2.7450 (2.7203)  time: 1.974900  data: 1.341542  max mem: 33127
I20241029 22:25:06 2766422 dinov2 helpers.py:102] Training  [  2400/125000]  eta: 2 days, 23:00:28  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6282 (13.2288)  dino_local_crops_loss: 8.8168 (9.3360)  dino_global_crops_loss: 1.0886 (1.1615)  koleo_loss: -0.0224 (0.0109)  ibot_loss: 2.7440 (2.7204)  time: 2.141460  data: 1.507519  max mem: 33127
I20241029 22:25:27 2766422 dinov2 helpers.py:102] Training  [  2410/125000]  eta: 2 days, 23:00:07  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6178 (13.2262)  dino_local_crops_loss: 8.8092 (9.3338)  dino_global_crops_loss: 1.0886 (1.1612)  koleo_loss: -0.0222 (0.0108)  ibot_loss: 2.7439 (2.7205)  time: 2.122598  data: 1.488677  max mem: 33127
I20241029 22:25:46 2766422 dinov2 helpers.py:102] Training  [  2420/125000]  eta: 2 days, 22:58:06  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5848 (13.2234)  dino_local_crops_loss: 8.7784 (9.3313)  dino_global_crops_loss: 1.0858 (1.1608)  koleo_loss: -0.0221 (0.0106)  ibot_loss: 2.7432 (2.7206)  time: 1.986100  data: 1.351745  max mem: 33127
I20241029 22:26:07 2766422 dinov2 helpers.py:102] Training  [  2430/125000]  eta: 2 days, 22:57:25  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5969 (13.2210)  dino_local_crops_loss: 8.7862 (9.3292)  dino_global_crops_loss: 1.0880 (1.1605)  koleo_loss: -0.0225 (0.0105)  ibot_loss: 2.7425 (2.7207)  time: 1.965920  data: 1.330953  max mem: 33127
I20241029 22:26:26 2766422 dinov2 helpers.py:102] Training  [  2440/125000]  eta: 2 days, 22:55:51  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6418 (13.2185)  dino_local_crops_loss: 8.8264 (9.3271)  dino_global_crops_loss: 1.0907 (1.1602)  koleo_loss: -0.0225 (0.0104)  ibot_loss: 2.7416 (2.7208)  time: 1.991587  data: 1.356597  max mem: 33127
I20241029 22:26:47 2766422 dinov2 helpers.py:102] Training  [  2450/125000]  eta: 2 days, 22:55:20  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6249 (13.2161)  dino_local_crops_loss: 8.8180 (9.3251)  dino_global_crops_loss: 1.0872 (1.1600)  koleo_loss: -0.0226 (0.0102)  ibot_loss: 2.7402 (2.7209)  time: 2.001382  data: 1.366361  max mem: 33127
I20241029 22:27:02 2766422 dinov2 helpers.py:102] Training  [  2460/125000]  eta: 2 days, 22:50:47  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5885 (13.2135)  dino_local_crops_loss: 8.7849 (9.3228)  dino_global_crops_loss: 1.0858 (1.1596)  koleo_loss: -0.0228 (0.0101)  ibot_loss: 2.7398 (2.7209)  time: 1.820486  data: 1.186193  max mem: 33127
I20241029 22:27:23 2766422 dinov2 helpers.py:102] Training  [  2470/125000]  eta: 2 days, 22:50:09  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5969 (13.2112)  dino_local_crops_loss: 8.7914 (9.3209)  dino_global_crops_loss: 1.0884 (1.1594)  koleo_loss: -0.0228 (0.0100)  ibot_loss: 2.7388 (2.7210)  time: 1.811891  data: 1.178254  max mem: 33127
I20241029 22:27:44 2766422 dinov2 helpers.py:102] Training  [  2480/125000]  eta: 2 days, 22:50:01  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6064 (13.2090)  dino_local_crops_loss: 8.8012 (9.3189)  dino_global_crops_loss: 1.0884 (1.1591)  koleo_loss: -0.0223 (0.0098)  ibot_loss: 2.7387 (2.7211)  time: 2.076740  data: 1.444408  max mem: 33127
I20241029 22:28:03 2766422 dinov2 helpers.py:102] Training  [  2490/125000]  eta: 2 days, 22:48:40  lr: 0.0003 (0.0001)  wd: 0.0403 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6649 (13.2071)  dino_local_crops_loss: 8.8609 (9.3173)  dino_global_crops_loss: 1.0930 (1.1589)  koleo_loss: -0.0223 (0.0097)  ibot_loss: 2.7370 (2.7211)  time: 2.032379  data: 1.402251  max mem: 33127
I20241029 22:28:24 2766422 dinov2 helpers.py:102] Training  [  2500/125000]  eta: 2 days, 22:47:59  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7027 (13.2050)  dino_local_crops_loss: 8.8888 (9.3156)  dino_global_crops_loss: 1.0981 (1.1586)  koleo_loss: -0.0224 (0.0096)  ibot_loss: 2.7383 (2.7212)  time: 1.998949  data: 1.370248  max mem: 33127
I20241029 22:28:46 2766422 dinov2 helpers.py:102] Training  [  2510/125000]  eta: 2 days, 22:48:17  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7104 (13.2032)  dino_local_crops_loss: 8.8962 (9.3141)  dino_global_crops_loss: 1.0981 (1.1584)  koleo_loss: -0.0222 (0.0095)  ibot_loss: 2.7362 (2.7213)  time: 2.100467  data: 1.472998  max mem: 33127
I20241029 22:29:08 2766422 dinov2 helpers.py:102] Training  [  2520/125000]  eta: 2 days, 22:48:59  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6884 (13.2011)  dino_local_crops_loss: 8.8826 (9.3123)  dino_global_crops_loss: 1.0960 (1.1582)  koleo_loss: -0.0223 (0.0093)  ibot_loss: 2.7350 (2.7213)  time: 2.185548  data: 1.558770  max mem: 33127
I20241029 22:29:27 2766422 dinov2 helpers.py:102] Training  [  2530/125000]  eta: 2 days, 22:47:37  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6677 (13.1992)  dino_local_crops_loss: 8.8652 (9.3107)  dino_global_crops_loss: 1.0933 (1.1579)  koleo_loss: -0.0227 (0.0092)  ibot_loss: 2.7329 (2.7214)  time: 2.081827  data: 1.454849  max mem: 33127
I20241029 22:29:48 2766422 dinov2 helpers.py:102] Training  [  2540/125000]  eta: 2 days, 22:47:36  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7150 (13.1973)  dino_local_crops_loss: 8.9015 (9.3091)  dino_global_crops_loss: 1.0972 (1.1577)  koleo_loss: -0.0226 (0.0091)  ibot_loss: 2.7322 (2.7214)  time: 2.038263  data: 1.410425  max mem: 33127
I20241029 22:30:09 2766422 dinov2 helpers.py:102] Training  [  2550/125000]  eta: 2 days, 22:47:11  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7674 (13.1956)  dino_local_crops_loss: 8.9541 (9.3077)  dino_global_crops_loss: 1.1051 (1.1575)  koleo_loss: -0.0223 (0.0090)  ibot_loss: 2.7327 (2.7214)  time: 2.098210  data: 1.469456  max mem: 33127
I20241029 22:30:30 2766422 dinov2 helpers.py:102] Training  [  2560/125000]  eta: 2 days, 22:47:04  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7674 (13.1939)  dino_local_crops_loss: 8.9541 (9.3063)  dino_global_crops_loss: 1.1044 (1.1573)  koleo_loss: -0.0227 (0.0088)  ibot_loss: 2.7301 (2.7215)  time: 2.091206  data: 1.460572  max mem: 33127
I20241029 22:30:51 2766422 dinov2 helpers.py:102] Training  [  2570/125000]  eta: 2 days, 22:46:30  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7537 (13.1922)  dino_local_crops_loss: 8.9351 (9.3049)  dino_global_crops_loss: 1.1058 (1.1571)  koleo_loss: -0.0227 (0.0087)  ibot_loss: 2.7289 (2.7215)  time: 2.081178  data: 1.450115  max mem: 33127
I20241029 22:31:11 2766422 dinov2 helpers.py:102] Training  [  2580/125000]  eta: 2 days, 22:45:27  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7682 (13.1907)  dino_local_crops_loss: 8.9583 (9.3036)  dino_global_crops_loss: 1.1039 (1.1569)  koleo_loss: -0.0226 (0.0086)  ibot_loss: 2.7285 (2.7215)  time: 2.022396  data: 1.392016  max mem: 33127
I20241029 22:31:31 2766422 dinov2 helpers.py:102] Training  [  2590/125000]  eta: 2 days, 22:44:58  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7852 (13.1892)  dino_local_crops_loss: 8.9740 (9.3024)  dino_global_crops_loss: 1.1071 (1.1567)  koleo_loss: -0.0227 (0.0085)  ibot_loss: 2.7321 (2.7216)  time: 2.028259  data: 1.396254  max mem: 33127
I20241029 22:31:52 2766422 dinov2 helpers.py:102] Training  [  2600/125000]  eta: 2 days, 22:44:09  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8157 (13.1878)  dino_local_crops_loss: 8.9951 (9.3013)  dino_global_crops_loss: 1.1126 (1.1566)  koleo_loss: -0.0228 (0.0083)  ibot_loss: 2.7309 (2.7216)  time: 2.042717  data: 1.409500  max mem: 33127
I20241029 22:32:10 2766422 dinov2 helpers.py:102] Training  [  2610/125000]  eta: 2 days, 22:42:18  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7677 (13.1860)  dino_local_crops_loss: 8.9559 (9.2998)  dino_global_crops_loss: 1.1083 (1.1564)  koleo_loss: -0.0229 (0.0082)  ibot_loss: 2.7272 (2.7216)  time: 1.954106  data: 1.321412  max mem: 33127
I20241029 22:32:30 2766422 dinov2 helpers.py:102] Training  [  2620/125000]  eta: 2 days, 22:41:23  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7480 (13.1845)  dino_local_crops_loss: 8.9443 (9.2986)  dino_global_crops_loss: 1.1047 (1.1562)  koleo_loss: -0.0228 (0.0081)  ibot_loss: 2.7248 (2.7216)  time: 1.946673  data: 1.314805  max mem: 33127
I20241029 22:32:52 2766422 dinov2 helpers.py:102] Training  [  2630/125000]  eta: 2 days, 22:41:23  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7719 (13.1830)  dino_local_crops_loss: 8.9688 (9.2974)  dino_global_crops_loss: 1.1069 (1.1560)  koleo_loss: -0.0228 (0.0080)  ibot_loss: 2.7239 (2.7216)  time: 2.065038  data: 1.433633  max mem: 33127
I20241029 22:33:14 2766422 dinov2 helpers.py:102] Training  [  2640/125000]  eta: 2 days, 22:42:31  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7756 (13.1814)  dino_local_crops_loss: 8.9709 (9.2961)  dino_global_crops_loss: 1.1090 (1.1558)  koleo_loss: -0.0228 (0.0079)  ibot_loss: 2.7242 (2.7217)  time: 2.198225  data: 1.567901  max mem: 33127
I20241029 22:33:35 2766422 dinov2 helpers.py:102] Training  [  2650/125000]  eta: 2 days, 22:42:19  lr: 0.0003 (0.0001)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7961 (13.1801)  dino_local_crops_loss: 8.9861 (9.2950)  dino_global_crops_loss: 1.1127 (1.1557)  koleo_loss: -0.0229 (0.0078)  ibot_loss: 2.7230 (2.7217)  time: 2.185215  data: 1.556327  max mem: 33127
I20241029 22:33:56 2766422 dinov2 helpers.py:102] Training  [  2660/125000]  eta: 2 days, 22:41:35  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8100 (13.1786)  dino_local_crops_loss: 8.9916 (9.2938)  dino_global_crops_loss: 1.1118 (1.1555)  koleo_loss: -0.0230 (0.0076)  ibot_loss: 2.7225 (2.7217)  time: 2.064499  data: 1.436522  max mem: 33127
I20241029 22:34:18 2766422 dinov2 helpers.py:102] Training  [  2670/125000]  eta: 2 days, 22:42:39  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8100 (13.1773)  dino_local_crops_loss: 8.9931 (9.2928)  dino_global_crops_loss: 1.1124 (1.1553)  koleo_loss: -0.0229 (0.0075)  ibot_loss: 2.7225 (2.7217)  time: 2.147907  data: 1.520994  max mem: 33127
I20241029 22:34:39 2766422 dinov2 helpers.py:102] Training  [  2680/125000]  eta: 2 days, 22:42:29  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8145 (13.1758)  dino_local_crops_loss: 9.0130 (9.2916)  dino_global_crops_loss: 1.1129 (1.1552)  koleo_loss: -0.0228 (0.0074)  ibot_loss: 2.7223 (2.7217)  time: 2.185257  data: 1.558920  max mem: 33127
I20241029 22:35:00 2766422 dinov2 helpers.py:102] Training  [  2690/125000]  eta: 2 days, 22:42:14  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7745 (13.1744)  dino_local_crops_loss: 8.9772 (9.2904)  dino_global_crops_loss: 1.1099 (1.1550)  koleo_loss: -0.0230 (0.0073)  ibot_loss: 2.7186 (2.7216)  time: 2.099706  data: 1.471351  max mem: 33127
I20241029 22:35:22 2766422 dinov2 helpers.py:102] Training  [  2700/125000]  eta: 2 days, 22:42:23  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7983 (13.1730)  dino_local_crops_loss: 8.9953 (9.2894)  dino_global_crops_loss: 1.1104 (1.1548)  koleo_loss: -0.0231 (0.0072)  ibot_loss: 2.7195 (2.7216)  time: 2.119968  data: 1.489981  max mem: 33127
I20241029 22:35:41 2766422 dinov2 helpers.py:102] Training  [  2710/125000]  eta: 2 days, 22:40:46  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7830 (13.1716)  dino_local_crops_loss: 8.9856 (9.2882)  dino_global_crops_loss: 1.1090 (1.1547)  koleo_loss: -0.0229 (0.0071)  ibot_loss: 2.7202 (2.7216)  time: 2.029331  data: 1.399044  max mem: 33127
I20241029 22:36:03 2766422 dinov2 helpers.py:102] Training  [  2720/125000]  eta: 2 days, 22:41:25  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7922 (13.1703)  dino_local_crops_loss: 8.9872 (9.2872)  dino_global_crops_loss: 1.1083 (1.1545)  koleo_loss: -0.0227 (0.0070)  ibot_loss: 2.7158 (2.7216)  time: 2.063066  data: 1.431079  max mem: 33127
I20241029 22:36:24 2766422 dinov2 helpers.py:102] Training  [  2730/125000]  eta: 2 days, 22:41:11  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7949 (13.1688)  dino_local_crops_loss: 8.9977 (9.2861)  dino_global_crops_loss: 1.1090 (1.1543)  koleo_loss: -0.0231 (0.0069)  ibot_loss: 2.7141 (2.7216)  time: 2.154569  data: 1.522363  max mem: 33127
I20241029 22:36:44 2766422 dinov2 helpers.py:102] Training  [  2740/125000]  eta: 2 days, 22:39:51  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7722 (13.1675)  dino_local_crops_loss: 8.9727 (9.2850)  dino_global_crops_loss: 1.1093 (1.1542)  koleo_loss: -0.0233 (0.0067)  ibot_loss: 2.7141 (2.7215)  time: 2.022283  data: 1.390378  max mem: 33127
I20241029 22:37:02 2766422 dinov2 helpers.py:102] Training  [  2750/125000]  eta: 2 days, 22:38:00  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7722 (13.1661)  dino_local_crops_loss: 8.9718 (9.2839)  dino_global_crops_loss: 1.1097 (1.1540)  koleo_loss: -0.0232 (0.0066)  ibot_loss: 2.7145 (2.7215)  time: 1.913213  data: 1.280744  max mem: 33127
I20241029 22:37:24 2766422 dinov2 helpers.py:102] Training  [  2760/125000]  eta: 2 days, 22:38:22  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8022 (13.1648)  dino_local_crops_loss: 9.0025 (9.2829)  dino_global_crops_loss: 1.1114 (1.1539)  koleo_loss: -0.0231 (0.0065)  ibot_loss: 2.7132 (2.7215)  time: 2.027373  data: 1.394155  max mem: 33127
I20241029 22:37:44 2766422 dinov2 helpers.py:102] Training  [  2770/125000]  eta: 2 days, 22:37:28  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.8016 (13.1634)  dino_local_crops_loss: 8.9957 (9.2818)  dino_global_crops_loss: 1.1076 (1.1537)  koleo_loss: -0.0232 (0.0064)  ibot_loss: 2.7118 (2.7215)  time: 2.091459  data: 1.458516  max mem: 33127
I20241029 22:38:06 2766422 dinov2 helpers.py:102] Training  [  2780/125000]  eta: 2 days, 22:37:38  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7543 (13.1619)  dino_local_crops_loss: 8.9690 (9.2807)  dino_global_crops_loss: 1.1051 (1.1536)  koleo_loss: -0.0234 (0.0063)  ibot_loss: 2.7076 (2.7214)  time: 2.078194  data: 1.446170  max mem: 33127
I20241029 22:38:28 2766422 dinov2 helpers.py:102] Training  [  2790/125000]  eta: 2 days, 22:38:11  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7522 (13.1604)  dino_local_crops_loss: 8.9656 (9.2795)  dino_global_crops_loss: 1.1058 (1.1534)  koleo_loss: -0.0234 (0.0062)  ibot_loss: 2.7039 (2.7213)  time: 2.176549  data: 1.544029  max mem: 33127
I20241029 22:38:51 2766422 dinov2 helpers.py:102] Training  [  2800/125000]  eta: 2 days, 22:39:21  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7600 (13.1591)  dino_local_crops_loss: 8.9710 (9.2785)  dino_global_crops_loss: 1.1077 (1.1532)  koleo_loss: -0.0233 (0.0061)  ibot_loss: 2.7019 (2.7213)  time: 2.245827  data: 1.613449  max mem: 33127
I20241029 22:39:12 2766422 dinov2 helpers.py:102] Training  [  2810/125000]  eta: 2 days, 22:39:05  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0401)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7693 (13.1577)  dino_local_crops_loss: 8.9798 (9.2774)  dino_global_crops_loss: 1.1082 (1.1531)  koleo_loss: -0.0232 (0.0060)  ibot_loss: 2.7075 (2.7212)  time: 2.190245  data: 1.558681  max mem: 33127
I20241029 22:39:32 2766422 dinov2 helpers.py:102] Training  [  2820/125000]  eta: 2 days, 22:38:09  lr: 0.0003 (0.0002)  wd: 0.0404 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7693 (13.1564)  dino_local_crops_loss: 8.9798 (9.2764)  dino_global_crops_loss: 1.1076 (1.1529)  koleo_loss: -0.0230 (0.0059)  ibot_loss: 2.7095 (2.7212)  time: 2.046859  data: 1.414714  max mem: 33127
I20241029 22:39:53 2766422 dinov2 helpers.py:102] Training  [  2830/125000]  eta: 2 days, 22:38:06  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7399 (13.1549)  dino_local_crops_loss: 8.9475 (9.2752)  dino_global_crops_loss: 1.1074 (1.1528)  koleo_loss: -0.0229 (0.0058)  ibot_loss: 2.7089 (2.7211)  time: 2.061335  data: 1.428781  max mem: 33127
I20241029 22:40:13 2766422 dinov2 helpers.py:102] Training  [  2840/125000]  eta: 2 days, 22:37:05  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7586 (13.1536)  dino_local_crops_loss: 8.9690 (9.2742)  dino_global_crops_loss: 1.1086 (1.1526)  koleo_loss: -0.0232 (0.0057)  ibot_loss: 2.7002 (2.7210)  time: 2.055194  data: 1.424029  max mem: 33127
I20241029 22:40:33 2766422 dinov2 helpers.py:102] Training  [  2850/125000]  eta: 2 days, 22:36:24  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7586 (13.1522)  dino_local_crops_loss: 8.9700 (9.2732)  dino_global_crops_loss: 1.1090 (1.1525)  koleo_loss: -0.0233 (0.0056)  ibot_loss: 2.6975 (2.7210)  time: 2.011504  data: 1.381760  max mem: 33127
I20241029 22:40:54 2766422 dinov2 helpers.py:102] Training  [  2860/125000]  eta: 2 days, 22:36:27  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7241 (13.1507)  dino_local_crops_loss: 8.9466 (9.2721)  dino_global_crops_loss: 1.1046 (1.1523)  koleo_loss: -0.0233 (0.0055)  ibot_loss: 2.6975 (2.7209)  time: 2.085059  data: 1.455448  max mem: 33127
I20241029 22:41:14 2766422 dinov2 helpers.py:102] Training  [  2870/125000]  eta: 2 days, 22:34:56  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7228 (13.1493)  dino_local_crops_loss: 8.9456 (9.2710)  dino_global_crops_loss: 1.1056 (1.1521)  koleo_loss: -0.0233 (0.0054)  ibot_loss: 2.6924 (2.7208)  time: 2.026310  data: 1.398384  max mem: 33127
I20241029 22:41:33 2766422 dinov2 helpers.py:102] Training  [  2880/125000]  eta: 2 days, 22:33:47  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7228 (13.1479)  dino_local_crops_loss: 8.9498 (9.2699)  dino_global_crops_loss: 1.1065 (1.1520)  koleo_loss: -0.0232 (0.0053)  ibot_loss: 2.6920 (2.7207)  time: 1.940781  data: 1.313331  max mem: 33127
I20241029 22:41:55 2766422 dinov2 helpers.py:102] Training  [  2890/125000]  eta: 2 days, 22:34:07  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7215 (13.1464)  dino_local_crops_loss: 8.9397 (9.2688)  dino_global_crops_loss: 1.1040 (1.1518)  koleo_loss: -0.0233 (0.0052)  ibot_loss: 2.6994 (2.7206)  time: 2.072117  data: 1.442356  max mem: 33127
I20241029 22:42:15 2766422 dinov2 helpers.py:102] Training  [  2900/125000]  eta: 2 days, 22:33:11  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7269 (13.1450)  dino_local_crops_loss: 8.9485 (9.2677)  dino_global_crops_loss: 1.1056 (1.1517)  koleo_loss: -0.0233 (0.0051)  ibot_loss: 2.6978 (2.7205)  time: 2.087310  data: 1.456555  max mem: 33127
I20241029 22:42:35 2766422 dinov2 helpers.py:102] Training  [  2910/125000]  eta: 2 days, 22:32:17  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7269 (13.1436)  dino_local_crops_loss: 8.9591 (9.2667)  dino_global_crops_loss: 1.1056 (1.1515)  koleo_loss: -0.0233 (0.0050)  ibot_loss: 2.6972 (2.7204)  time: 1.998786  data: 1.366911  max mem: 33127
I20241029 22:42:55 2766422 dinov2 helpers.py:102] Training  [  2920/125000]  eta: 2 days, 22:31:28  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7267 (13.1422)  dino_local_crops_loss: 8.9584 (9.2656)  dino_global_crops_loss: 1.1047 (1.1514)  koleo_loss: -0.0233 (0.0049)  ibot_loss: 2.6905 (2.7203)  time: 2.006404  data: 1.373619  max mem: 33127
I20241029 22:43:17 2766422 dinov2 helpers.py:102] Training  [  2930/125000]  eta: 2 days, 22:31:33  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6947 (13.1407)  dino_local_crops_loss: 8.9260 (9.2645)  dino_global_crops_loss: 1.1018 (1.1512)  koleo_loss: -0.0232 (0.0048)  ibot_loss: 2.6897 (2.7202)  time: 2.076729  data: 1.443788  max mem: 33127
I20241029 22:43:40 2766422 dinov2 helpers.py:102] Training  [  2940/125000]  eta: 2 days, 22:32:46  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6947 (13.1392)  dino_local_crops_loss: 8.9260 (9.2633)  dino_global_crops_loss: 1.1024 (1.1510)  koleo_loss: -0.0233 (0.0047)  ibot_loss: 2.6873 (2.7201)  time: 2.224487  data: 1.590776  max mem: 33127
I20241029 22:44:00 2766422 dinov2 helpers.py:102] Training  [  2950/125000]  eta: 2 days, 22:32:17  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6915 (13.1377)  dino_local_crops_loss: 8.9372 (9.2622)  dino_global_crops_loss: 1.1024 (1.1509)  koleo_loss: -0.0234 (0.0046)  ibot_loss: 2.6824 (2.7200)  time: 2.183327  data: 1.549665  max mem: 33127
I20241029 22:44:22 2766422 dinov2 helpers.py:102] Training  [  2960/125000]  eta: 2 days, 22:32:21  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7180 (13.1363)  dino_local_crops_loss: 8.9585 (9.2612)  dino_global_crops_loss: 1.1019 (1.1507)  koleo_loss: -0.0234 (0.0045)  ibot_loss: 2.6815 (2.7198)  time: 2.100548  data: 1.467047  max mem: 33127
I20241029 22:44:40 2766422 dinov2 helpers.py:102] Training  [  2970/125000]  eta: 2 days, 22:30:34  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6968 (13.1347)  dino_local_crops_loss: 8.9338 (9.2600)  dino_global_crops_loss: 1.1019 (1.1506)  koleo_loss: -0.0235 (0.0044)  ibot_loss: 2.6833 (2.7197)  time: 2.006561  data: 1.373185  max mem: 33127
I20241029 22:45:03 2766422 dinov2 helpers.py:102] Training  [  2980/125000]  eta: 2 days, 22:31:39  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6875 (13.1333)  dino_local_crops_loss: 8.9241 (9.2590)  dino_global_crops_loss: 1.0992 (1.1504)  koleo_loss: -0.0236 (0.0043)  ibot_loss: 2.6828 (2.7196)  time: 2.079954  data: 1.445715  max mem: 33127
I20241029 22:45:25 2766422 dinov2 helpers.py:102] Training  [  2990/125000]  eta: 2 days, 22:31:52  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7115 (13.1320)  dino_local_crops_loss: 8.9473 (9.2581)  dino_global_crops_loss: 1.1075 (1.1503)  koleo_loss: -0.0235 (0.0042)  ibot_loss: 2.6804 (2.7195)  time: 2.225992  data: 1.591640  max mem: 33127
I20241029 22:45:45 2766422 dinov2 helpers.py:102] Training  [  3000/125000]  eta: 2 days, 22:31:24  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7197 (13.1306)  dino_local_crops_loss: 8.9625 (9.2570)  dino_global_crops_loss: 1.1060 (1.1501)  koleo_loss: -0.0232 (0.0042)  ibot_loss: 2.6838 (2.7193)  time: 2.113526  data: 1.480275  max mem: 33127
I20241029 22:46:02 2766422 dinov2 helpers.py:102] Training  [  3010/125000]  eta: 2 days, 22:28:19  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6629 (13.1291)  dino_local_crops_loss: 8.9105 (9.2559)  dino_global_crops_loss: 1.0992 (1.1499)  koleo_loss: -0.0232 (0.0041)  ibot_loss: 2.6847 (2.7192)  time: 1.870209  data: 1.236098  max mem: 33127
I20241029 22:46:23 2766422 dinov2 helpers.py:102] Training  [  3020/125000]  eta: 2 days, 22:28:04  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6645 (13.1277)  dino_local_crops_loss: 8.9105 (9.2548)  dino_global_crops_loss: 1.0987 (1.1498)  koleo_loss: -0.0235 (0.0040)  ibot_loss: 2.6854 (2.7191)  time: 1.884861  data: 1.249340  max mem: 33127
I20241029 22:46:44 2766422 dinov2 helpers.py:102] Training  [  3030/125000]  eta: 2 days, 22:28:02  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6896 (13.1263)  dino_local_crops_loss: 8.9238 (9.2538)  dino_global_crops_loss: 1.1032 (1.1496)  koleo_loss: -0.0237 (0.0039)  ibot_loss: 2.6842 (2.7190)  time: 2.109817  data: 1.473368  max mem: 33127
I20241029 22:47:06 2766422 dinov2 helpers.py:102] Training  [  3040/125000]  eta: 2 days, 22:27:56  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.7021 (13.1248)  dino_local_crops_loss: 8.9385 (9.2527)  dino_global_crops_loss: 1.1032 (1.1495)  koleo_loss: -0.0236 (0.0038)  ibot_loss: 2.6790 (2.7188)  time: 2.121626  data: 1.485514  max mem: 33127
I20241029 22:47:25 2766422 dinov2 helpers.py:102] Training  [  3050/125000]  eta: 2 days, 22:26:24  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6778 (13.1233)  dino_local_crops_loss: 8.9092 (9.2516)  dino_global_crops_loss: 1.0993 (1.1493)  koleo_loss: -0.0236 (0.0037)  ibot_loss: 2.6811 (2.7187)  time: 2.009731  data: 1.373989  max mem: 33127
I20241029 22:47:45 2766422 dinov2 helpers.py:102] Training  [  3060/125000]  eta: 2 days, 22:25:39  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6571 (13.1218)  dino_local_crops_loss: 8.9027 (9.2504)  dino_global_crops_loss: 1.0962 (1.1491)  koleo_loss: -0.0236 (0.0036)  ibot_loss: 2.6863 (2.7186)  time: 1.960681  data: 1.323925  max mem: 33127
I20241029 22:48:07 2766422 dinov2 helpers.py:102] Training  [  3070/125000]  eta: 2 days, 22:25:54  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6571 (13.1204)  dino_local_crops_loss: 8.9194 (9.2494)  dino_global_crops_loss: 1.0989 (1.1490)  koleo_loss: -0.0235 (0.0035)  ibot_loss: 2.6811 (2.7185)  time: 2.094067  data: 1.456995  max mem: 33127
I20241029 22:48:27 2766422 dinov2 helpers.py:102] Training  [  3080/125000]  eta: 2 days, 22:25:30  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6946 (13.1189)  dino_local_crops_loss: 8.9500 (9.2483)  dino_global_crops_loss: 1.1025 (1.1488)  koleo_loss: -0.0235 (0.0034)  ibot_loss: 2.6773 (2.7184)  time: 2.120134  data: 1.484552  max mem: 33127
I20241029 22:48:46 2766422 dinov2 helpers.py:102] Training  [  3090/125000]  eta: 2 days, 22:23:30  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6713 (13.1175)  dino_local_crops_loss: 8.9026 (9.2472)  dino_global_crops_loss: 1.1017 (1.1487)  koleo_loss: -0.0236 (0.0033)  ibot_loss: 2.6791 (2.7183)  time: 1.949494  data: 1.314291  max mem: 33127
I20241029 22:49:06 2766422 dinov2 helpers.py:102] Training  [  3100/125000]  eta: 2 days, 22:22:43  lr: 0.0003 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0003 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6458 (13.1161)  dino_local_crops_loss: 8.8836 (9.2461)  dino_global_crops_loss: 1.0996 (1.1485)  koleo_loss: -0.0233 (0.0033)  ibot_loss: 2.6787 (2.7181)  time: 1.919574  data: 1.282884  max mem: 33127
I20241029 22:49:26 2766422 dinov2 helpers.py:102] Training  [  3110/125000]  eta: 2 days, 22:22:03  lr: 0.0004 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6280 (13.1145)  dino_local_crops_loss: 8.8802 (9.2450)  dino_global_crops_loss: 1.0957 (1.1484)  koleo_loss: -0.0234 (0.0032)  ibot_loss: 2.6792 (2.7180)  time: 2.020038  data: 1.382940  max mem: 33127
I20241029 22:49:49 2766422 dinov2 helpers.py:102] Training  [  3120/125000]  eta: 2 days, 22:23:07  lr: 0.0004 (0.0002)  wd: 0.0405 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6270 (13.1130)  dino_local_crops_loss: 8.8770 (9.2439)  dino_global_crops_loss: 1.0971 (1.1482)  koleo_loss: -0.0237 (0.0031)  ibot_loss: 2.6709 (2.7178)  time: 2.162954  data: 1.526368  max mem: 33127
I20241029 22:50:09 2766422 dinov2 helpers.py:102] Training  [  3130/125000]  eta: 2 days, 22:22:17  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6348 (13.1115)  dino_local_crops_loss: 8.8770 (9.2427)  dino_global_crops_loss: 1.0971 (1.1480)  koleo_loss: -0.0237 (0.0030)  ibot_loss: 2.6738 (2.7177)  time: 2.149951  data: 1.514280  max mem: 33127
I20241029 22:50:31 2766422 dinov2 helpers.py:102] Training  [  3140/125000]  eta: 2 days, 22:22:27  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0001)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6385 (13.1101)  dino_local_crops_loss: 8.8960 (9.2417)  dino_global_crops_loss: 1.0977 (1.1479)  koleo_loss: -0.0236 (0.0029)  ibot_loss: 2.6758 (2.7176)  time: 2.080791  data: 1.445260  max mem: 33127
I20241029 22:50:53 2766422 dinov2 helpers.py:102] Training  [  3150/125000]  eta: 2 days, 22:22:53  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6454 (13.1087)  dino_local_crops_loss: 8.9055 (9.2407)  dino_global_crops_loss: 1.1013 (1.1477)  koleo_loss: -0.0235 (0.0028)  ibot_loss: 2.6700 (2.7174)  time: 2.178651  data: 1.542539  max mem: 33127
I20241029 22:51:13 2766422 dinov2 helpers.py:102] Training  [  3160/125000]  eta: 2 days, 22:22:07  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6430 (13.1072)  dino_local_crops_loss: 8.9062 (9.2396)  dino_global_crops_loss: 1.0988 (1.1476)  koleo_loss: -0.0235 (0.0028)  ibot_loss: 2.6660 (2.7172)  time: 2.107218  data: 1.471725  max mem: 33127
I20241029 22:51:30 2766422 dinov2 helpers.py:102] Training  [  3170/125000]  eta: 2 days, 22:19:43  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6212 (13.1056)  dino_local_crops_loss: 8.8935 (9.2385)  dino_global_crops_loss: 1.0977 (1.1474)  koleo_loss: -0.0239 (0.0027)  ibot_loss: 2.6638 (2.7171)  time: 1.887262  data: 1.251284  max mem: 33127
I20241029 22:51:51 2766422 dinov2 helpers.py:102] Training  [  3180/125000]  eta: 2 days, 22:19:18  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6212 (13.1042)  dino_local_crops_loss: 8.8924 (9.2374)  dino_global_crops_loss: 1.0993 (1.1473)  koleo_loss: -0.0240 (0.0026)  ibot_loss: 2.6638 (2.7169)  time: 1.912748  data: 1.275010  max mem: 33127
I20241029 22:52:12 2766422 dinov2 helpers.py:102] Training  [  3190/125000]  eta: 2 days, 22:18:47  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6162 (13.1026)  dino_local_crops_loss: 8.8848 (9.2363)  dino_global_crops_loss: 1.0993 (1.1471)  koleo_loss: -0.0240 (0.0025)  ibot_loss: 2.6552 (2.7167)  time: 2.058476  data: 1.420314  max mem: 33127
I20241029 22:52:33 2766422 dinov2 helpers.py:102] Training  [  3200/125000]  eta: 2 days, 22:18:38  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6232 (13.1011)  dino_local_crops_loss: 8.9004 (9.2352)  dino_global_crops_loss: 1.0969 (1.1470)  koleo_loss: -0.0239 (0.0024)  ibot_loss: 2.6549 (2.7165)  time: 2.080941  data: 1.443364  max mem: 33127
I20241029 22:52:51 2766422 dinov2 helpers.py:102] Training  [  3210/125000]  eta: 2 days, 22:16:35  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6516 (13.0997)  dino_local_crops_loss: 8.9069 (9.2342)  dino_global_crops_loss: 1.0988 (1.1468)  koleo_loss: -0.0239 (0.0023)  ibot_loss: 2.6641 (2.7163)  time: 1.958841  data: 1.322435  max mem: 33127
I20241029 22:53:11 2766422 dinov2 helpers.py:102] Training  [  3220/125000]  eta: 2 days, 22:15:57  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6085 (13.0982)  dino_local_crops_loss: 8.8782 (9.2331)  dino_global_crops_loss: 1.0965 (1.1467)  koleo_loss: -0.0238 (0.0023)  ibot_loss: 2.6640 (2.7162)  time: 1.919150  data: 1.282384  max mem: 33127
I20241029 22:53:33 2766422 dinov2 helpers.py:102] Training  [  3230/125000]  eta: 2 days, 22:16:13  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6234 (13.0968)  dino_local_crops_loss: 8.8932 (9.2321)  dino_global_crops_loss: 1.0962 (1.1465)  koleo_loss: -0.0238 (0.0022)  ibot_loss: 2.6603 (2.7160)  time: 2.103214  data: 1.465840  max mem: 33127
I20241029 22:53:53 2766422 dinov2 helpers.py:102] Training  [  3240/125000]  eta: 2 days, 22:15:40  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6234 (13.0954)  dino_local_crops_loss: 8.8968 (9.2311)  dino_global_crops_loss: 1.0966 (1.1464)  koleo_loss: -0.0241 (0.0021)  ibot_loss: 2.6597 (2.7158)  time: 2.110414  data: 1.474169  max mem: 33127
I20241029 22:54:14 2766422 dinov2 helpers.py:102] Training  [  3250/125000]  eta: 2 days, 22:15:11  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6134 (13.0938)  dino_local_crops_loss: 8.8777 (9.2300)  dino_global_crops_loss: 1.0993 (1.1462)  koleo_loss: -0.0242 (0.0020)  ibot_loss: 2.6580 (2.7156)  time: 2.049972  data: 1.413627  max mem: 33127
I20241029 22:54:36 2766422 dinov2 helpers.py:102] Training  [  3260/125000]  eta: 2 days, 22:15:48  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6013 (13.0923)  dino_local_crops_loss: 8.8713 (9.2288)  dino_global_crops_loss: 1.0976 (1.1461)  koleo_loss: -0.0240 (0.0019)  ibot_loss: 2.6615 (2.7155)  time: 2.144088  data: 1.507385  max mem: 33127
I20241029 22:54:56 2766422 dinov2 helpers.py:102] Training  [  3270/125000]  eta: 2 days, 22:15:04  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6098 (13.0908)  dino_local_crops_loss: 8.8713 (9.2277)  dino_global_crops_loss: 1.0953 (1.1459)  koleo_loss: -0.0238 (0.0019)  ibot_loss: 2.6622 (2.7153)  time: 2.124004  data: 1.488696  max mem: 33127
I20241029 22:55:19 2766422 dinov2 helpers.py:102] Training  [  3280/125000]  eta: 2 days, 22:15:50  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6179 (13.0893)  dino_local_crops_loss: 8.8863 (9.2267)  dino_global_crops_loss: 1.0975 (1.1457)  koleo_loss: -0.0239 (0.0018)  ibot_loss: 2.6609 (2.7151)  time: 2.135531  data: 1.499874  max mem: 33127
I20241029 22:55:41 2766422 dinov2 helpers.py:102] Training  [  3290/125000]  eta: 2 days, 22:16:12  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6208 (13.0879)  dino_local_crops_loss: 8.8916 (9.2256)  dino_global_crops_loss: 1.0975 (1.1456)  koleo_loss: -0.0238 (0.0017)  ibot_loss: 2.6572 (2.7150)  time: 2.225632  data: 1.590083  max mem: 33127
I20241029 22:56:02 2766422 dinov2 helpers.py:102] Training  [  3300/125000]  eta: 2 days, 22:16:07  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5909 (13.0862)  dino_local_crops_loss: 8.8730 (9.2244)  dino_global_crops_loss: 1.0948 (1.1454)  koleo_loss: -0.0235 (0.0016)  ibot_loss: 2.6528 (2.7148)  time: 2.157445  data: 1.523175  max mem: 33127
I20241029 22:56:21 2766422 dinov2 helpers.py:102] Training  [  3310/125000]  eta: 2 days, 22:14:48  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5218 (13.0846)  dino_local_crops_loss: 8.8074 (9.2232)  dino_global_crops_loss: 1.0879 (1.1453)  koleo_loss: -0.0234 (0.0015)  ibot_loss: 2.6495 (2.7146)  time: 2.020866  data: 1.385780  max mem: 33127
I20241029 22:56:42 2766422 dinov2 helpers.py:102] Training  [  3320/125000]  eta: 2 days, 22:14:32  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5941 (13.0832)  dino_local_crops_loss: 8.8677 (9.2222)  dino_global_crops_loss: 1.0932 (1.1451)  koleo_loss: -0.0236 (0.0015)  ibot_loss: 2.6548 (2.7144)  time: 2.006336  data: 1.370113  max mem: 33127
I20241029 22:57:03 2766422 dinov2 helpers.py:102] Training  [  3330/125000]  eta: 2 days, 22:14:07  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.6028 (13.0817)  dino_local_crops_loss: 8.8884 (9.2211)  dino_global_crops_loss: 1.0950 (1.1450)  koleo_loss: -0.0236 (0.0014)  ibot_loss: 2.6561 (2.7142)  time: 2.079616  data: 1.442922  max mem: 33127
I20241029 22:57:26 2766422 dinov2 helpers.py:102] Training  [  3340/125000]  eta: 2 days, 22:14:58  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5318 (13.0800)  dino_local_crops_loss: 8.8253 (9.2199)  dino_global_crops_loss: 1.0867 (1.1448)  koleo_loss: -0.0234 (0.0013)  ibot_loss: 2.6518 (2.7140)  time: 2.171387  data: 1.534937  max mem: 33127
I20241029 22:57:45 2766422 dinov2 helpers.py:102] Training  [  3350/125000]  eta: 2 days, 22:14:05  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5457 (13.0786)  dino_local_crops_loss: 8.8425 (9.2189)  dino_global_crops_loss: 1.0870 (1.1446)  koleo_loss: -0.0236 (0.0012)  ibot_loss: 2.6541 (2.7139)  time: 2.132569  data: 1.496701  max mem: 33127
I20241029 22:58:07 2766422 dinov2 helpers.py:102] Training  [  3360/125000]  eta: 2 days, 22:14:27  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5554 (13.0770)  dino_local_crops_loss: 8.8409 (9.2177)  dino_global_crops_loss: 1.0901 (1.1445)  koleo_loss: -0.0239 (0.0012)  ibot_loss: 2.6539 (2.7137)  time: 2.092630  data: 1.456093  max mem: 33127
I20241029 22:58:29 2766422 dinov2 helpers.py:102] Training  [  3370/125000]  eta: 2 days, 22:14:42  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4929 (13.0753)  dino_local_crops_loss: 8.7841 (9.2164)  dino_global_crops_loss: 1.0839 (1.1443)  koleo_loss: -0.0240 (0.0011)  ibot_loss: 2.6483 (2.7135)  time: 2.187124  data: 1.550548  max mem: 33127
I20241029 22:58:49 2766422 dinov2 helpers.py:102] Training  [  3380/125000]  eta: 2 days, 22:13:29  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5085 (13.0738)  dino_local_crops_loss: 8.7953 (9.2154)  dino_global_crops_loss: 1.0867 (1.1441)  koleo_loss: -0.0241 (0.0010)  ibot_loss: 2.6476 (2.7133)  time: 2.056012  data: 1.420857  max mem: 33127
I20241029 22:59:08 2766422 dinov2 helpers.py:102] Training  [  3390/125000]  eta: 2 days, 22:12:35  lr: 0.0004 (0.0002)  wd: 0.0406 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5367 (13.0722)  dino_local_crops_loss: 8.8032 (9.2142)  dino_global_crops_loss: 1.0867 (1.1440)  koleo_loss: -0.0241 (0.0009)  ibot_loss: 2.6512 (2.7131)  time: 1.959651  data: 1.324232  max mem: 33127
I20241029 22:59:31 2766422 dinov2 helpers.py:102] Training  [  3400/125000]  eta: 2 days, 22:13:33  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5020 (13.0706)  dino_local_crops_loss: 8.7914 (9.2130)  dino_global_crops_loss: 1.0834 (1.1438)  koleo_loss: -0.0240 (0.0009)  ibot_loss: 2.6481 (2.7129)  time: 2.142531  data: 1.505471  max mem: 33127
I20241029 22:59:52 2766422 dinov2 helpers.py:102] Training  [  3410/125000]  eta: 2 days, 22:12:59  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5057 (13.0689)  dino_local_crops_loss: 8.8138 (9.2118)  dino_global_crops_loss: 1.0869 (1.1436)  koleo_loss: -0.0241 (0.0008)  ibot_loss: 2.6429 (2.7127)  time: 2.171187  data: 1.535211  max mem: 33127
I20241029 23:00:13 2766422 dinov2 helpers.py:102] Training  [  3420/125000]  eta: 2 days, 22:12:50  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5123 (13.0672)  dino_local_crops_loss: 8.8043 (9.2106)  dino_global_crops_loss: 1.0841 (1.1434)  koleo_loss: -0.0240 (0.0007)  ibot_loss: 2.6482 (2.7125)  time: 2.077009  data: 1.442235  max mem: 33127
I20241029 23:00:35 2766422 dinov2 helpers.py:102] Training  [  3430/125000]  eta: 2 days, 22:13:10  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5143 (13.0656)  dino_local_crops_loss: 8.8043 (9.2093)  dino_global_crops_loss: 1.0834 (1.1433)  koleo_loss: -0.0241 (0.0007)  ibot_loss: 2.6524 (2.7123)  time: 2.152853  data: 1.517340  max mem: 33127
I20241029 23:00:57 2766422 dinov2 helpers.py:102] Training  [  3440/125000]  eta: 2 days, 22:13:48  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4999 (13.0639)  dino_local_crops_loss: 8.7869 (9.2080)  dino_global_crops_loss: 1.0844 (1.1431)  koleo_loss: -0.0240 (0.0006)  ibot_loss: 2.6505 (2.7122)  time: 2.220762  data: 1.585872  max mem: 33127
I20241029 23:01:18 2766422 dinov2 helpers.py:102] Training  [  3450/125000]  eta: 2 days, 22:13:33  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.5008 (13.0622)  dino_local_crops_loss: 8.7875 (9.2068)  dino_global_crops_loss: 1.0815 (1.1429)  koleo_loss: -0.0239 (0.0005)  ibot_loss: 2.6505 (2.7120)  time: 2.171482  data: 1.537241  max mem: 33127
I20241029 23:01:39 2766422 dinov2 helpers.py:102] Training  [  3460/125000]  eta: 2 days, 22:13:09  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4749 (13.0606)  dino_local_crops_loss: 8.7761 (9.2056)  dino_global_crops_loss: 1.0805 (1.1427)  koleo_loss: -0.0240 (0.0004)  ibot_loss: 2.6498 (2.7118)  time: 2.083467  data: 1.448372  max mem: 33127
I20241029 23:02:00 2766422 dinov2 helpers.py:102] Training  [  3470/125000]  eta: 2 days, 22:12:43  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4749 (13.0590)  dino_local_crops_loss: 8.7761 (9.2044)  dino_global_crops_loss: 1.0797 (1.1426)  koleo_loss: -0.0239 (0.0004)  ibot_loss: 2.6431 (2.7116)  time: 2.066869  data: 1.430975  max mem: 33127
I20241029 23:02:23 2766422 dinov2 helpers.py:102] Training  [  3480/125000]  eta: 2 days, 22:13:48  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4769 (13.0572)  dino_local_crops_loss: 8.7781 (9.2031)  dino_global_crops_loss: 1.0807 (1.1424)  koleo_loss: -0.0237 (0.0003)  ibot_loss: 2.6386 (2.7114)  time: 2.194474  data: 1.559182  max mem: 33127
I20241029 23:02:43 2766422 dinov2 helpers.py:102] Training  [  3490/125000]  eta: 2 days, 22:13:02  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4242 (13.0554)  dino_local_crops_loss: 8.7269 (9.2018)  dino_global_crops_loss: 1.0792 (1.1422)  koleo_loss: -0.0238 (0.0002)  ibot_loss: 2.6310 (2.7112)  time: 2.167855  data: 1.534667  max mem: 33127
I20241029 23:03:07 2766422 dinov2 helpers.py:102] Training  [  3500/125000]  eta: 2 days, 22:14:24  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4316 (13.0537)  dino_local_crops_loss: 8.7535 (9.2006)  dino_global_crops_loss: 1.0792 (1.1420)  koleo_loss: -0.0236 (0.0002)  ibot_loss: 2.6290 (2.7109)  time: 2.192459  data: 1.560724  max mem: 33127
I20241029 23:03:28 2766422 dinov2 helpers.py:102] Training  [  3510/125000]  eta: 2 days, 22:14:08  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4633 (13.0520)  dino_local_crops_loss: 8.7708 (9.1993)  dino_global_crops_loss: 1.0798 (1.1419)  koleo_loss: -0.0237 (0.0001)  ibot_loss: 2.6323 (2.7107)  time: 2.234684  data: 1.603413  max mem: 33127
I20241029 23:03:51 2766422 dinov2 helpers.py:102] Training  [  3520/125000]  eta: 2 days, 22:14:54  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3973 (13.0501)  dino_local_crops_loss: 8.7209 (9.1980)  dino_global_crops_loss: 1.0757 (1.1417)  koleo_loss: -0.0242 (0.0000)  ibot_loss: 2.6275 (2.7105)  time: 2.184904  data: 1.553860  max mem: 33127
I20241029 23:04:10 2766422 dinov2 helpers.py:102] Training  [  3530/125000]  eta: 2 days, 22:13:54  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3920 (13.0483)  dino_local_crops_loss: 8.7209 (9.1967)  dino_global_crops_loss: 1.0757 (1.1415)  koleo_loss: -0.0244 (-0.0000)  ibot_loss: 2.6257 (2.7102)  time: 2.121820  data: 1.490743  max mem: 33127
I20241029 23:04:31 2766422 dinov2 helpers.py:102] Training  [  3540/125000]  eta: 2 days, 22:13:30  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.4011 (13.0465)  dino_local_crops_loss: 8.7206 (9.1953)  dino_global_crops_loss: 1.0756 (1.1413)  koleo_loss: -0.0243 (-0.0001)  ibot_loss: 2.6214 (2.7100)  time: 2.019936  data: 1.388010  max mem: 33127
I20241029 23:04:54 2766422 dinov2 helpers.py:102] Training  [  3550/125000]  eta: 2 days, 22:14:22  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3652 (13.0445)  dino_local_crops_loss: 8.6982 (9.1939)  dino_global_crops_loss: 1.0729 (1.1411)  koleo_loss: -0.0242 (-0.0002)  ibot_loss: 2.6204 (2.7097)  time: 2.183196  data: 1.549550  max mem: 33127
I20241029 23:05:14 2766422 dinov2 helpers.py:102] Training  [  3560/125000]  eta: 2 days, 22:13:40  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3666 (13.0427)  dino_local_crops_loss: 8.6982 (9.1926)  dino_global_crops_loss: 1.0709 (1.1409)  koleo_loss: -0.0242 (-0.0002)  ibot_loss: 2.6204 (2.7094)  time: 2.157763  data: 1.524933  max mem: 33127
I20241029 23:05:34 2766422 dinov2 helpers.py:102] Training  [  3570/125000]  eta: 2 days, 22:13:02  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3666 (13.0408)  dino_local_crops_loss: 8.7077 (9.1912)  dino_global_crops_loss: 1.0709 (1.1407)  koleo_loss: -0.0240 (-0.0003)  ibot_loss: 2.6158 (2.7092)  time: 2.025468  data: 1.392843  max mem: 33127
I20241029 23:05:54 2766422 dinov2 helpers.py:102] Training  [  3580/125000]  eta: 2 days, 22:11:55  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3466 (13.0389)  dino_local_crops_loss: 8.6938 (9.1898)  dino_global_crops_loss: 1.0707 (1.1405)  koleo_loss: -0.0239 (-0.0004)  ibot_loss: 2.6122 (2.7089)  time: 1.988383  data: 1.354691  max mem: 33127
I20241029 23:06:14 2766422 dinov2 helpers.py:102] Training  [  3590/125000]  eta: 2 days, 22:11:23  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3390 (13.0370)  dino_local_crops_loss: 8.6899 (9.1885)  dino_global_crops_loss: 1.0706 (1.1403)  koleo_loss: -0.0240 (-0.0004)  ibot_loss: 2.6078 (2.7086)  time: 1.997283  data: 1.362808  max mem: 33127
I20241029 23:06:35 2766422 dinov2 helpers.py:102] Training  [  3600/125000]  eta: 2 days, 22:11:03  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3390 (13.0351)  dino_local_crops_loss: 8.6950 (9.1871)  dino_global_crops_loss: 1.0713 (1.1401)  koleo_loss: -0.0239 (-0.0005)  ibot_loss: 2.6044 (2.7084)  time: 2.065284  data: 1.429631  max mem: 33127
I20241029 23:06:54 2766422 dinov2 helpers.py:102] Training  [  3610/125000]  eta: 2 days, 22:09:44  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3591 (13.0331)  dino_local_crops_loss: 8.7016 (9.1857)  dino_global_crops_loss: 1.0698 (1.1399)  koleo_loss: -0.0239 (-0.0006)  ibot_loss: 2.6012 (2.7081)  time: 1.995543  data: 1.359354  max mem: 33127
I20241029 23:07:15 2766422 dinov2 helpers.py:102] Training  [  3620/125000]  eta: 2 days, 22:09:16  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3591 (13.0313)  dino_local_crops_loss: 8.7016 (9.1844)  dino_global_crops_loss: 1.0690 (1.1398)  koleo_loss: -0.0240 (-0.0006)  ibot_loss: 2.6087 (2.7078)  time: 1.984501  data: 1.348442  max mem: 33127
I20241029 23:07:34 2766422 dinov2 helpers.py:102] Training  [  3630/125000]  eta: 2 days, 22:07:51  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0402)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3076 (13.0293)  dino_local_crops_loss: 8.6635 (9.1830)  dino_global_crops_loss: 1.0689 (1.1396)  koleo_loss: -0.0241 (-0.0007)  ibot_loss: 2.6029 (2.7075)  time: 1.974762  data: 1.338951  max mem: 33127
I20241029 23:07:56 2766422 dinov2 helpers.py:102] Training  [  3640/125000]  eta: 2 days, 22:08:23  lr: 0.0004 (0.0002)  wd: 0.0407 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3069 (13.0274)  dino_local_crops_loss: 8.6635 (9.1816)  dino_global_crops_loss: 1.0685 (1.1394)  koleo_loss: -0.0238 (-0.0008)  ibot_loss: 2.5993 (2.7072)  time: 2.063503  data: 1.427824  max mem: 33127
I20241029 23:08:14 2766422 dinov2 helpers.py:102] Training  [  3650/125000]  eta: 2 days, 22:06:37  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2932 (13.0254)  dino_local_crops_loss: 8.6635 (9.1801)  dino_global_crops_loss: 1.0656 (1.1392)  koleo_loss: -0.0236 (-0.0008)  ibot_loss: 2.5967 (2.7069)  time: 2.030232  data: 1.395410  max mem: 33127
I20241029 23:08:36 2766422 dinov2 helpers.py:102] Training  [  3660/125000]  eta: 2 days, 22:06:47  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2937 (13.0234)  dino_local_crops_loss: 8.6540 (9.1787)  dino_global_crops_loss: 1.0630 (1.1390)  koleo_loss: -0.0238 (-0.0009)  ibot_loss: 2.6066 (2.7066)  time: 1.998063  data: 1.363766  max mem: 33127
I20241029 23:08:55 2766422 dinov2 helpers.py:102] Training  [  3670/125000]  eta: 2 days, 22:05:37  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3294 (13.0216)  dino_local_crops_loss: 8.6670 (9.1774)  dino_global_crops_loss: 1.0668 (1.1388)  koleo_loss: -0.0240 (-0.0009)  ibot_loss: 2.6092 (2.7064)  time: 2.052627  data: 1.418229  max mem: 33127
I20241029 23:09:18 2766422 dinov2 helpers.py:102] Training  [  3680/125000]  eta: 2 days, 22:06:27  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3347 (13.0197)  dino_local_crops_loss: 8.6668 (9.1760)  dino_global_crops_loss: 1.0690 (1.1386)  koleo_loss: -0.0238 (-0.0010)  ibot_loss: 2.6039 (2.7061)  time: 2.113074  data: 1.479555  max mem: 33127
I20241029 23:09:38 2766422 dinov2 helpers.py:102] Training  [  3690/125000]  eta: 2 days, 22:05:34  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3347 (13.0178)  dino_local_crops_loss: 8.6785 (9.1746)  dino_global_crops_loss: 1.0691 (1.1384)  koleo_loss: -0.0237 (-0.0011)  ibot_loss: 2.6018 (2.7058)  time: 2.138482  data: 1.506398  max mem: 33127
I20241029 23:10:00 2766422 dinov2 helpers.py:102] Training  [  3700/125000]  eta: 2 days, 22:05:31  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3427 (13.0158)  dino_local_crops_loss: 8.6988 (9.1732)  dino_global_crops_loss: 1.0709 (1.1382)  koleo_loss: -0.0237 (-0.0011)  ibot_loss: 2.6027 (2.7055)  time: 2.057934  data: 1.426040  max mem: 33127
I20241029 23:10:23 2766422 dinov2 helpers.py:102] Training  [  3710/125000]  eta: 2 days, 22:06:31  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3082 (13.0138)  dino_local_crops_loss: 8.6636 (9.1718)  dino_global_crops_loss: 1.0665 (1.1380)  koleo_loss: -0.0241 (-0.0012)  ibot_loss: 2.5937 (2.7052)  time: 2.230732  data: 1.597587  max mem: 33127
I20241029 23:10:46 2766422 dinov2 helpers.py:102] Training  [  3720/125000]  eta: 2 days, 22:07:18  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2855 (13.0119)  dino_local_crops_loss: 8.6432 (9.1705)  dino_global_crops_loss: 1.0644 (1.1378)  koleo_loss: -0.0241 (-0.0013)  ibot_loss: 2.5937 (2.7049)  time: 2.308935  data: 1.675747  max mem: 33127
I20241029 23:11:06 2766422 dinov2 helpers.py:102] Training  [  3730/125000]  eta: 2 days, 22:06:54  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2895 (13.0101)  dino_local_crops_loss: 8.6631 (9.1691)  dino_global_crops_loss: 1.0656 (1.1376)  koleo_loss: -0.0240 (-0.0013)  ibot_loss: 2.5945 (2.7046)  time: 2.180377  data: 1.547883  max mem: 33127
I20241029 23:11:27 2766422 dinov2 helpers.py:102] Training  [  3740/125000]  eta: 2 days, 22:06:33  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2895 (13.0081)  dino_local_crops_loss: 8.6552 (9.1677)  dino_global_crops_loss: 1.0649 (1.1374)  koleo_loss: -0.0239 (-0.0014)  ibot_loss: 2.5918 (2.7043)  time: 2.076745  data: 1.443630  max mem: 33127
I20241029 23:11:33 2766422 fvcore.common.checkpoint __init__.py:109] Saving checkpoint to /gpfs/data/shenlab/wz1492/HCC/dinov2/experiments/model_0003749.rank_0.pth
I20241029 23:11:51 2766422 dinov2 helpers.py:102] Training  [  3750/125000]  eta: 2 days, 22:07:45  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2994 (13.0063)  dino_local_crops_loss: 8.6587 (9.1665)  dino_global_crops_loss: 1.0664 (1.1372)  koleo_loss: -0.0241 (-0.0014)  ibot_loss: 2.5954 (2.7040)  time: 2.224416  data: 1.454265  max mem: 33127
I20241029 23:12:09 2766422 dinov2 helpers.py:102] Training  [  3760/125000]  eta: 2 days, 22:06:00  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.3045 (13.0044)  dino_local_crops_loss: 8.6727 (9.1651)  dino_global_crops_loss: 1.0673 (1.1371)  koleo_loss: -0.0241 (-0.0015)  ibot_loss: 2.5954 (2.7037)  time: 2.094306  data: 1.324221  max mem: 33127
I20241029 23:12:27 2766422 dinov2 helpers.py:102] Training  [  3770/125000]  eta: 2 days, 22:04:14  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2754 (13.0025)  dino_local_crops_loss: 8.6515 (9.1637)  dino_global_crops_loss: 1.0636 (1.1369)  koleo_loss: -0.0239 (-0.0016)  ibot_loss: 2.5900 (2.7034)  time: 1.819541  data: 1.184436  max mem: 33127
I20241029 23:12:47 2766422 dinov2 helpers.py:102] Training  [  3780/125000]  eta: 2 days, 22:03:22  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2487 (13.0006)  dino_local_crops_loss: 8.6299 (9.1624)  dino_global_crops_loss: 1.0655 (1.1367)  koleo_loss: -0.0238 (-0.0016)  ibot_loss: 2.5830 (2.7031)  time: 1.900435  data: 1.263987  max mem: 33127
I20241029 23:13:08 2766422 dinov2 helpers.py:102] Training  [  3790/125000]  eta: 2 days, 22:03:00  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2751 (12.9987)  dino_local_crops_loss: 8.6456 (9.1611)  dino_global_crops_loss: 1.0680 (1.1365)  koleo_loss: -0.0239 (-0.0017)  ibot_loss: 2.5789 (2.7028)  time: 2.030442  data: 1.394461  max mem: 33127
I20241029 23:13:29 2766422 dinov2 helpers.py:102] Training  [  3800/125000]  eta: 2 days, 22:02:49  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2771 (12.9968)  dino_local_crops_loss: 8.6456 (9.1597)  dino_global_crops_loss: 1.0673 (1.1363)  koleo_loss: -0.0239 (-0.0017)  ibot_loss: 2.5882 (2.7025)  time: 2.093402  data: 1.458461  max mem: 33127
I20241029 23:13:50 2766422 dinov2 helpers.py:102] Training  [  3810/125000]  eta: 2 days, 22:02:26  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2477 (12.9948)  dino_local_crops_loss: 8.6220 (9.1583)  dino_global_crops_loss: 1.0634 (1.1361)  koleo_loss: -0.0243 (-0.0018)  ibot_loss: 2.5852 (2.7022)  time: 2.091410  data: 1.456912  max mem: 33127
I20241029 23:14:10 2766422 dinov2 helpers.py:102] Training  [  3820/125000]  eta: 2 days, 22:01:47  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2544 (12.9930)  dino_local_crops_loss: 8.6359 (9.1570)  dino_global_crops_loss: 1.0650 (1.1359)  koleo_loss: -0.0243 (-0.0019)  ibot_loss: 2.5791 (2.7019)  time: 2.048231  data: 1.413318  max mem: 33127
I20241029 23:14:32 2766422 dinov2 helpers.py:102] Training  [  3830/125000]  eta: 2 days, 22:02:07  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2682 (12.9911)  dino_local_crops_loss: 8.6385 (9.1557)  dino_global_crops_loss: 1.0650 (1.1358)  koleo_loss: -0.0241 (-0.0019)  ibot_loss: 2.5766 (2.7015)  time: 2.117639  data: 1.482677  max mem: 33127
I20241029 23:14:53 2766422 dinov2 helpers.py:102] Training  [  3840/125000]  eta: 2 days, 22:02:01  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2503 (12.9893)  dino_local_crops_loss: 8.6337 (9.1545)  dino_global_crops_loss: 1.0640 (1.1356)  koleo_loss: -0.0236 (-0.0020)  ibot_loss: 2.5794 (2.7012)  time: 2.168351  data: 1.534481  max mem: 33127
I20241029 23:15:14 2766422 dinov2 helpers.py:102] Training  [  3850/125000]  eta: 2 days, 22:01:18  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2593 (12.9874)  dino_local_crops_loss: 8.6591 (9.1531)  dino_global_crops_loss: 1.0669 (1.1354)  koleo_loss: -0.0234 (-0.0020)  ibot_loss: 2.5837 (2.7009)  time: 2.067749  data: 1.434499  max mem: 33127
I20241029 23:15:36 2766422 dinov2 helpers.py:102] Training  [  3860/125000]  eta: 2 days, 22:02:04  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2593 (12.9856)  dino_local_crops_loss: 8.6619 (9.1519)  dino_global_crops_loss: 1.0672 (1.1352)  koleo_loss: -0.0236 (-0.0021)  ibot_loss: 2.5761 (2.7006)  time: 2.152861  data: 1.519053  max mem: 33127
I20241029 23:15:56 2766422 dinov2 helpers.py:102] Training  [  3870/125000]  eta: 2 days, 22:01:17  lr: 0.0004 (0.0002)  wd: 0.0408 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2390 (12.9837)  dino_local_crops_loss: 8.6386 (9.1506)  dino_global_crops_loss: 1.0649 (1.1350)  koleo_loss: -0.0239 (-0.0021)  ibot_loss: 2.5757 (2.7002)  time: 2.146504  data: 1.513079  max mem: 33127
I20241029 23:16:19 2766422 dinov2 helpers.py:102] Training  [  3880/125000]  eta: 2 days, 22:01:40  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2663 (12.9819)  dino_local_crops_loss: 8.6538 (9.1493)  dino_global_crops_loss: 1.0653 (1.1349)  koleo_loss: -0.0241 (-0.0022)  ibot_loss: 2.5677 (2.6999)  time: 2.108942  data: 1.475903  max mem: 33127
I20241029 23:16:40 2766422 dinov2 helpers.py:102] Training  [  3890/125000]  eta: 2 days, 22:01:27  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.2350 (12.9799)  dino_local_crops_loss: 8.6409 (9.1479)  dino_global_crops_loss: 1.0653 (1.1347)  koleo_loss: -0.0242 (-0.0022)  ibot_loss: 2.5573 (2.6995)  time: 2.162873  data: 1.530418  max mem: 33127
I20241029 23:17:00 2766422 dinov2 helpers.py:102] Training  [  3900/125000]  eta: 2 days, 22:00:39  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1854 (12.9780)  dino_local_crops_loss: 8.5920 (9.1466)  dino_global_crops_loss: 1.0566 (1.1345)  koleo_loss: -0.0242 (-0.0023)  ibot_loss: 2.5574 (2.6992)  time: 2.050333  data: 1.418311  max mem: 33127
I20241029 23:17:20 2766422 dinov2 helpers.py:102] Training  [  3910/125000]  eta: 2 days, 21:59:53  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1785 (12.9759)  dino_local_crops_loss: 8.5920 (9.1451)  dino_global_crops_loss: 1.0559 (1.1343)  koleo_loss: -0.0240 (-0.0024)  ibot_loss: 2.5679 (2.6989)  time: 1.997455  data: 1.364372  max mem: 33127
I20241029 23:17:42 2766422 dinov2 helpers.py:102] Training  [  3920/125000]  eta: 2 days, 22:00:25  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1823 (12.9740)  dino_local_crops_loss: 8.5915 (9.1438)  dino_global_crops_loss: 1.0576 (1.1341)  koleo_loss: -0.0239 (-0.0024)  ibot_loss: 2.5605 (2.6985)  time: 2.125829  data: 1.491401  max mem: 33127
I20241029 23:18:03 2766422 dinov2 helpers.py:102] Training  [  3930/125000]  eta: 2 days, 21:59:54  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1823 (12.9720)  dino_local_crops_loss: 8.5881 (9.1424)  dino_global_crops_loss: 1.0557 (1.1339)  koleo_loss: -0.0242 (-0.0025)  ibot_loss: 2.5589 (2.6982)  time: 2.149869  data: 1.515601  max mem: 33127
I20241029 23:18:24 2766422 dinov2 helpers.py:102] Training  [  3940/125000]  eta: 2 days, 21:59:52  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1355 (12.9698)  dino_local_crops_loss: 8.5489 (9.1409)  dino_global_crops_loss: 1.0496 (1.1337)  koleo_loss: -0.0243 (-0.0025)  ibot_loss: 2.5504 (2.6978)  time: 2.095342  data: 1.460922  max mem: 33127
I20241029 23:18:48 2766422 dinov2 helpers.py:102] Training  [  3950/125000]  eta: 2 days, 22:00:55  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1146 (12.9677)  dino_local_crops_loss: 8.5391 (9.1394)  dino_global_crops_loss: 1.0498 (1.1335)  koleo_loss: -0.0241 (-0.0026)  ibot_loss: 2.5526 (2.6974)  time: 2.249534  data: 1.614661  max mem: 33127
I20241029 23:19:10 2766422 dinov2 helpers.py:102] Training  [  3960/125000]  eta: 2 days, 22:01:29  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.1081 (12.9654)  dino_local_crops_loss: 8.5110 (9.1378)  dino_global_crops_loss: 1.0492 (1.1333)  koleo_loss: -0.0238 (-0.0026)  ibot_loss: 2.5526 (2.6970)  time: 2.308330  data: 1.674440  max mem: 33127
I20241029 23:19:30 2766422 dinov2 helpers.py:102] Training  [  3970/125000]  eta: 2 days, 22:00:20  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0591 (12.9632)  dino_local_crops_loss: 8.4847 (9.1361)  dino_global_crops_loss: 1.0430 (1.1330)  koleo_loss: -0.0237 (-0.0027)  ibot_loss: 2.5552 (2.6967)  time: 2.093506  data: 1.459194  max mem: 33127
I20241029 23:19:52 2766422 dinov2 helpers.py:102] Training  [  3980/125000]  eta: 2 days, 22:00:41  lr: 0.0004 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0004 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0650 (12.9609)  dino_local_crops_loss: 8.4847 (9.1345)  dino_global_crops_loss: 1.0427 (1.1328)  koleo_loss: -0.0240 (-0.0027)  ibot_loss: 2.5561 (2.6963)  time: 2.072986  data: 1.437070  max mem: 33127
I20241029 23:20:16 2766422 dinov2 helpers.py:102] Training  [  3990/125000]  eta: 2 days, 22:02:14  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0347 (12.9587)  dino_local_crops_loss: 8.4782 (9.1329)  dino_global_crops_loss: 1.0445 (1.1326)  koleo_loss: -0.0244 (-0.0028)  ibot_loss: 2.5463 (2.6959)  time: 2.338561  data: 1.702483  max mem: 33127
I20241029 23:20:37 2766422 dinov2 helpers.py:102] Training  [  4000/125000]  eta: 2 days, 22:02:01  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0075 (12.9562)  dino_local_crops_loss: 8.4482 (9.1311)  dino_global_crops_loss: 1.0400 (1.1324)  koleo_loss: -0.0243 (-0.0028)  ibot_loss: 2.5418 (2.6955)  time: 2.282765  data: 1.647757  max mem: 33127
I20241029 23:20:59 2766422 dinov2 helpers.py:102] Training  [  4010/125000]  eta: 2 days, 22:01:52  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9985 (12.9539)  dino_local_crops_loss: 8.4413 (9.1295)  dino_global_crops_loss: 1.0377 (1.1321)  koleo_loss: -0.0241 (-0.0029)  ibot_loss: 2.5461 (2.6952)  time: 2.116961  data: 1.481250  max mem: 33127
I20241029 23:21:18 2766422 dinov2 helpers.py:102] Training  [  4020/125000]  eta: 2 days, 22:00:56  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0042 (12.9515)  dino_local_crops_loss: 8.4413 (9.1278)  dino_global_crops_loss: 1.0381 (1.1319)  koleo_loss: -0.0241 (-0.0030)  ibot_loss: 2.5402 (2.6948)  time: 2.046049  data: 1.408400  max mem: 33127
I20241029 23:21:41 2766422 dinov2 helpers.py:102] Training  [  4030/125000]  eta: 2 days, 22:01:19  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9774 (12.9491)  dino_local_crops_loss: 8.4295 (9.1261)  dino_global_crops_loss: 1.0364 (1.1317)  koleo_loss: -0.0238 (-0.0030)  ibot_loss: 2.5354 (2.6944)  time: 2.097378  data: 1.457209  max mem: 33127
I20241029 23:22:03 2766422 dinov2 helpers.py:102] Training  [  4040/125000]  eta: 2 days, 22:01:55  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9949 (12.9469)  dino_local_crops_loss: 8.4376 (9.1245)  dino_global_crops_loss: 1.0428 (1.1314)  koleo_loss: -0.0241 (-0.0031)  ibot_loss: 2.5432 (2.6940)  time: 2.250706  data: 1.610860  max mem: 33127
I20241029 23:22:22 2766422 dinov2 helpers.py:102] Training  [  4050/125000]  eta: 2 days, 22:00:23  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9888 (12.9444)  dino_local_crops_loss: 8.4428 (9.1227)  dino_global_crops_loss: 1.0367 (1.1312)  koleo_loss: -0.0244 (-0.0031)  ibot_loss: 2.5403 (2.6936)  time: 2.060459  data: 1.422989  max mem: 33127
I20241029 23:22:41 2766422 dinov2 helpers.py:102] Training  [  4060/125000]  eta: 2 days, 21:59:11  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0159 (12.9423)  dino_local_crops_loss: 8.4479 (9.1212)  dino_global_crops_loss: 1.0361 (1.1310)  koleo_loss: -0.0242 (-0.0032)  ibot_loss: 2.5378 (2.6932)  time: 1.879120  data: 1.240968  max mem: 33127
I20241029 23:23:03 2766422 dinov2 helpers.py:102] Training  [  4070/125000]  eta: 2 days, 21:59:39  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0527 (12.9400)  dino_local_crops_loss: 8.4830 (9.1196)  dino_global_crops_loss: 1.0450 (1.1308)  koleo_loss: -0.0241 (-0.0032)  ibot_loss: 2.5422 (2.6929)  time: 2.079106  data: 1.440036  max mem: 33127
I20241029 23:23:25 2766422 dinov2 helpers.py:102] Training  [  4080/125000]  eta: 2 days, 21:59:42  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9911 (12.9378)  dino_local_crops_loss: 8.4321 (9.1180)  dino_global_crops_loss: 1.0357 (1.1305)  koleo_loss: -0.0243 (-0.0033)  ibot_loss: 2.5410 (2.6925)  time: 2.206889  data: 1.568171  max mem: 33127
I20241029 23:23:45 2766422 dinov2 helpers.py:102] Training  [  4090/125000]  eta: 2 days, 21:58:49  lr: 0.0005 (0.0002)  wd: 0.0409 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9580 (12.9354)  dino_local_crops_loss: 8.4179 (9.1163)  dino_global_crops_loss: 1.0357 (1.1303)  koleo_loss: -0.0244 (-0.0033)  ibot_loss: 2.5315 (2.6921)  time: 2.069447  data: 1.431360  max mem: 33127
I20241029 23:24:07 2766422 dinov2 helpers.py:102] Training  [  4100/125000]  eta: 2 days, 21:59:01  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9770 (12.9331)  dino_local_crops_loss: 8.4249 (9.1147)  dino_global_crops_loss: 1.0418 (1.1301)  koleo_loss: -0.0240 (-0.0034)  ibot_loss: 2.5280 (2.6917)  time: 2.084608  data: 1.446375  max mem: 33127
I20241029 23:24:30 2766422 dinov2 helpers.py:102] Training  [  4110/125000]  eta: 2 days, 21:59:45  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0347 (12.9310)  dino_local_crops_loss: 8.5007 (9.1132)  dino_global_crops_loss: 1.0441 (1.1299)  koleo_loss: -0.0240 (-0.0034)  ibot_loss: 2.5253 (2.6913)  time: 2.250667  data: 1.612885  max mem: 33127
I20241029 23:24:49 2766422 dinov2 helpers.py:102] Training  [  4120/125000]  eta: 2 days, 21:58:42  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0005 (12.9286)  dino_local_crops_loss: 8.4715 (9.1116)  dino_global_crops_loss: 1.0410 (1.1297)  koleo_loss: -0.0242 (-0.0035)  ibot_loss: 2.5145 (2.6908)  time: 2.122696  data: 1.485312  max mem: 33127
I20241029 23:25:13 2766422 dinov2 helpers.py:102] Training  [  4130/125000]  eta: 2 days, 21:59:51  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 12.0005 (12.9264)  dino_local_crops_loss: 8.4671 (9.1100)  dino_global_crops_loss: 1.0410 (1.1294)  koleo_loss: -0.0244 (-0.0035)  ibot_loss: 2.5145 (2.6904)  time: 2.165221  data: 1.527001  max mem: 33127
I20241029 23:25:35 2766422 dinov2 helpers.py:102] Training  [  4140/125000]  eta: 2 days, 22:00:10  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9959 (12.9241)  dino_local_crops_loss: 8.4650 (9.1085)  dino_global_crops_loss: 1.0396 (1.1292)  koleo_loss: -0.0243 (-0.0036)  ibot_loss: 2.5191 (2.6900)  time: 2.306257  data: 1.669260  max mem: 33127
I20241029 23:25:58 2766422 dinov2 helpers.py:102] Training  [  4150/125000]  eta: 2 days, 22:00:53  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9458 (12.9217)  dino_local_crops_loss: 8.4212 (9.1068)  dino_global_crops_loss: 1.0354 (1.1290)  koleo_loss: -0.0244 (-0.0036)  ibot_loss: 2.5055 (2.6895)  time: 2.263122  data: 1.627739  max mem: 33127
I20241029 23:26:21 2766422 dinov2 helpers.py:102] Training  [  4160/125000]  eta: 2 days, 22:01:11  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9458 (12.9195)  dino_local_crops_loss: 8.4220 (9.1052)  dino_global_crops_loss: 1.0354 (1.1288)  koleo_loss: -0.0244 (-0.0037)  ibot_loss: 2.4962 (2.6891)  time: 2.261431  data: 1.626356  max mem: 33127
I20241029 23:26:40 2766422 dinov2 helpers.py:102] Training  [  4170/125000]  eta: 2 days, 22:00:21  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9557 (12.9171)  dino_local_crops_loss: 8.4327 (9.1036)  dino_global_crops_loss: 1.0365 (1.1286)  koleo_loss: -0.0241 (-0.0037)  ibot_loss: 2.5015 (2.6886)  time: 2.101425  data: 1.466519  max mem: 33127
I20241029 23:27:01 2766422 dinov2 helpers.py:102] Training  [  4180/125000]  eta: 2 days, 21:59:56  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9000 (12.9148)  dino_local_crops_loss: 8.4030 (9.1020)  dino_global_crops_loss: 1.0336 (1.1283)  koleo_loss: -0.0240 (-0.0038)  ibot_loss: 2.4888 (2.6882)  time: 2.028471  data: 1.392153  max mem: 33127
I20241029 23:27:23 2766422 dinov2 helpers.py:102] Training  [  4190/125000]  eta: 2 days, 21:59:59  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.9082 (12.9125)  dino_local_crops_loss: 8.4076 (9.1005)  dino_global_crops_loss: 1.0361 (1.1281)  koleo_loss: -0.0240 (-0.0038)  ibot_loss: 2.4863 (2.6877)  time: 2.120120  data: 1.482925  max mem: 33127
I20241029 23:27:43 2766422 dinov2 helpers.py:102] Training  [  4200/125000]  eta: 2 days, 21:59:17  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8966 (12.9100)  dino_local_crops_loss: 8.4076 (9.0987)  dino_global_crops_loss: 1.0319 (1.1279)  koleo_loss: -0.0242 (-0.0039)  ibot_loss: 2.4845 (2.6872)  time: 2.091070  data: 1.455397  max mem: 33127
I20241029 23:28:04 2766422 dinov2 helpers.py:102] Training  [  4210/125000]  eta: 2 days, 21:59:13  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8966 (12.9077)  dino_local_crops_loss: 8.4110 (9.0971)  dino_global_crops_loss: 1.0300 (1.1277)  koleo_loss: -0.0243 (-0.0039)  ibot_loss: 2.4845 (2.6867)  time: 2.078717  data: 1.443594  max mem: 33127
I20241029 23:28:26 2766422 dinov2 helpers.py:102] Training  [  4220/125000]  eta: 2 days, 21:59:15  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8999 (12.9053)  dino_local_crops_loss: 8.4111 (9.0955)  dino_global_crops_loss: 1.0326 (1.1275)  koleo_loss: -0.0244 (-0.0040)  ibot_loss: 2.4875 (2.6863)  time: 2.154793  data: 1.519412  max mem: 33127
I20241029 23:28:48 2766422 dinov2 helpers.py:102] Training  [  4230/125000]  eta: 2 days, 21:59:10  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8981 (12.9028)  dino_local_crops_loss: 8.4097 (9.0938)  dino_global_crops_loss: 1.0326 (1.1272)  koleo_loss: -0.0244 (-0.0040)  ibot_loss: 2.4725 (2.6858)  time: 2.154611  data: 1.519658  max mem: 33127
I20241029 23:29:07 2766422 dinov2 helpers.py:102] Training  [  4240/125000]  eta: 2 days, 21:58:12  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8186 (12.9004)  dino_local_crops_loss: 8.3428 (9.0921)  dino_global_crops_loss: 1.0254 (1.1270)  koleo_loss: -0.0241 (-0.0041)  ibot_loss: 2.4739 (2.6853)  time: 2.047695  data: 1.412978  max mem: 33127
I20241029 23:29:29 2766422 dinov2 helpers.py:102] Training  [  4250/125000]  eta: 2 days, 21:58:09  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8418 (12.8979)  dino_local_crops_loss: 8.3517 (9.0904)  dino_global_crops_loss: 1.0264 (1.1268)  koleo_loss: -0.0241 (-0.0041)  ibot_loss: 2.4739 (2.6848)  time: 2.051370  data: 1.415622  max mem: 33127
I20241029 23:29:49 2766422 dinov2 helpers.py:102] Training  [  4260/125000]  eta: 2 days, 21:57:38  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8515 (12.8955)  dino_local_crops_loss: 8.3744 (9.0888)  dino_global_crops_loss: 1.0290 (1.1265)  koleo_loss: -0.0240 (-0.0041)  ibot_loss: 2.4730 (2.6843)  time: 2.100132  data: 1.464439  max mem: 33127
I20241029 23:30:11 2766422 dinov2 helpers.py:102] Training  [  4270/125000]  eta: 2 days, 21:57:44  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8535 (12.8930)  dino_local_crops_loss: 8.3684 (9.0871)  dino_global_crops_loss: 1.0290 (1.1263)  koleo_loss: -0.0240 (-0.0042)  ibot_loss: 2.4730 (2.6838)  time: 2.115167  data: 1.480223  max mem: 33127
I20241029 23:30:30 2766422 dinov2 helpers.py:102] Training  [  4280/125000]  eta: 2 days, 21:56:21  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.8501 (12.8906)  dino_local_crops_loss: 8.3680 (9.0855)  dino_global_crops_loss: 1.0272 (1.1261)  koleo_loss: -0.0244 (-0.0042)  ibot_loss: 2.4764 (2.6833)  time: 2.024122  data: 1.390254  max mem: 33127
I20241029 23:30:51 2766422 dinov2 helpers.py:102] Training  [  4290/125000]  eta: 2 days, 21:56:11  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0403)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.7970 (12.8880)  dino_local_crops_loss: 8.3545 (9.0836)  dino_global_crops_loss: 1.0253 (1.1258)  koleo_loss: -0.0244 (-0.0043)  ibot_loss: 2.4579 (2.6828)  time: 1.995752  data: 1.362118  max mem: 33127
I20241029 23:31:12 2766422 dinov2 helpers.py:102] Training  [  4300/125000]  eta: 2 days, 21:56:04  lr: 0.0005 (0.0002)  wd: 0.0410 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.7271 (12.8853)  dino_local_crops_loss: 8.2922 (9.0818)  dino_global_crops_loss: 1.0188 (1.1256)  koleo_loss: -0.0244 (-0.0043)  ibot_loss: 2.4409 (2.6822)  time: 2.129906  data: 1.494750  max mem: 33127
I20241029 23:31:34 2766422 dinov2 helpers.py:102] Training  [  4310/125000]  eta: 2 days, 21:55:55  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.7715 (12.8826)  dino_local_crops_loss: 8.3238 (9.0800)  dino_global_crops_loss: 1.0232 (1.1253)  koleo_loss: -0.0244 (-0.0044)  ibot_loss: 2.4394 (2.6817)  time: 2.132348  data: 1.496769  max mem: 33127
I20241029 23:31:54 2766422 dinov2 helpers.py:102] Training  [  4320/125000]  eta: 2 days, 21:55:13  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.7140 (12.8799)  dino_local_crops_loss: 8.2973 (9.0781)  dino_global_crops_loss: 1.0217 (1.1251)  koleo_loss: -0.0246 (-0.0044)  ibot_loss: 2.4340 (2.6811)  time: 2.069273  data: 1.432950  max mem: 33127
I20241029 23:32:13 2766422 dinov2 helpers.py:102] Training  [  4330/125000]  eta: 2 days, 21:54:12  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.6736 (12.8771)  dino_local_crops_loss: 8.2591 (9.0762)  dino_global_crops_loss: 1.0129 (1.1248)  koleo_loss: -0.0245 (-0.0045)  ibot_loss: 2.4283 (2.6805)  time: 1.974953  data: 1.337817  max mem: 33127
I20241029 23:32:33 2766422 dinov2 helpers.py:102] Training  [  4340/125000]  eta: 2 days, 21:53:40  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.6629 (12.8744)  dino_local_crops_loss: 8.2529 (9.0744)  dino_global_crops_loss: 1.0125 (1.1246)  koleo_loss: -0.0244 (-0.0045)  ibot_loss: 2.4232 (2.6799)  time: 1.993340  data: 1.355255  max mem: 33127
I20241029 23:32:57 2766422 dinov2 helpers.py:102] Training  [  4350/125000]  eta: 2 days, 21:54:34  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.7158 (12.8716)  dino_local_crops_loss: 8.2835 (9.0725)  dino_global_crops_loss: 1.0161 (1.1243)  koleo_loss: -0.0244 (-0.0046)  ibot_loss: 2.4229 (2.6793)  time: 2.200801  data: 1.563108  max mem: 33127
I20241029 23:33:16 2766422 dinov2 helpers.py:102] Training  [  4360/125000]  eta: 2 days, 21:53:11  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.6804 (12.8687)  dino_local_crops_loss: 8.2789 (9.0705)  dino_global_crops_loss: 1.0150 (1.1241)  koleo_loss: -0.0244 (-0.0046)  ibot_loss: 2.4163 (2.6787)  time: 2.107404  data: 1.472264  max mem: 33127
I20241029 23:33:35 2766422 dinov2 helpers.py:102] Training  [  4370/125000]  eta: 2 days, 21:52:22  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.5879 (12.8659)  dino_local_crops_loss: 8.2130 (9.0686)  dino_global_crops_loss: 1.0079 (1.1238)  koleo_loss: -0.0244 (-0.0047)  ibot_loss: 2.4056 (2.6781)  time: 1.921730  data: 1.287304  max mem: 33127
I20241029 23:33:57 2766422 dinov2 helpers.py:102] Training  [  4380/125000]  eta: 2 days, 21:52:25  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.5527 (12.8630)  dino_local_crops_loss: 8.1941 (9.0667)  dino_global_crops_loss: 1.0039 (1.1235)  koleo_loss: -0.0242 (-0.0047)  ibot_loss: 2.3936 (2.6775)  time: 2.078015  data: 1.443403  max mem: 33127
I20241029 23:34:18 2766422 dinov2 helpers.py:102] Training  [  4390/125000]  eta: 2 days, 21:51:50  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.5311 (12.8599)  dino_local_crops_loss: 8.1700 (9.0646)  dino_global_crops_loss: 1.0013 (1.1232)  koleo_loss: -0.0245 (-0.0047)  ibot_loss: 2.3899 (2.6768)  time: 2.102920  data: 1.468846  max mem: 33127
I20241029 23:34:39 2766422 dinov2 helpers.py:102] Training  [  4400/125000]  eta: 2 days, 21:51:49  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.5154 (12.8569)  dino_local_crops_loss: 8.1768 (9.0626)  dino_global_crops_loss: 1.0013 (1.1230)  koleo_loss: -0.0247 (-0.0048)  ibot_loss: 2.3818 (2.6761)  time: 2.095513  data: 1.461923  max mem: 33127
I20241029 23:35:01 2766422 dinov2 helpers.py:102] Training  [  4410/125000]  eta: 2 days, 21:51:50  lr: 0.0005 (0.0002)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.5154 (12.8538)  dino_local_crops_loss: 8.1816 (9.0605)  dino_global_crops_loss: 1.0016 (1.1227)  koleo_loss: -0.0242 (-0.0048)  ibot_loss: 2.3767 (2.6754)  time: 2.162660  data: 1.529886  max mem: 33127
I20241029 23:35:22 2766422 dinov2 helpers.py:102] Training  [  4420/125000]  eta: 2 days, 21:51:26  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.5539 (12.8508)  dino_local_crops_loss: 8.1816 (9.0585)  dino_global_crops_loss: 1.0034 (1.1224)  koleo_loss: -0.0241 (-0.0049)  ibot_loss: 2.3707 (2.6747)  time: 2.120245  data: 1.486996  max mem: 33127
I20241029 23:35:43 2766422 dinov2 helpers.py:102] Training  [  4430/125000]  eta: 2 days, 21:51:21  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.5218 (12.8477)  dino_local_crops_loss: 8.1551 (9.0565)  dino_global_crops_loss: 1.0002 (1.1221)  koleo_loss: -0.0240 (-0.0049)  ibot_loss: 2.3620 (2.6740)  time: 2.108424  data: 1.473820  max mem: 33127
I20241029 23:36:05 2766422 dinov2 helpers.py:102] Training  [  4440/125000]  eta: 2 days, 21:51:31  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4942 (12.8446)  dino_local_crops_loss: 8.1387 (9.0544)  dino_global_crops_loss: 0.9977 (1.1219)  koleo_loss: -0.0241 (-0.0050)  ibot_loss: 2.3602 (2.6733)  time: 2.172285  data: 1.537037  max mem: 33127
I20241029 23:36:26 2766422 dinov2 helpers.py:102] Training  [  4450/125000]  eta: 2 days, 21:51:07  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4570 (12.8414)  dino_local_crops_loss: 8.1317 (9.0523)  dino_global_crops_loss: 0.9956 (1.1216)  koleo_loss: -0.0240 (-0.0050)  ibot_loss: 2.3581 (2.6725)  time: 2.137533  data: 1.502250  max mem: 33127
I20241029 23:36:48 2766422 dinov2 helpers.py:102] Training  [  4460/125000]  eta: 2 days, 21:51:13  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4490 (12.8382)  dino_local_crops_loss: 8.1106 (9.0502)  dino_global_crops_loss: 0.9938 (1.1213)  koleo_loss: -0.0239 (-0.0050)  ibot_loss: 2.3543 (2.6718)  time: 2.128344  data: 1.492398  max mem: 33127
I20241029 23:37:08 2766422 dinov2 helpers.py:102] Training  [  4470/125000]  eta: 2 days, 21:50:44  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4164 (12.8351)  dino_local_crops_loss: 8.0948 (9.0481)  dino_global_crops_loss: 0.9914 (1.1210)  koleo_loss: -0.0238 (-0.0051)  ibot_loss: 2.3442 (2.6711)  time: 2.120661  data: 1.483963  max mem: 33127
I20241029 23:37:28 2766422 dinov2 helpers.py:102] Training  [  4480/125000]  eta: 2 days, 21:49:46  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4661 (12.8321)  dino_local_crops_loss: 8.1383 (9.0461)  dino_global_crops_loss: 0.9984 (1.1207)  koleo_loss: -0.0237 (-0.0051)  ibot_loss: 2.3433 (2.6703)  time: 2.003804  data: 1.366642  max mem: 33127
I20241029 23:37:50 2766422 dinov2 helpers.py:102] Training  [  4490/125000]  eta: 2 days, 21:50:03  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4305 (12.8288)  dino_local_crops_loss: 8.1383 (9.0440)  dino_global_crops_loss: 0.9961 (1.1204)  koleo_loss: -0.0236 (-0.0052)  ibot_loss: 2.3332 (2.6695)  time: 2.087731  data: 1.449656  max mem: 33127
I20241029 23:38:11 2766422 dinov2 helpers.py:102] Training  [  4500/125000]  eta: 2 days, 21:49:52  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3882 (12.8257)  dino_local_crops_loss: 8.0972 (9.0420)  dino_global_crops_loss: 0.9899 (1.1202)  koleo_loss: -0.0237 (-0.0052)  ibot_loss: 2.3222 (2.6688)  time: 2.175018  data: 1.538248  max mem: 33127
I20241029 23:38:33 2766422 dinov2 helpers.py:102] Training  [  4510/125000]  eta: 2 days, 21:49:53  lr: 0.0005 (0.0003)  wd: 0.0411 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4067 (12.8225)  dino_local_crops_loss: 8.1177 (9.0399)  dino_global_crops_loss: 0.9940 (1.1199)  koleo_loss: -0.0238 (-0.0053)  ibot_loss: 2.3222 (2.6680)  time: 2.144219  data: 1.509348  max mem: 33127
I20241029 23:38:53 2766422 dinov2 helpers.py:102] Training  [  4520/125000]  eta: 2 days, 21:49:22  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3277 (12.8192)  dino_local_crops_loss: 8.0752 (9.0377)  dino_global_crops_loss: 0.9929 (1.1196)  koleo_loss: -0.0241 (-0.0053)  ibot_loss: 2.3083 (2.6672)  time: 2.106706  data: 1.472278  max mem: 33127
I20241029 23:39:13 2766422 dinov2 helpers.py:102] Training  [  4530/125000]  eta: 2 days, 21:48:22  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3416 (12.8160)  dino_local_crops_loss: 8.0752 (9.0356)  dino_global_crops_loss: 0.9928 (1.1193)  koleo_loss: -0.0241 (-0.0053)  ibot_loss: 2.3130 (2.6664)  time: 1.994953  data: 1.359870  max mem: 33127
I20241029 23:39:33 2766422 dinov2 helpers.py:102] Training  [  4540/125000]  eta: 2 days, 21:47:58  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.4107 (12.8128)  dino_local_crops_loss: 8.1155 (9.0335)  dino_global_crops_loss: 0.9962 (1.1190)  koleo_loss: -0.0243 (-0.0054)  ibot_loss: 2.3123 (2.6656)  time: 2.007969  data: 1.370613  max mem: 33127
I20241029 23:39:54 2766422 dinov2 helpers.py:102] Training  [  4550/125000]  eta: 2 days, 21:47:22  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3656 (12.8096)  dino_local_crops_loss: 8.0992 (9.0315)  dino_global_crops_loss: 0.9920 (1.1188)  koleo_loss: -0.0243 (-0.0054)  ibot_loss: 2.3031 (2.6648)  time: 2.050682  data: 1.412112  max mem: 33127
I20241029 23:40:15 2766422 dinov2 helpers.py:102] Training  [  4560/125000]  eta: 2 days, 21:47:23  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3621 (12.8065)  dino_local_crops_loss: 8.0992 (9.0294)  dino_global_crops_loss: 0.9920 (1.1185)  koleo_loss: -0.0244 (-0.0055)  ibot_loss: 2.2930 (2.6640)  time: 2.097561  data: 1.459249  max mem: 33127
I20241029 23:40:36 2766422 dinov2 helpers.py:102] Training  [  4570/125000]  eta: 2 days, 21:46:43  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2862 (12.8031)  dino_local_crops_loss: 8.0397 (9.0273)  dino_global_crops_loss: 0.9822 (1.1182)  koleo_loss: -0.0244 (-0.0055)  ibot_loss: 2.2877 (2.6632)  time: 2.091239  data: 1.454483  max mem: 33127
I20241029 23:40:54 2766422 dinov2 helpers.py:102] Training  [  4580/125000]  eta: 2 days, 21:45:23  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2816 (12.7999)  dino_local_crops_loss: 8.0333 (9.0252)  dino_global_crops_loss: 0.9802 (1.1179)  koleo_loss: -0.0244 (-0.0055)  ibot_loss: 2.2815 (2.6623)  time: 1.937320  data: 1.301715  max mem: 33127
I20241029 23:41:14 2766422 dinov2 helpers.py:102] Training  [  4590/125000]  eta: 2 days, 21:44:45  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3102 (12.7966)  dino_local_crops_loss: 8.0556 (9.0231)  dino_global_crops_loss: 0.9840 (1.1176)  koleo_loss: -0.0244 (-0.0056)  ibot_loss: 2.2938 (2.6615)  time: 1.940866  data: 1.304571  max mem: 33127
I20241029 23:41:37 2766422 dinov2 helpers.py:102] Training  [  4600/125000]  eta: 2 days, 21:45:16  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3228 (12.7934)  dino_local_crops_loss: 8.0609 (9.0210)  dino_global_crops_loss: 0.9875 (1.1173)  koleo_loss: -0.0240 (-0.0056)  ibot_loss: 2.2869 (2.6607)  time: 2.151264  data: 1.514709  max mem: 33127
I20241029 23:41:56 2766422 dinov2 helpers.py:102] Training  [  4610/125000]  eta: 2 days, 21:43:57  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3134 (12.7902)  dino_local_crops_loss: 8.0799 (9.0189)  dino_global_crops_loss: 0.9878 (1.1170)  koleo_loss: -0.0237 (-0.0057)  ibot_loss: 2.2827 (2.6599)  time: 2.073453  data: 1.436909  max mem: 33127
I20241029 23:42:15 2766422 dinov2 helpers.py:102] Training  [  4620/125000]  eta: 2 days, 21:43:01  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3117 (12.7870)  dino_local_crops_loss: 8.0581 (9.0169)  dino_global_crops_loss: 0.9878 (1.1168)  koleo_loss: -0.0238 (-0.0057)  ibot_loss: 2.2655 (2.6590)  time: 1.906928  data: 1.269758  max mem: 33127
I20241029 23:42:35 2766422 dinov2 helpers.py:102] Training  [  4630/125000]  eta: 2 days, 21:42:04  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.3491 (12.7839)  dino_local_crops_loss: 8.0947 (9.0150)  dino_global_crops_loss: 0.9901 (1.1165)  koleo_loss: -0.0239 (-0.0057)  ibot_loss: 2.2732 (2.6582)  time: 1.947616  data: 1.309384  max mem: 33127
I20241029 23:42:56 2766422 dinov2 helpers.py:102] Training  [  4640/125000]  eta: 2 days, 21:41:51  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2681 (12.7806)  dino_local_crops_loss: 8.0647 (9.0128)  dino_global_crops_loss: 0.9892 (1.1162)  koleo_loss: -0.0239 (-0.0058)  ibot_loss: 2.2606 (2.6573)  time: 2.031382  data: 1.393289  max mem: 33127
I20241029 23:43:18 2766422 dinov2 helpers.py:102] Training  [  4650/125000]  eta: 2 days, 21:42:07  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2463 (12.7773)  dino_local_crops_loss: 8.0377 (9.0107)  dino_global_crops_loss: 0.9834 (1.1159)  koleo_loss: -0.0239 (-0.0058)  ibot_loss: 2.2538 (2.6565)  time: 2.170983  data: 1.534110  max mem: 33127
I20241029 23:43:38 2766422 dinov2 helpers.py:102] Training  [  4660/125000]  eta: 2 days, 21:41:09  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2584 (12.7741)  dino_local_crops_loss: 8.0468 (9.0087)  dino_global_crops_loss: 0.9834 (1.1156)  koleo_loss: -0.0239 (-0.0059)  ibot_loss: 2.2551 (2.6556)  time: 2.083315  data: 1.447045  max mem: 33127
I20241029 23:43:57 2766422 dinov2 helpers.py:102] Training  [  4670/125000]  eta: 2 days, 21:40:15  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2722 (12.7709)  dino_local_crops_loss: 8.0515 (9.0067)  dino_global_crops_loss: 0.9842 (1.1154)  koleo_loss: -0.0241 (-0.0059)  ibot_loss: 2.2564 (2.6548)  time: 1.948324  data: 1.311129  max mem: 33127
I20241029 23:44:18 2766422 dinov2 helpers.py:102] Training  [  4680/125000]  eta: 2 days, 21:39:59  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2722 (12.7677)  dino_local_crops_loss: 8.0515 (9.0046)  dino_global_crops_loss: 0.9846 (1.1151)  koleo_loss: -0.0245 (-0.0059)  ibot_loss: 2.2466 (2.6539)  time: 2.029903  data: 1.392056  max mem: 33127
I20241029 23:44:42 2766422 dinov2 helpers.py:102] Training  [  4690/125000]  eta: 2 days, 21:40:45  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2971 (12.7645)  dino_local_crops_loss: 8.0880 (9.0027)  dino_global_crops_loss: 0.9907 (1.1148)  koleo_loss: -0.0246 (-0.0060)  ibot_loss: 2.2403 (2.6530)  time: 2.224013  data: 1.586147  max mem: 33127
I20241029 23:45:03 2766422 dinov2 helpers.py:102] Training  [  4700/125000]  eta: 2 days, 21:40:39  lr: 0.0005 (0.0003)  wd: 0.0412 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2311 (12.7613)  dino_local_crops_loss: 8.0558 (9.0006)  dino_global_crops_loss: 0.9834 (1.1145)  koleo_loss: -0.0243 (-0.0060)  ibot_loss: 2.2333 (2.6521)  time: 2.243275  data: 1.606223  max mem: 33127
I20241029 23:45:26 2766422 dinov2 helpers.py:102] Training  [  4710/125000]  eta: 2 days, 21:41:03  lr: 0.0005 (0.0003)  wd: 0.0413 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2045 (12.7580)  dino_local_crops_loss: 8.0255 (8.9986)  dino_global_crops_loss: 0.9813 (1.1143)  koleo_loss: -0.0242 (-0.0061)  ibot_loss: 2.2199 (2.6512)  time: 2.201923  data: 1.566135  max mem: 33127
I20241029 23:45:48 2766422 dinov2 helpers.py:102] Training  [  4720/125000]  eta: 2 days, 21:41:27  lr: 0.0005 (0.0003)  wd: 0.0413 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2474 (12.7548)  dino_local_crops_loss: 8.0565 (8.9966)  dino_global_crops_loss: 0.9813 (1.1140)  koleo_loss: -0.0242 (-0.0061)  ibot_loss: 2.2192 (2.6503)  time: 2.262323  data: 1.626727  max mem: 33127
I20241029 23:46:08 2766422 dinov2 helpers.py:102] Training  [  4730/125000]  eta: 2 days, 21:40:30  lr: 0.0005 (0.0003)  wd: 0.0413 (0.0404)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0005 (0.0002)  current_batch_size: 64.0000 (64.0000)  total_loss: 11.2184 (12.7515)  dino_local_crops_loss: 8.0496 (8.9945)  dino_global_crops_loss: 0.9788 (1.1137)  koleo_loss: -0.0242 (-0.0061)  ibot_loss: 2.2140 (2.6494)  time: 2.103784  data: 1.468648  max mem: 33127
