I20241103 16:19:02 2286570 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 16:19:02 2286570 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 16:19:02 2286570 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 16:19:02 2286570 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 16:19:02 2286570 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 16:19:06 2286570 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 16:19:06 2286570 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 16:19:06 2286570 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
E20241103 16:19:06 2286570 submitit submission.py:68] Submitted job triggered an exception
I20241103 16:30:34 2287727 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 16:30:34 2287727 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 16:30:34 2287727 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 16:30:34 2287727 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 16:30:34 2287727 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 16:30:40 2287727 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 16:30:40 2287727 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 16:30:41 2287727 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
I20241103 16:30:41 2287727 dinov2 loaders.py:91] # of dataset samples: 34,076
E20241103 16:30:41 2287727 submitit submission.py:68] Submitted job triggered an exception
I20241103 20:49:26 1320750 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 20:49:26 1320750 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 20:49:26 1320750 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 20:49:26 1320750 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 20:49:26 1320750 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 20:49:32 1320750 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 20:49:32 1320750 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 20:49:33 1320750 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
E20241103 20:49:33 1320750 submitit submission.py:68] Submitted job triggered an exception
I20241103 20:50:54 1321058 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 20:50:54 1321058 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 20:50:54 1321058 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 20:50:54 1321058 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 20:50:54 1321058 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 20:50:57 1321058 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 20:50:57 1321058 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 20:50:58 1321058 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
I20241103 20:50:58 1321058 dinov2 loaders.py:91] # of dataset samples: 34,076
I20241103 20:50:59 1321058 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241103 20:50:59 1321058 dinov2 loaders.py:124] sampler: sharded infinite
I20241103 20:50:59 1321058 dinov2 loaders.py:208] using PyTorch data loader
I20241103 20:50:59 1321058 dinov2 loaders.py:223] infinite data loader
I20241103 20:50:59 1321058 dinov2 loaders.py:86] using dataset: "ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
I20241103 20:50:59 1321058 dinov2 loaders.py:91] # of dataset samples: 11,290
I20241103 20:50:59 1321058 dinov2 loaders.py:149] sampler: distributed
I20241103 20:50:59 1321058 dinov2 loaders.py:208] using PyTorch data loader
I20241103 20:50:59 1321058 dinov2 loaders.py:221] # of batches: 45
I20241103 20:50:59 1321058 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241103 20:50:59 1321058 dinov2 linear.py:338] Starting training from iteration 0
I20241103 20:51:03 1321058 dinov2 helpers.py:102] Training  [    0/12500]  eta: 11:37:09  loss: 147.6970 (147.6970)  lr: 0.0000 (0.0000)  time: 3.346354  data: 3.027611  max mem: 2723
I20241103 20:51:03 1321058 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241103 20:51:05 1321058 dinov2 helpers.py:102] Training  [   10/12500]  eta: 1:37:58  loss: 125.7663 (136.7317)  lr: 0.0000 (0.0000)  time: 0.470651  data: 0.315208  max mem: 3153
I20241103 20:51:07 1321058 dinov2 helpers.py:102] Training  [   20/12500]  eta: 1:12:16  loss: 125.7663 (129.8727)  lr: 0.0000 (0.0000)  time: 0.197512  data: 0.059349  max mem: 3153
I20241103 20:51:09 1321058 dinov2 helpers.py:102] Training  [   30/12500]  eta: 1:01:40  loss: 116.1546 (124.4868)  lr: 0.0000 (0.0000)  time: 0.201067  data: 0.064389  max mem: 3153
I20241103 20:51:11 1321058 dinov2 helpers.py:102] Training  [   40/12500]  eta: 0:56:44  loss: 116.1546 (120.4772)  lr: 0.0000 (0.0000)  time: 0.195278  data: 0.048644  max mem: 3153
I20241103 20:51:12 1321058 dinov2 helpers.py:102] Training  [   50/12500]  eta: 0:51:55  loss: 108.3294 (116.9892)  lr: 0.0000 (0.0000)  time: 0.178245  data: 0.024084  max mem: 3153
I20241103 20:51:14 1321058 dinov2 helpers.py:102] Training  [   60/12500]  eta: 0:48:05  loss: 108.3294 (114.5840)  lr: 0.0000 (0.0000)  time: 0.147260  data: 0.002571  max mem: 3153
I20241103 20:51:15 1321058 dinov2 helpers.py:102] Training  [   70/12500]  eta: 0:45:30  loss: 104.4386 (112.5669)  lr: 0.0000 (0.0000)  time: 0.141685  data: 0.000262  max mem: 3153
I20241103 20:51:16 1321058 dinov2 helpers.py:102] Training  [   80/12500]  eta: 0:43:24  loss: 104.4386 (110.4280)  lr: 0.0000 (0.0000)  time: 0.141825  data: 0.000361  max mem: 3153
I20241103 20:51:18 1321058 dinov2 helpers.py:102] Training  [   90/12500]  eta: 0:41:45  loss: 100.1526 (108.5346)  lr: 0.0000 (0.0000)  time: 0.138764  data: 0.000411  max mem: 3153
I20241103 20:51:19 1321058 dinov2 helpers.py:102] Training  [  100/12500]  eta: 0:40:30  loss: 100.1526 (107.7010)  lr: 0.0000 (0.0000)  time: 0.140723  data: 0.000367  max mem: 3153
I20241103 20:51:21 1321058 dinov2 helpers.py:102] Training  [  110/12500]  eta: 0:39:25  loss: 99.5493 (106.4462)  lr: 0.0000 (0.0000)  time: 0.140767  data: 0.000342  max mem: 3153
I20241103 20:51:22 1321058 dinov2 helpers.py:102] Training  [  120/12500]  eta: 0:38:40  loss: 99.5493 (105.5042)  lr: 0.0000 (0.0000)  time: 0.144066  data: 0.005684  max mem: 3153
I20241103 20:51:24 1321058 dinov2 helpers.py:102] Training  [  130/12500]  eta: 0:37:57  loss: 99.3648 (104.5974)  lr: 0.0000 (0.0000)  time: 0.146732  data: 0.007726  max mem: 3153
I20241103 20:51:25 1321058 dinov2 helpers.py:102] Training  [  140/12500]  eta: 0:37:16  loss: 99.3648 (103.4782)  lr: 0.0000 (0.0000)  time: 0.141700  data: 0.002368  max mem: 3153
I20241103 20:51:26 1321058 dinov2 helpers.py:102] Training  [  150/12500]  eta: 0:36:40  loss: 98.4474 (102.5098)  lr: 0.0000 (0.0000)  time: 0.138865  data: 0.000247  max mem: 3153
I20241103 20:51:28 1321058 dinov2 helpers.py:102] Training  [  160/12500]  eta: 0:36:08  loss: 98.4474 (101.7228)  lr: 0.0000 (0.0000)  time: 0.138889  data: 0.000208  max mem: 3153
I20241103 20:51:29 1321058 dinov2 helpers.py:102] Training  [  170/12500]  eta: 0:35:40  loss: 94.2001 (100.9141)  lr: 0.0000 (0.0000)  time: 0.139025  data: 0.000233  max mem: 3153
I20241103 20:51:31 1321058 dinov2 helpers.py:102] Training  [  180/12500]  eta: 0:35:14  loss: 94.2001 (100.3075)  lr: 0.0000 (0.0000)  time: 0.138961  data: 0.000267  max mem: 3153
I20241103 20:51:32 1321058 dinov2 helpers.py:102] Training  [  190/12500]  eta: 0:34:52  loss: 93.3168 (99.6546)  lr: 0.0000 (0.0000)  time: 0.139088  data: 0.000266  max mem: 3153
I20241103 20:51:33 1321058 dinov2 helpers.py:102] Training  [  200/12500]  eta: 0:34:31  loss: 92.8092 (99.0047)  lr: 0.0000 (0.0000)  time: 0.139233  data: 0.000238  max mem: 3153
I20241103 20:51:35 1321058 dinov2 helpers.py:102] Training  [  210/12500]  eta: 0:34:13  loss: 92.6434 (98.6542)  lr: 0.0000 (0.0000)  time: 0.139292  data: 0.000212  max mem: 3153
I20241103 20:51:36 1321058 dinov2 helpers.py:102] Training  [  220/12500]  eta: 0:33:55  loss: 91.4944 (98.1200)  lr: 0.0000 (0.0000)  time: 0.139235  data: 0.000228  max mem: 3153
I20241103 20:51:38 1321058 dinov2 helpers.py:102] Training  [  230/12500]  eta: 0:33:40  loss: 91.2938 (97.2275)  lr: 0.0000 (0.0000)  time: 0.139249  data: 0.000255  max mem: 3153
I20241103 20:51:39 1321058 dinov2 helpers.py:102] Training  [  240/12500]  eta: 0:33:25  loss: 89.3898 (96.5068)  lr: 0.0000 (0.0000)  time: 0.139428  data: 0.000230  max mem: 3153
I20241103 20:51:40 1321058 dinov2 helpers.py:102] Training  [  250/12500]  eta: 0:33:16  loss: 89.1309 (96.0534)  lr: 0.0000 (0.0000)  time: 0.143724  data: 0.005400  max mem: 3153
I20241103 20:51:42 1321058 dinov2 helpers.py:102] Training  [  260/12500]  eta: 0:33:03  loss: 87.9837 (95.6504)  lr: 0.0000 (0.0000)  time: 0.143640  data: 0.005404  max mem: 3153
I20241103 20:51:43 1321058 dinov2 helpers.py:102] Training  [  270/12500]  eta: 0:32:51  loss: 87.8089 (95.0890)  lr: 0.0000 (0.0000)  time: 0.139426  data: 0.000248  max mem: 3153
I20241103 20:51:45 1321058 dinov2 helpers.py:102] Training  [  280/12500]  eta: 0:32:40  loss: 87.2484 (94.6601)  lr: 0.0000 (0.0000)  time: 0.139445  data: 0.000222  max mem: 3153
I20241103 20:51:46 1321058 dinov2 helpers.py:102] Training  [  290/12500]  eta: 0:32:30  loss: 87.1656 (94.1623)  lr: 0.0000 (0.0000)  time: 0.139627  data: 0.000258  max mem: 3153
I20241103 20:51:47 1321058 dinov2 helpers.py:102] Training  [  300/12500]  eta: 0:32:20  loss: 86.3670 (93.8676)  lr: 0.0000 (0.0000)  time: 0.139677  data: 0.000318  max mem: 3153
I20241103 20:51:49 1321058 dinov2 helpers.py:102] Training  [  310/12500]  eta: 0:32:11  loss: 86.0073 (93.5166)  lr: 0.0000 (0.0000)  time: 0.139668  data: 0.000215  max mem: 3153
I20241103 20:51:50 1321058 dinov2 helpers.py:102] Training  [  320/12500]  eta: 0:32:02  loss: 85.1722 (93.0402)  lr: 0.0000 (0.0000)  time: 0.139638  data: 0.000155  max mem: 3153
I20241103 20:51:52 1321058 dinov2 helpers.py:102] Training  [  330/12500]  eta: 0:31:54  loss: 85.0257 (92.6432)  lr: 0.0000 (0.0000)  time: 0.139680  data: 0.000202  max mem: 3153
I20241103 20:51:53 1321058 dinov2 helpers.py:102] Training  [  340/12500]  eta: 0:31:46  loss: 84.7208 (92.3269)  lr: 0.0000 (0.0000)  time: 0.139863  data: 0.000231  max mem: 3153
I20241103 20:51:54 1321058 dinov2 helpers.py:102] Training  [  350/12500]  eta: 0:31:39  loss: 82.6490 (91.7976)  lr: 0.0000 (0.0000)  time: 0.139776  data: 0.000201  max mem: 3153
I20241103 20:51:56 1321058 dinov2 helpers.py:102] Training  [  360/12500]  eta: 0:31:32  loss: 82.6490 (91.5530)  lr: 0.0000 (0.0000)  time: 0.139787  data: 0.000300  max mem: 3153
I20241103 20:51:57 1321058 dinov2 helpers.py:102] Training  [  370/12500]  eta: 0:31:25  loss: 82.6343 (91.1868)  lr: 0.0000 (0.0000)  time: 0.139967  data: 0.000315  max mem: 3153
I20241103 20:51:59 1321058 dinov2 helpers.py:102] Training  [  380/12500]  eta: 0:31:19  loss: 81.5736 (90.7897)  lr: 0.0000 (0.0000)  time: 0.140050  data: 0.000280  max mem: 3153
I20241103 20:52:00 1321058 dinov2 helpers.py:102] Training  [  390/12500]  eta: 0:31:15  loss: 79.9315 (90.4734)  lr: 0.0000 (0.0000)  time: 0.144386  data: 0.005541  max mem: 3153
I20241103 20:52:01 1321058 dinov2 helpers.py:102] Training  [  400/12500]  eta: 0:31:09  loss: 79.7278 (90.0318)  lr: 0.0000 (0.0000)  time: 0.144304  data: 0.005527  max mem: 3153
I20241103 20:52:03 1321058 dinov2 helpers.py:102] Training  [  410/12500]  eta: 0:31:03  loss: 79.5396 (89.7181)  lr: 0.0000 (0.0000)  time: 0.140030  data: 0.000286  max mem: 3153
I20241103 20:52:04 1321058 dinov2 helpers.py:102] Training  [  420/12500]  eta: 0:30:58  loss: 79.2098 (89.4554)  lr: 0.0000 (0.0000)  time: 0.140126  data: 0.000286  max mem: 3153
I20241103 20:52:06 1321058 dinov2 helpers.py:102] Training  [  430/12500]  eta: 0:30:52  loss: 79.2098 (89.2156)  lr: 0.0000 (0.0000)  time: 0.140074  data: 0.000380  max mem: 3153
I20241103 20:52:07 1321058 dinov2 helpers.py:102] Training  [  440/12500]  eta: 0:30:47  loss: 78.9020 (88.9806)  lr: 0.0000 (0.0000)  time: 0.140337  data: 0.000384  max mem: 3153
I20241103 20:52:09 1321058 dinov2 helpers.py:102] Training  [  450/12500]  eta: 0:30:42  loss: 78.6430 (88.7335)  lr: 0.0000 (0.0000)  time: 0.140411  data: 0.000353  max mem: 3153
I20241103 20:52:10 1321058 dinov2 helpers.py:102] Training  [  460/12500]  eta: 0:30:37  loss: 78.4240 (88.3998)  lr: 0.0000 (0.0000)  time: 0.140040  data: 0.000303  max mem: 3153
I20241103 20:52:11 1321058 dinov2 helpers.py:102] Training  [  470/12500]  eta: 0:30:33  loss: 78.1358 (88.1607)  lr: 0.0000 (0.0000)  time: 0.140069  data: 0.000289  max mem: 3153
I20241103 20:52:13 1321058 dinov2 helpers.py:102] Training  [  480/12500]  eta: 0:30:28  loss: 77.7975 (87.8607)  lr: 0.0000 (0.0000)  time: 0.140411  data: 0.000327  max mem: 3153
I20241103 20:52:14 1321058 dinov2 helpers.py:102] Training  [  490/12500]  eta: 0:30:24  loss: 77.6365 (87.6024)  lr: 0.0000 (0.0000)  time: 0.140438  data: 0.000347  max mem: 3153
I20241103 20:52:16 1321058 dinov2 helpers.py:102] Training  [  500/12500]  eta: 0:30:19  loss: 77.6145 (87.3825)  lr: 0.0000 (0.0000)  time: 0.140398  data: 0.000348  max mem: 3153
I20241103 20:52:17 1321058 dinov2 helpers.py:102] Training  [  510/12500]  eta: 0:30:15  loss: 76.9211 (87.0898)  lr: 0.0000 (0.0000)  time: 0.140680  data: 0.000305  max mem: 3153
I20241103 20:52:18 1321058 dinov2 helpers.py:102] Training  [  520/12500]  eta: 0:30:13  loss: 76.8562 (86.8141)  lr: 0.0000 (0.0000)  time: 0.145156  data: 0.005458  max mem: 3153
I20241103 20:52:20 1321058 dinov2 helpers.py:102] Training  [  530/12500]  eta: 0:30:10  loss: 76.3879 (86.6152)  lr: 0.0000 (0.0000)  time: 0.145285  data: 0.005421  max mem: 3153
I20241103 20:52:21 1321058 dinov2 helpers.py:102] Training  [  540/12500]  eta: 0:30:06  loss: 76.0714 (86.4111)  lr: 0.0000 (0.0000)  time: 0.140706  data: 0.000274  max mem: 3153
I20241103 20:52:23 1321058 dinov2 helpers.py:102] Training  [  550/12500]  eta: 0:30:02  loss: 76.0714 (86.1679)  lr: 0.0000 (0.0000)  time: 0.140677  data: 0.000291  max mem: 3153
I20241103 20:52:24 1321058 dinov2 helpers.py:102] Training  [  560/12500]  eta: 0:29:58  loss: 75.7022 (85.9640)  lr: 0.0000 (0.0000)  time: 0.140945  data: 0.000289  max mem: 3153
I20241103 20:52:25 1321058 dinov2 helpers.py:102] Training  [  570/12500]  eta: 0:29:55  loss: 75.3943 (85.7459)  lr: 0.0000 (0.0000)  time: 0.140581  data: 0.000305  max mem: 3153
I20241103 20:52:27 1321058 dinov2 helpers.py:102] Training  [  580/12500]  eta: 0:29:51  loss: 74.9454 (85.5028)  lr: 0.0000 (0.0000)  time: 0.140733  data: 0.000273  max mem: 3153
I20241103 20:52:28 1321058 dinov2 helpers.py:102] Training  [  590/12500]  eta: 0:29:48  loss: 74.5450 (85.2385)  lr: 0.0000 (0.0000)  time: 0.140802  data: 0.000256  max mem: 3153
I20241103 20:52:30 1321058 dinov2 helpers.py:102] Training  [  600/12500]  eta: 0:29:44  loss: 74.5450 (85.0369)  lr: 0.0000 (0.0000)  time: 0.140786  data: 0.000316  max mem: 3153
I20241103 20:52:31 1321058 dinov2 helpers.py:102] Training  [  610/12500]  eta: 0:29:41  loss: 74.5450 (84.8740)  lr: 0.0000 (0.0000)  time: 0.141059  data: 0.000296  max mem: 3153
I20241103 20:52:33 1321058 dinov2 helpers.py:102] Training  [  620/12500]  eta: 0:29:38  loss: 73.8832 (84.6995)  lr: 0.0000 (0.0000)  time: 0.140692  data: 0.000198  max mem: 3153
I20241103 20:52:34 1321058 dinov2 helpers.py:102] Training  [  630/12500]  eta: 0:29:35  loss: 73.4640 (84.5100)  lr: 0.0000 (0.0000)  time: 0.140624  data: 0.000211  max mem: 3153
I20241103 20:52:35 1321058 dinov2 helpers.py:102] Training  [  640/12500]  eta: 0:29:32  loss: 73.3140 (84.2733)  lr: 0.0000 (0.0000)  time: 0.140896  data: 0.000242  max mem: 3153
I20241103 20:52:37 1321058 dinov2 helpers.py:102] Training  [  650/12500]  eta: 0:29:30  loss: 73.0479 (84.0692)  lr: 0.0000 (0.0000)  time: 0.145152  data: 0.005472  max mem: 3153
I20241103 20:52:38 1321058 dinov2 helpers.py:102] Training  [  660/12500]  eta: 0:29:27  loss: 72.9411 (83.8174)  lr: 0.0000 (0.0000)  time: 0.145177  data: 0.005491  max mem: 3153
I20241103 20:52:40 1321058 dinov2 helpers.py:102] Training  [  670/12500]  eta: 0:29:24  loss: 72.9411 (83.6765)  lr: 0.0000 (0.0000)  time: 0.140923  data: 0.000244  max mem: 3153
I20241103 20:52:41 1321058 dinov2 helpers.py:102] Training  [  680/12500]  eta: 0:29:21  loss: 72.7904 (83.4571)  lr: 0.0000 (0.0000)  time: 0.140704  data: 0.000248  max mem: 3153
I20241103 20:52:42 1321058 dinov2 helpers.py:102] Training  [  690/12500]  eta: 0:29:18  loss: 72.5737 (83.2841)  lr: 0.0000 (0.0000)  time: 0.140750  data: 0.000237  max mem: 3153
I20241103 20:52:44 1321058 dinov2 helpers.py:102] Training  [  700/12500]  eta: 0:29:16  loss: 72.5737 (83.1496)  lr: 0.0000 (0.0000)  time: 0.141226  data: 0.000217  max mem: 3153
I20241103 20:52:45 1321058 dinov2 helpers.py:102] Training  [  710/12500]  eta: 0:29:13  loss: 72.5737 (82.9567)  lr: 0.0000 (0.0000)  time: 0.141404  data: 0.000215  max mem: 3153
I20241103 20:52:47 1321058 dinov2 helpers.py:102] Training  [  720/12500]  eta: 0:29:10  loss: 72.5737 (82.7923)  lr: 0.0000 (0.0000)  time: 0.141416  data: 0.000204  max mem: 3153
I20241103 20:52:48 1321058 dinov2 helpers.py:102] Training  [  730/12500]  eta: 0:29:08  loss: 72.5737 (82.6569)  lr: 0.0000 (0.0000)  time: 0.141425  data: 0.000221  max mem: 3153
I20241103 20:52:50 1321058 dinov2 helpers.py:102] Training  [  740/12500]  eta: 0:29:05  loss: 71.4060 (82.5023)  lr: 0.0000 (0.0000)  time: 0.141433  data: 0.000279  max mem: 3153
I20241103 20:52:51 1321058 dinov2 helpers.py:102] Training  [  750/12500]  eta: 0:29:02  loss: 71.3447 (82.3418)  lr: 0.0000 (0.0000)  time: 0.141410  data: 0.000321  max mem: 3153
I20241103 20:52:52 1321058 dinov2 helpers.py:102] Training  [  760/12500]  eta: 0:29:00  loss: 71.0630 (82.1468)  lr: 0.0000 (0.0000)  time: 0.141494  data: 0.000277  max mem: 3153
I20241103 20:52:54 1321058 dinov2 helpers.py:102] Training  [  770/12500]  eta: 0:28:57  loss: 70.9549 (81.9612)  lr: 0.0000 (0.0000)  time: 0.141519  data: 0.000278  max mem: 3153
I20241103 20:52:55 1321058 dinov2 helpers.py:102] Training  [  780/12500]  eta: 0:28:55  loss: 70.8066 (81.8044)  lr: 0.0000 (0.0000)  time: 0.141469  data: 0.000252  max mem: 3153
I20241103 20:52:57 1321058 dinov2 helpers.py:102] Training  [  790/12500]  eta: 0:28:54  loss: 70.9549 (81.6721)  lr: 0.0000 (0.0000)  time: 0.145590  data: 0.005412  max mem: 3153
I20241103 20:52:58 1321058 dinov2 helpers.py:102] Training  [  800/12500]  eta: 0:28:51  loss: 70.8066 (81.5200)  lr: 0.0000 (0.0000)  time: 0.145346  data: 0.005419  max mem: 3153
I20241103 20:53:00 1321058 dinov2 helpers.py:102] Training  [  810/12500]  eta: 0:28:49  loss: 70.8066 (81.4069)  lr: 0.0000 (0.0000)  time: 0.141197  data: 0.000215  max mem: 3153
I20241103 20:53:01 1321058 dinov2 helpers.py:102] Training  [  820/12500]  eta: 0:28:46  loss: 70.3051 (81.1737)  lr: 0.0000 (0.0000)  time: 0.141317  data: 0.000271  max mem: 3153
I20241103 20:53:02 1321058 dinov2 helpers.py:102] Training  [  830/12500]  eta: 0:28:44  loss: 69.5762 (80.9922)  lr: 0.0000 (0.0000)  time: 0.141289  data: 0.000274  max mem: 3153
I20241103 20:53:04 1321058 dinov2 helpers.py:102] Training  [  840/12500]  eta: 0:28:41  loss: 69.5762 (80.8092)  lr: 0.0000 (0.0000)  time: 0.141344  data: 0.000200  max mem: 3153
I20241103 20:53:05 1321058 dinov2 helpers.py:102] Training  [  850/12500]  eta: 0:28:39  loss: 69.5762 (80.6818)  lr: 0.0000 (0.0000)  time: 0.141370  data: 0.000181  max mem: 3153
I20241103 20:53:07 1321058 dinov2 helpers.py:102] Training  [  860/12500]  eta: 0:28:37  loss: 69.5762 (80.5063)  lr: 0.0000 (0.0000)  time: 0.141385  data: 0.000243  max mem: 3153
I20241103 20:53:08 1321058 dinov2 helpers.py:102] Training  [  870/12500]  eta: 0:28:35  loss: 69.3521 (80.3034)  lr: 0.0000 (0.0000)  time: 0.141417  data: 0.000317  max mem: 3153
I20241103 20:53:09 1321058 dinov2 helpers.py:102] Training  [  880/12500]  eta: 0:28:32  loss: 69.5762 (80.2120)  lr: 0.0000 (0.0000)  time: 0.141448  data: 0.000283  max mem: 3153
I20241103 20:53:11 1321058 dinov2 helpers.py:102] Training  [  890/12500]  eta: 0:28:30  loss: 69.3521 (80.0752)  lr: 0.0000 (0.0000)  time: 0.141479  data: 0.000227  max mem: 3153
I20241103 20:53:12 1321058 dinov2 helpers.py:102] Training  [  900/12500]  eta: 0:28:28  loss: 69.2624 (79.9397)  lr: 0.0000 (0.0000)  time: 0.141496  data: 0.000213  max mem: 3153
I20241103 20:53:14 1321058 dinov2 helpers.py:102] Training  [  910/12500]  eta: 0:28:26  loss: 67.9044 (79.7489)  lr: 0.0000 (0.0000)  time: 0.141599  data: 0.000197  max mem: 3153
I20241103 20:53:15 1321058 dinov2 helpers.py:102] Training  [  920/12500]  eta: 0:28:24  loss: 67.7439 (79.6037)  lr: 0.0000 (0.0000)  time: 0.145748  data: 0.005431  max mem: 3153
I20241103 20:53:17 1321058 dinov2 helpers.py:102] Training  [  930/12500]  eta: 0:28:22  loss: 67.6713 (79.4499)  lr: 0.0000 (0.0000)  time: 0.145622  data: 0.005483  max mem: 3153
I20241103 20:53:18 1321058 dinov2 helpers.py:102] Training  [  940/12500]  eta: 0:28:20  loss: 67.6713 (79.3334)  lr: 0.0000 (0.0000)  time: 0.141438  data: 0.000318  max mem: 3153
I20241103 20:53:19 1321058 dinov2 helpers.py:102] Training  [  950/12500]  eta: 0:28:18  loss: 67.6713 (79.2343)  lr: 0.0000 (0.0000)  time: 0.141512  data: 0.000285  max mem: 3153
I20241103 20:53:21 1321058 dinov2 helpers.py:102] Training  [  960/12500]  eta: 0:28:16  loss: 67.6713 (79.0928)  lr: 0.0000 (0.0000)  time: 0.141454  data: 0.000228  max mem: 3153
I20241103 20:53:22 1321058 dinov2 helpers.py:102] Training  [  970/12500]  eta: 0:28:14  loss: 66.2495 (78.9473)  lr: 0.0000 (0.0000)  time: 0.141403  data: 0.000236  max mem: 3153
I20241103 20:53:24 1321058 dinov2 helpers.py:102] Training  [  980/12500]  eta: 0:28:12  loss: 66.2495 (78.8581)  lr: 0.0000 (0.0000)  time: 0.141467  data: 0.000238  max mem: 3153
I20241103 20:53:25 1321058 dinov2 helpers.py:102] Training  [  990/12500]  eta: 0:28:09  loss: 65.9265 (78.7098)  lr: 0.0000 (0.0000)  time: 0.141546  data: 0.000277  max mem: 3153
I20241103 20:53:26 1321058 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 0:28:07  loss: 65.5957 (78.5799)  lr: 0.0000 (0.0000)  time: 0.141548  data: 0.000292  max mem: 3153
I20241103 20:53:28 1321058 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 0:28:05  loss: 65.5054 (78.4148)  lr: 0.0000 (0.0000)  time: 0.141545  data: 0.000265  max mem: 3153
I20241103 20:53:29 1321058 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 0:28:03  loss: 65.5957 (78.3357)  lr: 0.0000 (0.0000)  time: 0.141490  data: 0.000272  max mem: 3153
I20241103 20:53:31 1321058 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 0:28:01  loss: 65.5054 (78.1925)  lr: 0.0000 (0.0000)  time: 0.141552  data: 0.000315  max mem: 3153
I20241103 20:53:32 1321058 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 0:27:59  loss: 65.5957 (78.0854)  lr: 0.0000 (0.0000)  time: 0.141475  data: 0.000312  max mem: 3153
I20241103 20:53:34 1321058 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 0:27:58  loss: 65.5054 (77.9647)  lr: 0.0000 (0.0000)  time: 0.145601  data: 0.005456  max mem: 3153
I20241103 20:53:35 1321058 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 0:27:56  loss: 65.5054 (77.8253)  lr: 0.0000 (0.0000)  time: 0.146048  data: 0.005504  max mem: 3153
I20241103 20:53:36 1321058 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 0:27:54  loss: 65.5957 (77.7650)  lr: 0.0000 (0.0000)  time: 0.141508  data: 0.000306  max mem: 3153
I20241103 20:53:38 1321058 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 0:27:52  loss: 65.5054 (77.6455)  lr: 0.0000 (0.0000)  time: 0.141072  data: 0.000258  max mem: 3153
I20241103 20:53:39 1321058 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 0:27:50  loss: 65.2878 (77.5102)  lr: 0.0000 (0.0000)  time: 0.141510  data: 0.000269  max mem: 3153
I20241103 20:53:41 1321058 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 0:27:48  loss: 65.1425 (77.3808)  lr: 0.0000 (0.0000)  time: 0.141592  data: 0.000294  max mem: 3153
I20241103 20:53:42 1321058 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 0:27:46  loss: 65.1425 (77.2435)  lr: 0.0000 (0.0000)  time: 0.141443  data: 0.000307  max mem: 3153
I20241103 20:53:44 1321058 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 0:27:44  loss: 65.1425 (77.1414)  lr: 0.0000 (0.0000)  time: 0.141330  data: 0.000280  max mem: 3153
I20241103 20:53:45 1321058 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 0:27:42  loss: 64.8347 (77.0193)  lr: 0.0000 (0.0000)  time: 0.141322  data: 0.000245  max mem: 3153
I20241103 20:53:46 1321058 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 0:27:40  loss: 64.8347 (76.9144)  lr: 0.0000 (0.0000)  time: 0.141661  data: 0.000211  max mem: 3153
I20241103 20:53:48 1321058 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 0:27:38  loss: 64.8347 (76.8200)  lr: 0.0000 (0.0000)  time: 0.141456  data: 0.000237  max mem: 3153
I20241103 20:53:49 1321058 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 0:27:36  loss: 64.7474 (76.6713)  lr: 0.0000 (0.0000)  time: 0.141340  data: 0.000283  max mem: 3153
I20241103 20:53:51 1321058 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 0:27:35  loss: 64.0246 (76.5576)  lr: 0.0000 (0.0000)  time: 0.141558  data: 0.000275  max mem: 3153
I20241103 20:53:52 1321058 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 0:27:33  loss: 64.0246 (76.4740)  lr: 0.0000 (0.0000)  time: 0.141501  data: 0.000282  max mem: 3153
I20241103 20:53:54 1321058 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 0:27:32  loss: 63.4445 (76.3608)  lr: 0.0000 (0.0000)  time: 0.146481  data: 0.005592  max mem: 3153
I20241103 20:53:55 1321058 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 0:27:30  loss: 63.2568 (76.2125)  lr: 0.0000 (0.0000)  time: 0.146373  data: 0.005594  max mem: 3153
I20241103 20:53:56 1321058 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 0:27:28  loss: 63.2568 (76.0943)  lr: 0.0000 (0.0000)  time: 0.141218  data: 0.000328  max mem: 3153
I20241103 20:53:58 1321058 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 0:27:26  loss: 63.2297 (75.9835)  lr: 0.0000 (0.0000)  time: 0.141462  data: 0.000367  max mem: 3153
I20241103 20:53:59 1321058 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 0:27:24  loss: 63.1457 (75.8630)  lr: 0.0000 (0.0000)  time: 0.141377  data: 0.000340  max mem: 3153
I20241103 20:54:01 1321058 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 0:27:22  loss: 63.1457 (75.7666)  lr: 0.0000 (0.0000)  time: 0.141229  data: 0.000304  max mem: 3153
I20241103 20:54:02 1321058 dinov2 linear.py:272] running validation !
E20241103 20:54:02 1321058 submitit submission.py:68] Submitted job triggered an exception
