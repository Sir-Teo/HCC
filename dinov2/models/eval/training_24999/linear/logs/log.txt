I20241103 16:19:02 2286569 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 16:19:02 2286569 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 16:19:02 2286569 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 16:19:02 2286569 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 16:19:02 2286569 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 16:19:07 2286569 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 16:19:07 2286569 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 16:19:08 2286569 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
E20241103 16:19:08 2286569 submitit submission.py:68] Submitted job triggered an exception
I20241103 16:30:34 2287726 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 16:30:34 2287726 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 16:30:34 2287726 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 16:30:34 2287726 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 16:30:34 2287726 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 16:30:41 2287726 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 16:30:42 2287726 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 16:30:42 2287726 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
I20241103 16:30:42 2287726 dinov2 loaders.py:91] # of dataset samples: 34,076
E20241103 16:30:42 2287726 submitit submission.py:68] Submitted job triggered an exception
I20241103 20:49:26 1320749 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 20:49:26 1320749 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 20:49:26 1320749 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 20:49:26 1320749 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 20:49:26 1320749 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 20:49:32 1320749 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 20:49:33 1320749 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 20:49:33 1320749 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
E20241103 20:49:33 1320749 submitit submission.py:68] Submitted job triggered an exception
I20241103 20:50:54 1321057 dinov2 config.py:59] git:
  sha: 3619aae120a42692da11bec13b7b1914b878038a, status: has uncommitted changes, branch: main

I20241103 20:50:54 1321057 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: /gpfs/data/mankowskilab/HCC/models/config.yaml
cpus_per_task: 16
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
mem: 100
ngpus: 2
no_resume: False
nodes: 1
num_workers: 8
opts: ['train.output_dir=/gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear']
output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
partition: a100_short
pretrained_weights: /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
timeout: 4319
train_dataset_str: ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification
val_metric_type: mean_accuracy
I20241103 20:50:54 1321057 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0014142135623730952
I20241103 20:50:54 1321057 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: UnlabeledMedicalImageDataset:root=/gpfs/data/mankowskilab/HCC/data/images
  output_dir: /gpfs/data/shenlab/wz1492/HCC/dinov2/models/eval/training_24999/linear
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0014142135623730952
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
output_dir: /gpfs/data/mankowskilab/HCC/models

I20241103 20:50:54 1321057 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20241103 20:50:57 1321057 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20241103 20:50:58 1321057 dinov2 utils.py:33] Pretrained weights found at /gpfs/data/mankowskilab/HCC/models/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20241103 20:50:58 1321057 dinov2 loaders.py:86] using dataset: "ImageNet:split=TRAIN:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
I20241103 20:50:58 1321057 dinov2 loaders.py:91] # of dataset samples: 34,076
I20241103 20:50:59 1321057 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241103 20:50:59 1321057 dinov2 loaders.py:124] sampler: sharded infinite
I20241103 20:50:59 1321057 dinov2 loaders.py:208] using PyTorch data loader
I20241103 20:50:59 1321057 dinov2 loaders.py:223] infinite data loader
I20241103 20:50:59 1321057 dinov2 loaders.py:86] using dataset: "ImageNet:split=VAL:root=/gpfs/data/mankowskilab/HCC/data/Series_Classification:extra=/gpfs/data/mankowskilab/HCC/data/Series_Classification"
I20241103 20:50:59 1321057 dinov2 loaders.py:91] # of dataset samples: 11,290
I20241103 20:50:59 1321057 dinov2 loaders.py:149] sampler: distributed
I20241103 20:50:59 1321057 dinov2 loaders.py:208] using PyTorch data loader
I20241103 20:50:59 1321057 dinov2 loaders.py:221] # of batches: 45
I20241103 20:50:59 1321057 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241103 20:50:59 1321057 dinov2 linear.py:338] Starting training from iteration 0
I20241103 20:51:03 1321057 dinov2 helpers.py:102] Training  [    0/12500]  eta: 11:37:07  loss: 147.7223 (147.7223)  lr: 0.0000 (0.0000)  time: 3.346178  data: 3.131710  max mem: 2723
I20241103 20:51:03 1321057 torch.nn.parallel.distributed distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I20241103 20:51:05 1321057 dinov2 helpers.py:102] Training  [   10/12500]  eta: 1:37:58  loss: 123.5782 (135.6503)  lr: 0.0000 (0.0000)  time: 0.470648  data: 0.327117  max mem: 3153
I20241103 20:51:07 1321057 dinov2 helpers.py:102] Training  [   20/12500]  eta: 1:12:16  loss: 123.5782 (129.4806)  lr: 0.0000 (0.0000)  time: 0.197516  data: 0.061282  max mem: 3153
I20241103 20:51:09 1321057 dinov2 helpers.py:102] Training  [   30/12500]  eta: 1:01:40  loss: 117.1413 (125.1028)  lr: 0.0000 (0.0000)  time: 0.201066  data: 0.063121  max mem: 3153
I20241103 20:51:11 1321057 dinov2 helpers.py:102] Training  [   40/12500]  eta: 0:56:44  loss: 117.1413 (121.2052)  lr: 0.0000 (0.0000)  time: 0.195285  data: 0.057255  max mem: 3153
I20241103 20:51:12 1321057 dinov2 helpers.py:102] Training  [   50/12500]  eta: 0:51:55  loss: 111.9692 (118.6069)  lr: 0.0000 (0.0000)  time: 0.178245  data: 0.040039  max mem: 3153
I20241103 20:51:14 1321057 dinov2 helpers.py:102] Training  [   60/12500]  eta: 0:48:05  loss: 111.9692 (116.2128)  lr: 0.0000 (0.0000)  time: 0.147257  data: 0.008252  max mem: 3153
I20241103 20:51:15 1321057 dinov2 helpers.py:102] Training  [   70/12500]  eta: 0:45:30  loss: 105.6156 (114.2264)  lr: 0.0000 (0.0000)  time: 0.141738  data: 0.004865  max mem: 3153
I20241103 20:51:16 1321057 dinov2 helpers.py:102] Training  [   80/12500]  eta: 0:43:24  loss: 105.6156 (112.6304)  lr: 0.0000 (0.0000)  time: 0.141822  data: 0.004832  max mem: 3153
I20241103 20:51:18 1321057 dinov2 helpers.py:102] Training  [   90/12500]  eta: 0:41:49  loss: 105.6149 (110.5611)  lr: 0.0000 (0.0000)  time: 0.140052  data: 0.000575  max mem: 3153
I20241103 20:51:19 1321057 dinov2 helpers.py:102] Training  [  100/12500]  eta: 0:40:29  loss: 105.6149 (109.0238)  lr: 0.0000 (0.0000)  time: 0.140179  data: 0.000545  max mem: 3153
I20241103 20:51:21 1321057 dinov2 helpers.py:102] Training  [  110/12500]  eta: 0:39:23  loss: 101.8478 (108.1414)  lr: 0.0000 (0.0000)  time: 0.138919  data: 0.000480  max mem: 3153
I20241103 20:51:22 1321057 dinov2 helpers.py:102] Training  [  120/12500]  eta: 0:38:39  loss: 101.8478 (106.9654)  lr: 0.0000 (0.0000)  time: 0.144094  data: 0.005714  max mem: 3153
I20241103 20:51:24 1321057 dinov2 helpers.py:102] Training  [  130/12500]  eta: 0:37:56  loss: 100.3215 (105.9735)  lr: 0.0000 (0.0000)  time: 0.146729  data: 0.007751  max mem: 3153
I20241103 20:51:25 1321057 dinov2 helpers.py:102] Training  [  140/12500]  eta: 0:37:15  loss: 100.3215 (104.9584)  lr: 0.0000 (0.0000)  time: 0.141685  data: 0.002512  max mem: 3153
I20241103 20:51:26 1321057 dinov2 helpers.py:102] Training  [  150/12500]  eta: 0:36:39  loss: 99.8627 (104.3480)  lr: 0.0000 (0.0000)  time: 0.138840  data: 0.000478  max mem: 3153
I20241103 20:51:28 1321057 dinov2 helpers.py:102] Training  [  160/12500]  eta: 0:36:07  loss: 99.8627 (103.4617)  lr: 0.0000 (0.0000)  time: 0.138862  data: 0.000498  max mem: 3153
I20241103 20:51:29 1321057 dinov2 helpers.py:102] Training  [  170/12500]  eta: 0:35:39  loss: 98.4349 (102.5872)  lr: 0.0000 (0.0000)  time: 0.139025  data: 0.000498  max mem: 3153
I20241103 20:51:31 1321057 dinov2 helpers.py:102] Training  [  180/12500]  eta: 0:35:14  loss: 98.4349 (101.7812)  lr: 0.0000 (0.0000)  time: 0.138970  data: 0.000484  max mem: 3153
I20241103 20:51:32 1321057 dinov2 helpers.py:102] Training  [  190/12500]  eta: 0:34:51  loss: 95.1926 (101.1495)  lr: 0.0000 (0.0000)  time: 0.139096  data: 0.000510  max mem: 3153
I20241103 20:51:33 1321057 dinov2 helpers.py:102] Training  [  200/12500]  eta: 0:34:31  loss: 93.6499 (100.5234)  lr: 0.0000 (0.0000)  time: 0.139287  data: 0.000538  max mem: 3153
I20241103 20:51:35 1321057 dinov2 helpers.py:102] Training  [  210/12500]  eta: 0:34:12  loss: 93.0797 (99.8756)  lr: 0.0000 (0.0000)  time: 0.139302  data: 0.000506  max mem: 3153
I20241103 20:51:36 1321057 dinov2 helpers.py:102] Training  [  220/12500]  eta: 0:33:55  loss: 92.8535 (99.1463)  lr: 0.0000 (0.0000)  time: 0.139229  data: 0.000471  max mem: 3153
I20241103 20:51:38 1321057 dinov2 helpers.py:102] Training  [  230/12500]  eta: 0:33:39  loss: 91.9378 (98.5964)  lr: 0.0000 (0.0000)  time: 0.139248  data: 0.000438  max mem: 3153
I20241103 20:51:39 1321057 dinov2 helpers.py:102] Training  [  240/12500]  eta: 0:33:25  loss: 90.7465 (98.0476)  lr: 0.0000 (0.0000)  time: 0.139406  data: 0.000412  max mem: 3153
I20241103 20:51:40 1321057 dinov2 helpers.py:102] Training  [  250/12500]  eta: 0:33:15  loss: 89.2808 (97.4688)  lr: 0.0000 (0.0000)  time: 0.143720  data: 0.005776  max mem: 3153
I20241103 20:51:42 1321057 dinov2 helpers.py:102] Training  [  260/12500]  eta: 0:33:03  loss: 89.1470 (96.8458)  lr: 0.0000 (0.0000)  time: 0.143643  data: 0.005803  max mem: 3153
I20241103 20:51:43 1321057 dinov2 helpers.py:102] Training  [  270/12500]  eta: 0:32:51  loss: 88.3735 (96.5432)  lr: 0.0000 (0.0000)  time: 0.139422  data: 0.000430  max mem: 3153
I20241103 20:51:45 1321057 dinov2 helpers.py:102] Training  [  280/12500]  eta: 0:32:40  loss: 88.0025 (95.9571)  lr: 0.0000 (0.0000)  time: 0.139442  data: 0.000407  max mem: 3153
I20241103 20:51:46 1321057 dinov2 helpers.py:102] Training  [  290/12500]  eta: 0:32:30  loss: 87.7197 (95.4931)  lr: 0.0000 (0.0000)  time: 0.139615  data: 0.000445  max mem: 3153
I20241103 20:51:47 1321057 dinov2 helpers.py:102] Training  [  300/12500]  eta: 0:32:20  loss: 87.2737 (95.0429)  lr: 0.0000 (0.0000)  time: 0.139658  data: 0.000529  max mem: 3153
I20241103 20:51:49 1321057 dinov2 helpers.py:102] Training  [  310/12500]  eta: 0:32:11  loss: 86.2718 (94.4709)  lr: 0.0000 (0.0000)  time: 0.139665  data: 0.000554  max mem: 3153
I20241103 20:51:50 1321057 dinov2 helpers.py:102] Training  [  320/12500]  eta: 0:32:02  loss: 85.9490 (93.9811)  lr: 0.0000 (0.0000)  time: 0.139656  data: 0.000543  max mem: 3153
I20241103 20:51:52 1321057 dinov2 helpers.py:102] Training  [  330/12500]  eta: 0:31:54  loss: 84.8772 (93.5673)  lr: 0.0000 (0.0000)  time: 0.139681  data: 0.000554  max mem: 3153
I20241103 20:51:53 1321057 dinov2 helpers.py:102] Training  [  340/12500]  eta: 0:31:46  loss: 83.0999 (93.1351)  lr: 0.0000 (0.0000)  time: 0.139817  data: 0.000518  max mem: 3153
I20241103 20:51:54 1321057 dinov2 helpers.py:102] Training  [  350/12500]  eta: 0:31:38  loss: 82.9993 (92.7268)  lr: 0.0000 (0.0000)  time: 0.139751  data: 0.000494  max mem: 3153
I20241103 20:51:56 1321057 dinov2 helpers.py:102] Training  [  360/12500]  eta: 0:31:31  loss: 82.0360 (92.4057)  lr: 0.0000 (0.0000)  time: 0.139794  data: 0.000508  max mem: 3153
I20241103 20:51:57 1321057 dinov2 helpers.py:102] Training  [  370/12500]  eta: 0:31:25  loss: 81.5374 (91.9448)  lr: 0.0000 (0.0000)  time: 0.139950  data: 0.000520  max mem: 3153
I20241103 20:51:59 1321057 dinov2 helpers.py:102] Training  [  380/12500]  eta: 0:31:18  loss: 80.8464 (91.4764)  lr: 0.0000 (0.0000)  time: 0.139832  data: 0.000498  max mem: 3153
I20241103 20:52:00 1321057 dinov2 helpers.py:102] Training  [  390/12500]  eta: 0:31:15  loss: 80.6474 (91.1089)  lr: 0.0000 (0.0000)  time: 0.144194  data: 0.005832  max mem: 3153
I20241103 20:52:01 1321057 dinov2 helpers.py:102] Training  [  400/12500]  eta: 0:31:09  loss: 79.9107 (90.7162)  lr: 0.0000 (0.0000)  time: 0.144310  data: 0.005854  max mem: 3153
I20241103 20:52:03 1321057 dinov2 helpers.py:102] Training  [  410/12500]  eta: 0:31:03  loss: 79.5457 (90.2855)  lr: 0.0000 (0.0000)  time: 0.140012  data: 0.000536  max mem: 3153
I20241103 20:52:04 1321057 dinov2 helpers.py:102] Training  [  420/12500]  eta: 0:30:57  loss: 79.2618 (90.0292)  lr: 0.0000 (0.0000)  time: 0.140106  data: 0.000502  max mem: 3153
I20241103 20:52:06 1321057 dinov2 helpers.py:102] Training  [  430/12500]  eta: 0:30:52  loss: 78.4414 (89.6627)  lr: 0.0000 (0.0000)  time: 0.140078  data: 0.000484  max mem: 3153
I20241103 20:52:07 1321057 dinov2 helpers.py:102] Training  [  440/12500]  eta: 0:30:47  loss: 78.4345 (89.3027)  lr: 0.0000 (0.0000)  time: 0.140350  data: 0.000516  max mem: 3153
I20241103 20:52:09 1321057 dinov2 helpers.py:102] Training  [  450/12500]  eta: 0:30:42  loss: 78.3077 (89.0416)  lr: 0.0000 (0.0000)  time: 0.140394  data: 0.000515  max mem: 3153
I20241103 20:52:10 1321057 dinov2 helpers.py:102] Training  [  460/12500]  eta: 0:30:37  loss: 77.2961 (88.7881)  lr: 0.0000 (0.0000)  time: 0.140018  data: 0.000503  max mem: 3153
I20241103 20:52:11 1321057 dinov2 helpers.py:102] Training  [  470/12500]  eta: 0:30:32  loss: 77.1248 (88.5412)  lr: 0.0000 (0.0000)  time: 0.140073  data: 0.000468  max mem: 3153
I20241103 20:52:13 1321057 dinov2 helpers.py:102] Training  [  480/12500]  eta: 0:30:28  loss: 76.9387 (88.2408)  lr: 0.0000 (0.0000)  time: 0.140413  data: 0.000492  max mem: 3153
I20241103 20:52:14 1321057 dinov2 helpers.py:102] Training  [  490/12500]  eta: 0:30:23  loss: 76.7767 (87.9397)  lr: 0.0000 (0.0000)  time: 0.140440  data: 0.000524  max mem: 3153
I20241103 20:52:16 1321057 dinov2 helpers.py:102] Training  [  500/12500]  eta: 0:30:19  loss: 76.7402 (87.6501)  lr: 0.0000 (0.0000)  time: 0.140388  data: 0.000509  max mem: 3153
I20241103 20:52:17 1321057 dinov2 helpers.py:102] Training  [  510/12500]  eta: 0:30:15  loss: 75.9519 (87.4251)  lr: 0.0000 (0.0000)  time: 0.140685  data: 0.000503  max mem: 3153
I20241103 20:52:18 1321057 dinov2 helpers.py:102] Training  [  520/12500]  eta: 0:30:13  loss: 75.5120 (87.2003)  lr: 0.0000 (0.0000)  time: 0.145180  data: 0.005868  max mem: 3153
I20241103 20:52:20 1321057 dinov2 helpers.py:102] Training  [  530/12500]  eta: 0:30:09  loss: 75.5120 (87.0447)  lr: 0.0000 (0.0000)  time: 0.144886  data: 0.005853  max mem: 3153
I20241103 20:52:21 1321057 dinov2 helpers.py:102] Training  [  540/12500]  eta: 0:30:05  loss: 75.5120 (86.8350)  lr: 0.0000 (0.0000)  time: 0.140304  data: 0.000502  max mem: 3153
I20241103 20:52:23 1321057 dinov2 helpers.py:102] Training  [  550/12500]  eta: 0:30:01  loss: 75.0102 (86.5771)  lr: 0.0000 (0.0000)  time: 0.140672  data: 0.000526  max mem: 3153
I20241103 20:52:24 1321057 dinov2 helpers.py:102] Training  [  560/12500]  eta: 0:29:58  loss: 75.0102 (86.4146)  lr: 0.0000 (0.0000)  time: 0.140925  data: 0.000509  max mem: 3153
I20241103 20:52:25 1321057 dinov2 helpers.py:102] Training  [  570/12500]  eta: 0:29:54  loss: 75.0102 (86.2046)  lr: 0.0000 (0.0000)  time: 0.140580  data: 0.000503  max mem: 3153
I20241103 20:52:27 1321057 dinov2 helpers.py:102] Training  [  580/12500]  eta: 0:29:51  loss: 75.5120 (86.0344)  lr: 0.0000 (0.0000)  time: 0.140751  data: 0.000516  max mem: 3153
I20241103 20:52:28 1321057 dinov2 helpers.py:102] Training  [  590/12500]  eta: 0:29:47  loss: 75.0102 (85.8426)  lr: 0.0000 (0.0000)  time: 0.140795  data: 0.000520  max mem: 3153
I20241103 20:52:30 1321057 dinov2 helpers.py:102] Training  [  600/12500]  eta: 0:29:44  loss: 74.5270 (85.6025)  lr: 0.0000 (0.0000)  time: 0.140755  data: 0.000509  max mem: 3153
I20241103 20:52:31 1321057 dinov2 helpers.py:102] Training  [  610/12500]  eta: 0:29:41  loss: 74.5270 (85.3761)  lr: 0.0000 (0.0000)  time: 0.141043  data: 0.000526  max mem: 3153
I20241103 20:52:33 1321057 dinov2 helpers.py:102] Training  [  620/12500]  eta: 0:29:37  loss: 74.2345 (85.1492)  lr: 0.0000 (0.0000)  time: 0.140700  data: 0.000539  max mem: 3153
I20241103 20:52:34 1321057 dinov2 helpers.py:102] Training  [  630/12500]  eta: 0:29:34  loss: 74.2528 (84.9790)  lr: 0.0000 (0.0000)  time: 0.140636  data: 0.000520  max mem: 3153
I20241103 20:52:35 1321057 dinov2 helpers.py:102] Training  [  640/12500]  eta: 0:29:31  loss: 74.2528 (84.7871)  lr: 0.0000 (0.0000)  time: 0.140901  data: 0.000534  max mem: 3153
I20241103 20:52:37 1321057 dinov2 helpers.py:102] Training  [  650/12500]  eta: 0:29:30  loss: 74.2345 (84.6114)  lr: 0.0000 (0.0000)  time: 0.145148  data: 0.005857  max mem: 3153
I20241103 20:52:38 1321057 dinov2 helpers.py:102] Training  [  660/12500]  eta: 0:29:27  loss: 73.8199 (84.4228)  lr: 0.0000 (0.0000)  time: 0.145174  data: 0.005820  max mem: 3153
I20241103 20:52:40 1321057 dinov2 helpers.py:102] Training  [  670/12500]  eta: 0:29:24  loss: 73.1909 (84.1773)  lr: 0.0000 (0.0000)  time: 0.140925  data: 0.000493  max mem: 3153
I20241103 20:52:41 1321057 dinov2 helpers.py:102] Training  [  680/12500]  eta: 0:29:21  loss: 73.1909 (84.0273)  lr: 0.0000 (0.0000)  time: 0.140701  data: 0.000487  max mem: 3153
I20241103 20:52:42 1321057 dinov2 helpers.py:102] Training  [  690/12500]  eta: 0:29:18  loss: 73.8225 (83.9305)  lr: 0.0000 (0.0000)  time: 0.140745  data: 0.000487  max mem: 3153
I20241103 20:52:44 1321057 dinov2 helpers.py:102] Training  [  700/12500]  eta: 0:29:15  loss: 73.8225 (83.7610)  lr: 0.0000 (0.0000)  time: 0.141216  data: 0.000511  max mem: 3153
I20241103 20:52:45 1321057 dinov2 helpers.py:102] Training  [  710/12500]  eta: 0:29:12  loss: 73.1909 (83.5912)  lr: 0.0000 (0.0000)  time: 0.141416  data: 0.000508  max mem: 3153
I20241103 20:52:47 1321057 dinov2 helpers.py:102] Training  [  720/12500]  eta: 0:29:10  loss: 72.5083 (83.3872)  lr: 0.0000 (0.0000)  time: 0.141415  data: 0.000463  max mem: 3153
I20241103 20:52:48 1321057 dinov2 helpers.py:102] Training  [  730/12500]  eta: 0:29:07  loss: 72.4651 (83.2396)  lr: 0.0000 (0.0000)  time: 0.141425  data: 0.000455  max mem: 3153
I20241103 20:52:50 1321057 dinov2 helpers.py:102] Training  [  740/12500]  eta: 0:29:05  loss: 72.3909 (83.0685)  lr: 0.0000 (0.0000)  time: 0.141462  data: 0.000510  max mem: 3153
I20241103 20:52:51 1321057 dinov2 helpers.py:102] Training  [  750/12500]  eta: 0:29:02  loss: 71.9724 (82.8433)  lr: 0.0000 (0.0000)  time: 0.141401  data: 0.000534  max mem: 3153
I20241103 20:52:52 1321057 dinov2 helpers.py:102] Training  [  760/12500]  eta: 0:28:59  loss: 71.8930 (82.6865)  lr: 0.0000 (0.0000)  time: 0.141468  data: 0.000497  max mem: 3153
I20241103 20:52:54 1321057 dinov2 helpers.py:102] Training  [  770/12500]  eta: 0:28:57  loss: 71.8930 (82.5693)  lr: 0.0000 (0.0000)  time: 0.141512  data: 0.000499  max mem: 3153
I20241103 20:52:55 1321057 dinov2 helpers.py:102] Training  [  780/12500]  eta: 0:28:54  loss: 71.8930 (82.4397)  lr: 0.0000 (0.0000)  time: 0.141452  data: 0.000521  max mem: 3153
I20241103 20:52:57 1321057 dinov2 helpers.py:102] Training  [  790/12500]  eta: 0:28:53  loss: 71.5635 (82.2885)  lr: 0.0000 (0.0000)  time: 0.145541  data: 0.005858  max mem: 3153
I20241103 20:52:58 1321057 dinov2 helpers.py:102] Training  [  800/12500]  eta: 0:28:51  loss: 71.5635 (82.1233)  lr: 0.0000 (0.0000)  time: 0.145312  data: 0.005872  max mem: 3153
I20241103 20:53:00 1321057 dinov2 helpers.py:102] Training  [  810/12500]  eta: 0:28:48  loss: 71.5378 (81.9905)  lr: 0.0000 (0.0000)  time: 0.141194  data: 0.000499  max mem: 3153
I20241103 20:53:01 1321057 dinov2 helpers.py:102] Training  [  820/12500]  eta: 0:28:46  loss: 71.5378 (81.8278)  lr: 0.0000 (0.0000)  time: 0.141322  data: 0.000472  max mem: 3153
I20241103 20:53:02 1321057 dinov2 helpers.py:102] Training  [  830/12500]  eta: 0:28:43  loss: 71.2303 (81.6693)  lr: 0.0000 (0.0000)  time: 0.141266  data: 0.000477  max mem: 3153
I20241103 20:53:04 1321057 dinov2 helpers.py:102] Training  [  840/12500]  eta: 0:28:41  loss: 70.7678 (81.5333)  lr: 0.0000 (0.0000)  time: 0.141329  data: 0.000492  max mem: 3153
I20241103 20:53:05 1321057 dinov2 helpers.py:102] Training  [  850/12500]  eta: 0:28:39  loss: 70.4123 (81.3744)  lr: 0.0000 (0.0000)  time: 0.141368  data: 0.000491  max mem: 3153
I20241103 20:53:07 1321057 dinov2 helpers.py:102] Training  [  860/12500]  eta: 0:28:36  loss: 70.3416 (81.2285)  lr: 0.0000 (0.0000)  time: 0.141347  data: 0.000458  max mem: 3153
I20241103 20:53:08 1321057 dinov2 helpers.py:102] Training  [  870/12500]  eta: 0:28:34  loss: 70.3416 (81.0691)  lr: 0.0000 (0.0000)  time: 0.141405  data: 0.000484  max mem: 3153
I20241103 20:53:09 1321057 dinov2 helpers.py:102] Training  [  880/12500]  eta: 0:28:32  loss: 70.3416 (80.9903)  lr: 0.0000 (0.0000)  time: 0.141436  data: 0.000495  max mem: 3153
I20241103 20:53:11 1321057 dinov2 helpers.py:102] Training  [  890/12500]  eta: 0:28:30  loss: 70.1071 (80.8375)  lr: 0.0000 (0.0000)  time: 0.141441  data: 0.000496  max mem: 3153
I20241103 20:53:12 1321057 dinov2 helpers.py:102] Training  [  900/12500]  eta: 0:28:27  loss: 68.9108 (80.6914)  lr: 0.0000 (0.0000)  time: 0.141452  data: 0.000481  max mem: 3153
I20241103 20:53:14 1321057 dinov2 helpers.py:102] Training  [  910/12500]  eta: 0:28:25  loss: 68.9108 (80.5771)  lr: 0.0000 (0.0000)  time: 0.141573  data: 0.000474  max mem: 3153
I20241103 20:53:15 1321057 dinov2 helpers.py:102] Training  [  920/12500]  eta: 0:28:24  loss: 70.1071 (80.5006)  lr: 0.0000 (0.0000)  time: 0.145760  data: 0.005845  max mem: 3153
I20241103 20:53:17 1321057 dinov2 helpers.py:102] Training  [  930/12500]  eta: 0:28:22  loss: 68.9108 (80.3623)  lr: 0.0000 (0.0000)  time: 0.145621  data: 0.005824  max mem: 3153
I20241103 20:53:18 1321057 dinov2 helpers.py:102] Training  [  940/12500]  eta: 0:28:20  loss: 68.6757 (80.2050)  lr: 0.0000 (0.0000)  time: 0.141433  data: 0.000446  max mem: 3153
I20241103 20:53:19 1321057 dinov2 helpers.py:102] Training  [  950/12500]  eta: 0:28:18  loss: 68.6757 (80.0624)  lr: 0.0000 (0.0000)  time: 0.141515  data: 0.000446  max mem: 3153
I20241103 20:53:21 1321057 dinov2 helpers.py:102] Training  [  960/12500]  eta: 0:28:15  loss: 68.5176 (79.8923)  lr: 0.0000 (0.0000)  time: 0.141453  data: 0.000455  max mem: 3153
I20241103 20:53:22 1321057 dinov2 helpers.py:102] Training  [  970/12500]  eta: 0:28:13  loss: 68.5176 (79.7839)  lr: 0.0000 (0.0000)  time: 0.141387  data: 0.000476  max mem: 3153
I20241103 20:53:24 1321057 dinov2 helpers.py:102] Training  [  980/12500]  eta: 0:28:11  loss: 68.5176 (79.6893)  lr: 0.0000 (0.0000)  time: 0.141421  data: 0.000467  max mem: 3153
I20241103 20:53:25 1321057 dinov2 helpers.py:102] Training  [  990/12500]  eta: 0:28:09  loss: 68.5176 (79.5885)  lr: 0.0000 (0.0000)  time: 0.141545  data: 0.000447  max mem: 3153
I20241103 20:53:26 1321057 dinov2 helpers.py:102] Training  [ 1000/12500]  eta: 0:28:07  loss: 68.4865 (79.4626)  lr: 0.0000 (0.0000)  time: 0.141554  data: 0.000466  max mem: 3153
I20241103 20:53:28 1321057 dinov2 helpers.py:102] Training  [ 1010/12500]  eta: 0:28:05  loss: 67.8714 (79.2920)  lr: 0.0000 (0.0000)  time: 0.141523  data: 0.000516  max mem: 3153
I20241103 20:53:29 1321057 dinov2 helpers.py:102] Training  [ 1020/12500]  eta: 0:28:03  loss: 67.8714 (79.1993)  lr: 0.0000 (0.0000)  time: 0.141493  data: 0.000519  max mem: 3153
I20241103 20:53:31 1321057 dinov2 helpers.py:102] Training  [ 1030/12500]  eta: 0:28:01  loss: 67.5422 (79.0678)  lr: 0.0000 (0.0000)  time: 0.141552  data: 0.000485  max mem: 3153
I20241103 20:53:32 1321057 dinov2 helpers.py:102] Training  [ 1040/12500]  eta: 0:27:59  loss: 67.5081 (78.9191)  lr: 0.0000 (0.0000)  time: 0.141499  data: 0.000483  max mem: 3153
I20241103 20:53:34 1321057 dinov2 helpers.py:102] Training  [ 1050/12500]  eta: 0:27:58  loss: 67.5081 (78.8161)  lr: 0.0000 (0.0000)  time: 0.145609  data: 0.005813  max mem: 3153
I20241103 20:53:35 1321057 dinov2 helpers.py:102] Training  [ 1060/12500]  eta: 0:27:56  loss: 67.2571 (78.7081)  lr: 0.0000 (0.0000)  time: 0.145490  data: 0.005820  max mem: 3153
I20241103 20:53:36 1321057 dinov2 helpers.py:102] Training  [ 1070/12500]  eta: 0:27:54  loss: 67.2571 (78.5985)  lr: 0.0000 (0.0000)  time: 0.140971  data: 0.000478  max mem: 3153
I20241103 20:53:38 1321057 dinov2 helpers.py:102] Training  [ 1080/12500]  eta: 0:27:52  loss: 67.2571 (78.5189)  lr: 0.0000 (0.0000)  time: 0.141077  data: 0.000504  max mem: 3153
I20241103 20:53:39 1321057 dinov2 helpers.py:102] Training  [ 1090/12500]  eta: 0:27:50  loss: 67.2571 (78.3972)  lr: 0.0000 (0.0000)  time: 0.141525  data: 0.000453  max mem: 3153
I20241103 20:53:41 1321057 dinov2 helpers.py:102] Training  [ 1100/12500]  eta: 0:27:48  loss: 66.8724 (78.2809)  lr: 0.0000 (0.0000)  time: 0.141590  data: 0.000351  max mem: 3153
I20241103 20:53:42 1321057 dinov2 helpers.py:102] Training  [ 1110/12500]  eta: 0:27:46  loss: 66.8714 (78.1662)  lr: 0.0000 (0.0000)  time: 0.141422  data: 0.000339  max mem: 3153
I20241103 20:53:44 1321057 dinov2 helpers.py:102] Training  [ 1120/12500]  eta: 0:27:44  loss: 66.7360 (78.0651)  lr: 0.0000 (0.0000)  time: 0.141316  data: 0.000334  max mem: 3153
I20241103 20:53:45 1321057 dinov2 helpers.py:102] Training  [ 1130/12500]  eta: 0:27:42  loss: 66.5064 (77.9539)  lr: 0.0000 (0.0000)  time: 0.141297  data: 0.000343  max mem: 3153
I20241103 20:53:46 1321057 dinov2 helpers.py:102] Training  [ 1140/12500]  eta: 0:27:40  loss: 66.5064 (77.8214)  lr: 0.0000 (0.0000)  time: 0.141646  data: 0.000416  max mem: 3153
I20241103 20:53:48 1321057 dinov2 helpers.py:102] Training  [ 1150/12500]  eta: 0:27:38  loss: 66.1233 (77.7206)  lr: 0.0000 (0.0000)  time: 0.141463  data: 0.000414  max mem: 3153
I20241103 20:53:49 1321057 dinov2 helpers.py:102] Training  [ 1160/12500]  eta: 0:27:36  loss: 66.1233 (77.6150)  lr: 0.0000 (0.0000)  time: 0.141337  data: 0.000376  max mem: 3153
I20241103 20:53:51 1321057 dinov2 helpers.py:102] Training  [ 1170/12500]  eta: 0:27:34  loss: 65.5283 (77.4495)  lr: 0.0000 (0.0000)  time: 0.141537  data: 0.000394  max mem: 3153
I20241103 20:53:52 1321057 dinov2 helpers.py:102] Training  [ 1180/12500]  eta: 0:27:32  loss: 65.5283 (77.3576)  lr: 0.0000 (0.0000)  time: 0.141481  data: 0.000406  max mem: 3153
I20241103 20:53:54 1321057 dinov2 helpers.py:102] Training  [ 1190/12500]  eta: 0:27:31  loss: 65.5283 (77.2735)  lr: 0.0000 (0.0000)  time: 0.146469  data: 0.005744  max mem: 3153
I20241103 20:53:55 1321057 dinov2 helpers.py:102] Training  [ 1200/12500]  eta: 0:27:29  loss: 65.4893 (77.1557)  lr: 0.0000 (0.0000)  time: 0.146376  data: 0.005732  max mem: 3153
I20241103 20:53:56 1321057 dinov2 helpers.py:102] Training  [ 1210/12500]  eta: 0:27:27  loss: 65.5283 (77.0677)  lr: 0.0000 (0.0000)  time: 0.141194  data: 0.000399  max mem: 3153
I20241103 20:53:58 1321057 dinov2 helpers.py:102] Training  [ 1220/12500]  eta: 0:27:26  loss: 65.4893 (76.9307)  lr: 0.0000 (0.0000)  time: 0.141423  data: 0.000394  max mem: 3153
I20241103 20:53:59 1321057 dinov2 helpers.py:102] Training  [ 1230/12500]  eta: 0:27:24  loss: 65.4369 (76.8049)  lr: 0.0000 (0.0000)  time: 0.141383  data: 0.000394  max mem: 3153
I20241103 20:54:01 1321057 dinov2 helpers.py:102] Training  [ 1240/12500]  eta: 0:27:22  loss: 65.4893 (76.7152)  lr: 0.0000 (0.0000)  time: 0.141228  data: 0.000397  max mem: 3153
I20241103 20:54:02 1321057 dinov2 linear.py:272] running validation !
E20241103 20:54:02 1321057 submitit submission.py:68] Submitted job triggered an exception
